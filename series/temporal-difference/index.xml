<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Temporal Difference on Data Artificer and code:Breaker</title><link>https://n1o.github.io/series/temporal-difference/</link><description>Recent content in Temporal Difference on Data Artificer and code:Breaker</description><generator>Hugo</generator><language>en</language><lastBuildDate>Tue, 18 Feb 2025 06:20:35 +0100</lastBuildDate><atom:link href="https://n1o.github.io/series/temporal-difference/index.xml" rel="self" type="application/rss+xml"/><item><title>RL Bite: Computing the Value Function</title><link>https://n1o.github.io/posts/rl-bite-computing-value-functions/</link><pubDate>Tue, 18 Feb 2025 06:20:35 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-computing-value-functions/</guid><description>Abstract Link to heading In the last RL-Bite I wrote about Bellman&amp;rsquo;s Equations and the Value Function and now we will figure out how we actually apply these equations to compute the Value Function!
Known World Model Link to heading Let&amp;rsquo;s start with the simple case, and make an assumption that the underlying World Model of the Markov Decision Process is known, and we have finite discrete states. If we include that the Discount Factor $\gamma &amp;lt; 1$, we can find the Optimal Value function exactly using:</description></item></channel></rss>