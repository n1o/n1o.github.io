<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Awesome State Space Models on Data, Code and Breaking Stuff</title><link>https://n1o.github.io/series/awesome-state-space-models/</link><description>Recent content in Awesome State Space Models on Data, Code and Breaking Stuff</description><generator>Hugo</generator><language>en</language><lastBuildDate>Thu, 08 Aug 2024 09:57:32 +0200</lastBuildDate><atom:link href="https://n1o.github.io/series/awesome-state-space-models/index.xml" rel="self" type="application/rss+xml"/><item><title>From Mamba to Mamba-2</title><link>https://n1o.github.io/posts/from-mamba-to-mamba2/</link><pubDate>Thu, 08 Aug 2024 09:57:32 +0200</pubDate><guid>https://n1o.github.io/posts/from-mamba-to-mamba2/</guid><description>Abstract Link to heading This is not my first gig where I write about State Space Models. I already mentioned them here and here. Now what is the deal with this Mamba(2) thing? They are proving to be an alternative to the strong Transformer++ architecture (Transformer++ models like LLaMa are based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention). Hold on, if this Transformer++ models work well, why do we need altneratives?</description></item></channel></rss>