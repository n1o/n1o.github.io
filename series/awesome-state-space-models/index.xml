<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Awesome State Space Models on Artificial Intelligence and Machine Learning Research</title><link>https://n1o.github.io/series/awesome-state-space-models/</link><description>Recent content in Awesome State Space Models on Artificial Intelligence and Machine Learning Research</description><generator>Hugo</generator><language>en</language><lastBuildDate>Thu, 29 Aug 2024 11:53:51 +0200</lastBuildDate><atom:link href="https://n1o.github.io/series/awesome-state-space-models/index.xml" rel="self" type="application/rss+xml"/><item><title>Hydra a Double Headed Mamba</title><link>https://n1o.github.io/posts/hydra-a-double-headed-mamba/</link><pubDate>Thu, 29 Aug 2024 11:53:51 +0200</pubDate><guid>https://n1o.github.io/posts/hydra-a-double-headed-mamba/</guid><description>Abstract Link to heading State Space Models are awesome, models like Mamba and Mamba2 boast unparalleled performance especially when it comes to long sequences. The only downside is that they are causal, which means they model one token at a time, looking only at past tokens. Bidirectional models like Bert, CodeBERT and GraphCodeBERT have been shown to excel when it comes to code understanding. One way to put it is that by looking into the past and the future simultaneously we can get a better understanding of what is happening.</description></item><item><title>From Mamba to Mamba-2</title><link>https://n1o.github.io/posts/from-mamba-to-mamba2/</link><pubDate>Thu, 08 Aug 2024 09:57:32 +0200</pubDate><guid>https://n1o.github.io/posts/from-mamba-to-mamba2/</guid><description>Abstract Link to heading This is not my first gig where I write about State Space Models. I already mentioned them here and here. Now what is the deal with this Mamba(2) thing? They are proving to be an alternative to the strong Transformer++ architecture (Transformer++ models like LLaMa are based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention). Hold on, if this Transformer++ models work well, why do we need altneratives?</description></item></channel></rss>