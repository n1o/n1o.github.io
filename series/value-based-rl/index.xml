<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Value Based RL on Data Artificer and code:Breaker</title><link>https://n1o.github.io/series/value-based-rl/</link><description>Recent content in Value Based RL on Data Artificer and code:Breaker</description><generator>Hugo</generator><language>en</language><lastBuildDate>Tue, 18 Feb 2025 06:20:35 +0100</lastBuildDate><atom:link href="https://n1o.github.io/series/value-based-rl/index.xml" rel="self" type="application/rss+xml"/><item><title>RL Bite: Computing the Value Function</title><link>https://n1o.github.io/posts/rl-bite-computing-value-functions/</link><pubDate>Tue, 18 Feb 2025 06:20:35 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-computing-value-functions/</guid><description>Abstract Link to heading In the last RL-Bite I wrote about Bellman&amp;rsquo;s Equations and the Value Function and now we will figure out how we actually apply these equations to compute the Value Function!
Known World Model Link to heading Let&amp;rsquo;s start with the simple case, and make an assumption that the underlying World Model of the Markov Decision Process is known, and we have finite discrete states. If we include that the Discount Factor $\gamma &amp;lt; 1$, we can find the Optimal Value function exactly using:</description></item><item><title>RL Bite: Bellmans Equations and Value Functions</title><link>https://n1o.github.io/posts/rl-bite-bellmans-equations-and-value-functions/</link><pubDate>Wed, 12 Feb 2025 07:00:52 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-bellmans-equations-and-value-functions/</guid><description>Value Based Reinforced Learning Link to heading In value based Reinforced Learning we learn a Value Function:
$$ V_{\pi}(s) = E_{\pi}[G_0|s_0 = s] = E_{\pi}[\sum_{t=0}^T \gamma^t r_t|s_0 = s] $$
$G_t$ is the Total Return at time t, this is just the sum of Rewards an Agent gets walking the trajectory T (fancy name but this is just a sequence of actions the agent takes) $\gamma^t$ is the Discount Factor, long story short this is between $&amp;lt;0,1&amp;gt;$, the closer this value is to zero, the more the agent will focus on the immediate reward, the closer it is to 1 the more it will take future rewards into account.</description></item></channel></rss>