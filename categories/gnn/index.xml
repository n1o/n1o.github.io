<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GNN on Data Artificer and code:Breaker</title><link>https://n1o.github.io/categories/gnn/</link><description>Recent content in GNN on Data Artificer and code:Breaker</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sun, 23 Feb 2025 13:04:42 +0100</lastBuildDate><atom:link href="https://n1o.github.io/categories/gnn/index.xml" rel="self" type="application/rss+xml"/><item><title>TLDR; Graph Contrastive Learning: Representation Scattering</title><link>https://n1o.github.io/posts/tldr-graph-contrastive-repr-representation-shattering/</link><pubDate>Sun, 23 Feb 2025 13:04:42 +0100</pubDate><guid>https://n1o.github.io/posts/tldr-graph-contrastive-repr-representation-shattering/</guid><description>Source Link to heading Paper link: https://openreview.net/pdf?id=R8SolCx62K Source Code: https://github.com/hedongxiao-tju/SGRL Abstract Link to heading Contrastive Learning (CL) is one of my favorite techniques, it is a self-supervised approach for learning latent representations with a special property: Similar elements have representations that are closer together and elements that are different are farther from each other. The paper: Exploitation of a Latent Mechanism in Graph Contrastive Learning Representation Scattering takes a very novel approach to CL and it gives a nice theoretical foundation of CL and Graph!</description></item><item><title>TLDR; HC-GAE The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation Learning</title><link>https://n1o.github.io/posts/tldr-hc-gae/</link><pubDate>Sun, 16 Feb 2025 14:48:13 +0100</pubDate><guid>https://n1o.github.io/posts/tldr-hc-gae/</guid><description>Source Link to heading Paper link: https://arxiv.org/abs/2405.14742 Source Code: https://github.com/JonathanGXu/HC-GAE Abstract Link to heading Graph Representation Learning is an essential topic in Graph ML, and it is all about compressing a whole Graph (arbitrarily large) into a fixed representation. Usually these techniques leverage Graph Auto Encoders, which are trained in a self-supervised fashion. This is all good; however, they usually focus on node feature reconstruction, and they tend to lose the topological information that the input Graph encodes.</description></item><item><title>TLDR; Duplex: Dual GAT for Complex Embeddings of Directed Graphs</title><link>https://n1o.github.io/posts/tldr-duplex/</link><pubDate>Mon, 10 Feb 2025 09:22:15 +0100</pubDate><guid>https://n1o.github.io/posts/tldr-duplex/</guid><description>Source Link to heading Paper link: https://arxiv.org/abs/2406.05391 Source Code: https://github.com/alipay/DUPLEX Abstract Link to heading I am a huge fan of Graph Machine Learning, it has a lot of cool applications, and I am particularly interested in Source Code understanding and Vulnerability Detection, where Graph Neural Networks (GNN) are unambiguous. One of the obvious downsides of general GNNs is that they mostly focus on undirected graphs, which makes their approach somewhat limiting for Digraphs (fancy name for directed graphs).</description></item></channel></rss>