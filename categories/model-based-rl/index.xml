<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Model Based RL on Data Artificer and code:Breaker</title><link>https://n1o.github.io/categories/model-based-rl/</link><description>Recent content in Model Based RL on Data Artificer and code:Breaker</description><generator>Hugo</generator><language>en</language><lastBuildDate>Thu, 10 Apr 2025 09:00:56 +0200</lastBuildDate><atom:link href="https://n1o.github.io/categories/model-based-rl/index.xml" rel="self" type="application/rss+xml"/><item><title>RL Bite: Monte Carlo Search Tree</title><link>https://n1o.github.io/posts/rl-bite-monte-carlo-search-tree/</link><pubDate>Thu, 10 Apr 2025 09:00:56 +0200</pubDate><guid>https://n1o.github.io/posts/rl-bite-monte-carlo-search-tree/</guid><description>Abstract Link to heading Let&amp;rsquo;s talk a bit about Model-Based Reinforcement Learning. The idea is that our RL Agent not just learns a policy to follow or/and a value function (Q function) but also tries to model the environment it is in. This is done by learning the transition dynamics $p(s&amp;rsquo;|s,a)$ (also known as World Model) and a Reward function $\hat{R}(s,a)$. Once we have our world model, we can use it to simulate data and learn the model&amp;rsquo;s policy on the simulations.</description></item></channel></rss>