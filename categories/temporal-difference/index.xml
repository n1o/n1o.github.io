<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Temporal Difference on Artificial Intelligence and Machine Learning Research</title><link>https://n1o.github.io/categories/temporal-difference/</link><description>Recent content in Temporal Difference on Artificial Intelligence and Machine Learning Research</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sat, 08 Mar 2025 13:24:13 +0100</lastBuildDate><atom:link href="https://n1o.github.io/categories/temporal-difference/index.xml" rel="self" type="application/rss+xml"/><item><title>RL Bite: Policy Gradient and Reinforce</title><link>https://n1o.github.io/posts/rl-bite-policy-gradient-and-reinforce/</link><pubDate>Sat, 08 Mar 2025 13:24:13 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-policy-gradient-and-reinforce/</guid><description>Abstract Link to heading Till now we have considered only learning the Value or Q function and estimating the policy from those. In the next few posts, we are going to look into directly learning the policy. Why directly learn the policy? First, Q learning has a lot of issues involving the Deadly Triad; second, if we have continuous actions we cannot really use it; and lastly, Q learning always learns a deterministic policy, and in cases of partially observed stochastic environments (which is nearly always what we have), having a stochastic policy is proven to be better.</description></item><item><title>RL Bite: Learning the Q Function</title><link>https://n1o.github.io/posts/rl-bite-learning-the-q-function/</link><pubDate>Mon, 03 Mar 2025 08:59:26 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-learning-the-q-function/</guid><description>Abstract Link to heading We already know how to learn the Value function, however we also know that the Value function by itself is not enough since it averages over all possible actions, instead of taking into consideration specific actions the agent should take. We can derive the Q function from the Value function, however we can also try to directly learn it. Especially directly approximating the Q function with Deep Neural Networks has been a huge success when applied to RL Agents playing Atari computer games.</description></item><item><title>RL Bite: Computing the Value Function</title><link>https://n1o.github.io/posts/rl-bite-computing-value-functions/</link><pubDate>Tue, 18 Feb 2025 06:20:35 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-computing-value-functions/</guid><description>Abstract Link to heading In the last RL-Bite I wrote about Bellman&amp;rsquo;s Equations and the Value Function and now we will figure out how we actually apply these equations to compute the Value Function!
Known World Model Link to heading Let&amp;rsquo;s start with the simple case, and make an assumption that the underlying World Model of the Markov Decision Process is known, and we have finite discrete states. If we include that the Discount Factor $\gamma &amp;lt; 1$, we can find the Optimal Value function exactly using:</description></item></channel></rss>