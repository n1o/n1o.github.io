<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SSM on Data Artificer and code:Breaker</title><link>https://n1o.github.io/tags/ssm/</link><description>Recent content in SSM on Data Artificer and code:Breaker</description><generator>Hugo</generator><language>en</language><lastBuildDate>Mon, 28 Oct 2024 10:12:27 +0100</lastBuildDate><atom:link href="https://n1o.github.io/tags/ssm/index.xml" rel="self" type="application/rss+xml"/><item><title>Distilling State Space Models from Transformers</title><link>https://n1o.github.io/posts/distilling-ssm-from-transformers/</link><pubDate>Mon, 28 Oct 2024 10:12:27 +0100</pubDate><guid>https://n1o.github.io/posts/distilling-ssm-from-transformers/</guid><description>&lt;h1 id="abstract">
 Abstract
 &lt;a class="heading-link" href="#abstract">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>It is notoriously expensive to train a Language Model from scratch, making independent research impossible and trying out new architectures extremely risky. Because of these costs, Transformer++ models like LLaMa, based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention, are the de facto standard, not because they are the best, but because they are proven to work! In previous posts I covered &lt;a href="https://n1o.github.io/posts/ssm-transformer-hybrids-guide/" >SSM-Transformer Hybrids&lt;/a>. The biggest benefit of hybridization is their reduced inference cost and minimal memory overhead due to reduced KV cache, and combining SSMs can Attention can make the model more expressive. The biggest obstacle of these models is that they need to be pretrained from scratch. For example &lt;a href="https://www.zyphra.com/post/zamba2-7b" class="external-link" target="_blank" rel="noopener">Zamba2 7B&lt;/a>, is a 7B model that was trained on 128 H100 GPUs for 50 days, bringing its pretraining costs to around 600K US dollars. This makes it one of the cheapest (but by far not weakest) SSM-Attention hybrids, however if we look into detail the model was trained only on 3T (+100B high quality for annealing) tokens. In comparison Llama3 was trained on 15T tokens, if we would apply the same number of tokens to Zamba we would end up with costs around 3M US dollars. It is not hard to see that these kinds of budgets are out of scope for any individual, but also out of scope for many medium sized research organizations and academia.&lt;/p></description></item><item><title>Mamba(2) and Transformer Hybrids: An Overview</title><link>https://n1o.github.io/posts/ssm-transformer-hybrids-guide/</link><pubDate>Wed, 18 Sep 2024 11:26:25 +0200</pubDate><guid>https://n1o.github.io/posts/ssm-transformer-hybrids-guide/</guid><description>&lt;h1 id="abstract">
 Abstract
 &lt;a class="heading-link" href="#abstract">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>We have already looked into &lt;a href="https://n1o.github.io/posts/from-mamba-to-mamba2/" >Mamba and Mamba2&lt;/a>. In terms of efficiency, with their linear complexity and the absence of Key-Value cache, they are a significant improvement over Attention-based models in terms of throughput and memory usage. However, not everything is perfect. Transformers have a certain advantage when it comes to in-context learning. In-context learning is the ability to adapt the model without retraining it. This is done by providing relevant (or not) context to the model in the form of a prompt.&lt;/p></description></item><item><title>Hydra a Double Headed Mamba</title><link>https://n1o.github.io/posts/hydra-a-double-headed-mamba/</link><pubDate>Thu, 29 Aug 2024 11:53:51 +0200</pubDate><guid>https://n1o.github.io/posts/hydra-a-double-headed-mamba/</guid><description>&lt;h1 id="abstract">
 Abstract
 &lt;a class="heading-link" href="#abstract">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>State Space Models are awesome, models like Mamba and Mamba2 boast unparalleled performance especially when it comes to long sequences. The only downside is that they are causal, which means they model one token at a time, looking only at past tokens. Bidirectional models like &lt;a href="https://codebreakers.re/articles/detail/bert-codebert-and-graphcodebert/" class="external-link" target="_blank" rel="noopener">Bert, CodeBERT and GraphCodeBERT&lt;/a> have been shown to excel when it comes to code understanding. One way to put it is that by looking into the past and the future simultaneously we can get a better understanding of what is happening. Hydra is a bidirectional extension of Mamba2 that builds upon solid mathematical foundations, instead of just naively taking two Mamba(2)&amp;rsquo;s, flipping one of them, and somehow combining them.&lt;/p></description></item><item><title>From Mamba to Mamba-2</title><link>https://n1o.github.io/posts/from-mamba-to-mamba2/</link><pubDate>Thu, 08 Aug 2024 09:57:32 +0200</pubDate><guid>https://n1o.github.io/posts/from-mamba-to-mamba2/</guid><description>&lt;h1 id="abstract">
 Abstract
 &lt;a class="heading-link" href="#abstract">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>This is not my first gig where I write about State Space Models. I already mentioned them &lt;a href="https://n1o.github.io/posts/hungry-hungry-hippos/" >here&lt;/a> and &lt;a href="https://n1o.github.io/posts/butterflies-monarchs-hyenas-and-lightning-fast-bert/" >here&lt;/a>. Now what is the deal with this Mamba(2) thing? They are proving to be an alternative to the strong Transformer++ architecture (Transformer++ models like LLaMa are based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention). Hold on, if this Transformer++ models work well, why do we need altneratives? There are multiple reason:&lt;/p></description></item><item><title>Butterflies, Monarchs, Hyenas, and Lightning Fast BERT</title><link>https://n1o.github.io/posts/butterflies-monarchs-hyenas-and-lightning-fast-bert/</link><pubDate>Fri, 12 Jul 2024 13:36:49 +0200</pubDate><guid>https://n1o.github.io/posts/butterflies-monarchs-hyenas-and-lightning-fast-bert/</guid><description>&lt;h1 id="abstract">
 Abstract
 &lt;a class="heading-link" href="#abstract">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>I have been working on a project of creating my own Large Language Model, as I am huge fan of T5, or to be more concrete I recognize the added value of having an Encoder-Decoder architecture. The biggest challenge, at least in my opinion, in training an LLM is the sheer computational costs required to do so. I was originally planning to take the Encoder introduced by &lt;a href="https://n1o.github.io/posts/longer-context-for-t5/" >ColT5&lt;/a> but than I came across of &lt;a href="https://hazyresearch.stanford.edu/blog/2024-05-20-m2-bert-retrieval" class="external-link" target="_blank" rel="noopener">M2 BERT&lt;/a> and suddenly I went down the rabbit hole of Structured Matrices, Butterflies, Monarch and Hyeans.&lt;/p></description></item><item><title>Paper overview: Hungry Hungry Hippos: Towards Language Modeling with State Space Models</title><link>https://n1o.github.io/posts/hungry-hungry-hippos/</link><pubDate>Mon, 06 Feb 2023 09:39:03 +0100</pubDate><guid>https://n1o.github.io/posts/hungry-hungry-hippos/</guid><description>&lt;h1 id="high-level-overview">
 High level overview
 &lt;a class="heading-link" href="#high-level-overview">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>By combining State Space Models (SSMs) with Attention, we get a model that generates text more efficiently, with a speed increase of approximately 1.6 times. Additionally, this approach requires less paremters, enabling the development of larger models on existing hardware.&lt;/p>
&lt;h1 id="language-modeling-requirements">
 Language modeling requirements
 &lt;a class="heading-link" href="#language-modeling-requirements">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>The Transformer architecture, which forms the basis of ChatGPT, is riding high on the hype train due to its impressive performance. Although it is just a basic &lt;a href="ttps://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html" >Transformer&lt;/a>, it has proven to be extremely effective. The reason for its success is explored in the paper &amp;ldquo;&lt;a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html" class="external-link" target="_blank" rel="noopener">In-Context Learning and Induction Heads&lt;/a>.&amp;rdquo; The authors argue that the majority of the in-context learning capacity of the Transformer architecture can be evaluated by two tests:&lt;/p></description></item></channel></rss>