<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SSM on Data, Code and Breaking Stuff</title><link>https://n1o.github.io/tags/ssm/</link><description>Recent content in SSM on Data, Code and Breaking Stuff</description><generator>Hugo</generator><language>en</language><lastBuildDate>Wed, 18 Sep 2024 11:26:25 +0200</lastBuildDate><atom:link href="https://n1o.github.io/tags/ssm/index.xml" rel="self" type="application/rss+xml"/><item><title>Mamba(2) and Transformer Hybrids: An Overview</title><link>https://n1o.github.io/posts/ssm-transformer-hybrids-guide/</link><pubDate>Wed, 18 Sep 2024 11:26:25 +0200</pubDate><guid>https://n1o.github.io/posts/ssm-transformer-hybrids-guide/</guid><description>&lt;h1 id="abstract">
 Abstract
 &lt;a class="heading-link" href="#abstract">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>We have already looked into &lt;a href="https://n1o.github.io/posts/from-mamba-to-mamba2/" >Mamba and Mamba2&lt;/a>. In terms of efficiency, with their linear complexity and the absence of Key-Value cache, they are a significant improvement over Attention-based models in terms of throughput and memory usage. However, not everything is perfect. Transformers have a certain advantage when it comes to in-context learning. In-context learning is the ability to adapt the model without retraining it. This is done by providing relevant (or not) context to the model in the form of a prompt.&lt;/p></description></item><item><title>Hydra a Double Headed Mamba</title><link>https://n1o.github.io/posts/hydra-a-double-headed-mamba/</link><pubDate>Thu, 29 Aug 2024 11:53:51 +0200</pubDate><guid>https://n1o.github.io/posts/hydra-a-double-headed-mamba/</guid><description>&lt;h1 id="abstract">
 Abstract
 &lt;a class="heading-link" href="#abstract">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>State Space Models are awesome, models like Mamba and Mamba2 boast unparalleled performance especially when it comes to long sequences. The only downside is that they are causal, which means they model one token at a time, looking only at past tokens. Bidirectional models like &lt;a href="https://codebreakers.re/articles/detail/bert-codebert-and-graphcodebert/" class="external-link" target="_blank" rel="noopener">Bert, CodeBERT and GraphCodeBERT&lt;/a> have been shown to excel when it comes to code understanding. One way to put it is that by looking into the past and the future simultaneously we can get a better understanding of what is happening. Hydra is a bidirectional extension of Mamba2 that builds upon solid mathematical foundations, instead of just naively taking two Mamba(2)&amp;rsquo;s, flipping one of them, and somehow combining them.&lt;/p></description></item><item><title>From Mamba to Mamba-2</title><link>https://n1o.github.io/posts/from-mamba-to-mamba2/</link><pubDate>Thu, 08 Aug 2024 09:57:32 +0200</pubDate><guid>https://n1o.github.io/posts/from-mamba-to-mamba2/</guid><description>&lt;h1 id="abstract">
 Abstract
 &lt;a class="heading-link" href="#abstract">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>This is not my first gig where I write about State Space Models. I already mentioned them &lt;a href="https://n1o.github.io/posts/hungry-hungry-hippos/" >here&lt;/a> and &lt;a href="https://n1o.github.io/posts/butterflies-monarchs-hyenas-and-lightning-fast-bert/" >here&lt;/a>. Now what is the deal with this Mamba(2) thing? They are proving to be an alternative to the strong Transformer++ architecture (Transformer++ models like LLaMa are based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention). Hold on, if this Transformer++ models work well, why do we need altneratives? There are multiple reason:&lt;/p></description></item><item><title>Butterflies, Monarchs, Hyenas, and Lightning Fast BERT</title><link>https://n1o.github.io/posts/butterflies-monarchs-hyenas-and-lightning-fast-bert/</link><pubDate>Fri, 12 Jul 2024 13:36:49 +0200</pubDate><guid>https://n1o.github.io/posts/butterflies-monarchs-hyenas-and-lightning-fast-bert/</guid><description>&lt;h1 id="abstract">
 Abstract
 &lt;a class="heading-link" href="#abstract">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>I have been working on a project of creating my own Large Language Model, as I am huge fan of T5, or to be more concrete I recognize the added value of having an Encoder-Decoder architecture. The biggest challenge, at least in my opinion, in training an LLM is the sheer computational costs required to do so. I was originally planning to take the Encoder introduced by &lt;a href="https://n1o.github.io/posts/longer-context-for-t5/" >ColT5&lt;/a> but than I came across of &lt;a href="https://hazyresearch.stanford.edu/blog/2024-05-20-m2-bert-retrieval" class="external-link" target="_blank" rel="noopener">M2 BERT&lt;/a> and suddenly I went down the rabbit hole of Structured Matrices, Butterflies, Monarch and Hyeans.&lt;/p></description></item><item><title>Paper overview: Hungry Hungry Hippos: Towards Language Modeling with State Space Models</title><link>https://n1o.github.io/posts/hungry-hungry-hippos/</link><pubDate>Mon, 06 Feb 2023 09:39:03 +0100</pubDate><guid>https://n1o.github.io/posts/hungry-hungry-hippos/</guid><description>&lt;h1 id="high-level-overview">
 High level overview
 &lt;a class="heading-link" href="#high-level-overview">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>By combining State Space Models (SSMs) with Attention, we get a model that generates text more efficiently, with a speed increase of approximately 1.6 times. Additionally, this approach requires less paremters, enabling the development of larger models on existing hardware.&lt;/p>
&lt;h1 id="language-modeling-requirements">
 Language modeling requirements
 &lt;a class="heading-link" href="#language-modeling-requirements">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>The Transformer architecture, which forms the basis of ChatGPT, is riding high on the hype train due to its impressive performance. Although it is just a basic &lt;a href="ttps://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html" >Transformer&lt;/a>, it has proven to be extremely effective. The reason for its success is explored in the paper &amp;ldquo;&lt;a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html" class="external-link" target="_blank" rel="noopener">In-Context Learning and Induction Heads&lt;/a>.&amp;rdquo; The authors argue that the majority of the in-context learning capacity of the Transformer architecture can be evaluated by two tests:&lt;/p></description></item></channel></rss>