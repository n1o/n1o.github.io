<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Transformers on Data Artificer and code:Breaker</title><link>https://n1o.github.io/tags/transformers/</link><description>Recent content in Transformers on Data Artificer and code:Breaker</description><generator>Hugo</generator><language>en</language><lastBuildDate>Mon, 28 Oct 2024 10:12:27 +0100</lastBuildDate><atom:link href="https://n1o.github.io/tags/transformers/index.xml" rel="self" type="application/rss+xml"/><item><title>Distilling State Space Models from Transformers</title><link>https://n1o.github.io/posts/distilling-ssm-from-transformers/</link><pubDate>Mon, 28 Oct 2024 10:12:27 +0100</pubDate><guid>https://n1o.github.io/posts/distilling-ssm-from-transformers/</guid><description>&lt;h1 id="abstract">
 Abstract
 &lt;a class="heading-link" href="#abstract">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>It is notoriously expensive to train a Language Model from scratch, making independent research impossible and trying out new architectures extremely risky. Because of these costs, Transformer++ models like LLaMa, based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention, are the de facto standard, not because they are the best, but because they are proven to work! In previous posts I covered &lt;a href="https://n1o.github.io/posts/ssm-transformer-hybrids-guide/" >SSM-Transformer Hybrids&lt;/a>. The biggest benefit of hybridization is their reduced inference cost and minimal memory overhead due to reduced KV cache, and combining SSMs can Attention can make the model more expressive. The biggest obstacle of these models is that they need to be pretrained from scratch. For example &lt;a href="https://www.zyphra.com/post/zamba2-7b" class="external-link" target="_blank" rel="noopener">Zamba2 7B&lt;/a>, is a 7B model that was trained on 128 H100 GPUs for 50 days, bringing its pretraining costs to around 600K US dollars. This makes it one of the cheapest (but by far not weakest) SSM-Attention hybrids, however if we look into detail the model was trained only on 3T (+100B high quality for annealing) tokens. In comparison Llama3 was trained on 15T tokens, if we would apply the same number of tokens to Zamba we would end up with costs around 3M US dollars. It is not hard to see that these kinds of budgets are out of scope for any individual, but also out of scope for many medium sized research organizations and academia.&lt;/p></description></item><item><title>Mamba(2) and Transformer Hybrids: An Overview</title><link>https://n1o.github.io/posts/ssm-transformer-hybrids-guide/</link><pubDate>Wed, 18 Sep 2024 11:26:25 +0200</pubDate><guid>https://n1o.github.io/posts/ssm-transformer-hybrids-guide/</guid><description>&lt;h1 id="abstract">
 Abstract
 &lt;a class="heading-link" href="#abstract">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>We have already looked into &lt;a href="https://n1o.github.io/posts/from-mamba-to-mamba2/" >Mamba and Mamba2&lt;/a>. In terms of efficiency, with their linear complexity and the absence of Key-Value cache, they are a significant improvement over Attention-based models in terms of throughput and memory usage. However, not everything is perfect. Transformers have a certain advantage when it comes to in-context learning. In-context learning is the ability to adapt the model without retraining it. This is done by providing relevant (or not) context to the model in the form of a prompt.&lt;/p></description></item><item><title>BinT5 and HexT5 or T5 and Binary Reverse Engineering</title><link>https://n1o.github.io/posts/t5-and-reverse-engineering/</link><pubDate>Wed, 12 Jun 2024 14:07:17 +0200</pubDate><guid>https://n1o.github.io/posts/t5-and-reverse-engineering/</guid><description>&lt;h1 id="abstract">
 Abstract
 &lt;a class="heading-link" href="#abstract">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>For a while now I have a new passion and that is binary reverse engineering and vulnerability exploitation. This interest has led me to create &lt;a href="https://codebreakers.re" class="external-link" target="_blank" rel="noopener">CodeBreakers&lt;/a>
a platform dedicated to applying machine learning to reverse engineering, vulnerability detection, exploitation, and other cybersecurity-related applications.&lt;/p>
&lt;p>I found two notable research papers where &lt;a href="https://n1o.github.io/posts/t5-the-old-new-thing/" >T5&lt;/a> has been applied to reverse engineering are &lt;a href="https://arxiv.org/abs/2301.01701" class="external-link" target="_blank" rel="noopener">BinT5&lt;/a> and &lt;a href="https://www.semanticscholar.org/paper/HexT5%3A-Unified-Pre-Training-for-Stripped-Binary-Xiong-Chen/04c3fccfe01f42afe18dcdb027385f350ab3c9d1" class="external-link" target="_blank" rel="noopener">HexT5&lt;/a>. Before we dive deep into the details of these papers, let&amp;rsquo;s first explore the basics of reverse engineering.&lt;/p></description></item><item><title>CodeT5 and CodeT5+</title><link>https://n1o.github.io/posts/code-t5-plus/</link><pubDate>Sat, 01 Jun 2024 08:46:43 +0200</pubDate><guid>https://n1o.github.io/posts/code-t5-plus/</guid><description>&lt;h1 id="abstract">
 Abstract
 &lt;a class="heading-link" href="#abstract">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>In a previous post, &lt;a href="https://n1o.github.io/posts/t5-the-old-new-thing/" >T5 the Old New Thing&lt;/a>, we briefly touched upon CodeT5 and CodeT5+. Now, we aim to dive deeper into these topics.&lt;/p>
&lt;p>I have previously explored &lt;a href="https://codebreakers.re/articles/detail/bert-codebert-and-graphcodebert/" class="external-link" target="_blank" rel="noopener">CodeBERT and GraphCodeBERT&lt;/a>. These models, based on &lt;a href="https://arxiv.org/abs/1810.04805" class="external-link" target="_blank" rel="noopener">BERT&lt;/a> and &lt;a href="https://arxiv.org/abs/1907.11692" class="external-link" target="_blank" rel="noopener">RoBERTa&lt;/a> architectures, excel at code understanding and retrieval tasks. However, they fall short when it comes to code generation tasks. It&amp;rsquo;s worth noting that these models share a common theme: they utilize unique pretraining objectives tailored specifically for source code.&lt;/p></description></item><item><title>Longer Context for T5</title><link>https://n1o.github.io/posts/longer-context-for-t5/</link><pubDate>Mon, 29 Apr 2024 14:10:36 +0200</pubDate><guid>https://n1o.github.io/posts/longer-context-for-t5/</guid><description>&lt;h1 id="why-does-t5-need-a-longer-context">
 Why does T5 need a longer context?
 &lt;a class="heading-link" href="#why-does-t5-need-a-longer-context">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>In my previous post
&lt;a href="https://n1o.github.io/posts/t5-the-old-new-thing/" >T5 the Old New Thing&lt;/a>
we already explored why T5 is awesome. But one downside is its limited context length of 512 tokens.
However, it does have a limitation - its context length is restricted to 512 tokens. This can&amp;rsquo;t be directly compared to the context length of a decoder-only model, as T5 is an encoder-decoder model. This means that the encoder can process an input of up to 512 tokens, and the decoder can generate an output of up to 512 tokens, making the total context length 1024 tokens. In this article, we will discuss two extensions:&lt;/p></description></item><item><title>T5 the Old New Thing</title><link>https://n1o.github.io/posts/t5-the-old-new-thing/</link><pubDate>Wed, 06 Mar 2024 12:58:32 +0100</pubDate><guid>https://n1o.github.io/posts/t5-the-old-new-thing/</guid><description>&lt;h1 id="why-t5">
 Why T5
 &lt;a class="heading-link" href="#why-t5">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link to heading">&lt;/i>
 &lt;span class="sr-only">Link to heading&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>A couple of weeks ago I run into the following paper &lt;a href="https://arxiv.org/abs/2402.00841" class="external-link" target="_blank" rel="noopener">Tiny Titans&lt;/a>. It compares multiple smallish (up to 1B parameters) open source LLMs with bigger proprietary ones on meeting summarization. TLDR; the small models tend to perform worse in zero-shot setting as well after fine-tunnig than big ones. Except for FLAN-T5-Large which after finetuning performs way beyond its league, beating even the biggest proprietary models (GPT-3.5).&lt;/p></description></item></channel></rss>