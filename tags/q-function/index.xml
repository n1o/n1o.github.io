<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Q-Function on Data Artificer and code:Breaker</title><link>https://n1o.github.io/tags/q-function/</link><description>Recent content in Q-Function on Data Artificer and code:Breaker</description><generator>Hugo</generator><language>en</language><lastBuildDate>Mon, 03 Mar 2025 08:59:26 +0100</lastBuildDate><atom:link href="https://n1o.github.io/tags/q-function/index.xml" rel="self" type="application/rss+xml"/><item><title>RL Bite: Learning the Q Function</title><link>https://n1o.github.io/posts/rl-bite-learning-the-q-function/</link><pubDate>Mon, 03 Mar 2025 08:59:26 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-learning-the-q-function/</guid><description>Abstract Link to heading We already know how to learn the Value function, however we also know that the Value function by itself is not enough since it averages over all possible actions, instead of taking into consideration specific actions the agent should take. We can derive the Q function from the Value function, however we can also try to directly learn it. Especially directly approximating the Q function with Deep Neural Networks has been a huge success when applied to RL Agents playing Atari computer games.</description></item></channel></rss>