<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>RL Bite on Data Artificer and code:Breaker</title><link>https://n1o.github.io/tags/rl-bite/</link><description>Recent content in RL Bite on Data Artificer and code:Breaker</description><generator>Hugo</generator><language>en</language><lastBuildDate>Tue, 01 Apr 2025 08:44:11 +0200</lastBuildDate><atom:link href="https://n1o.github.io/tags/rl-bite/index.xml" rel="self" type="application/rss+xml"/><item><title>RL Bite: Monotonic Policy Improvement and Deriving Proximal Policy Optimization (PPO)</title><link>https://n1o.github.io/posts/rl-bite-policy-improvement/</link><pubDate>Tue, 01 Apr 2025 08:44:11 +0200</pubDate><guid>https://n1o.github.io/posts/rl-bite-policy-improvement/</guid><description>Abstract Link to heading A while ago we looked into Policy Gradient and Reinforce. Policy gradient is versatile and under mild conditions it is guaranteed to converge to a local minimum (if we choose the correct policy and step size). This is already a huge step up when compared to Q Learning, which may just diverge. However, we may still want stronger guarantees like monotonic improvement at each step.</description></item><item><title>RL Bite: Policy Gradient and Reinforce</title><link>https://n1o.github.io/posts/rl-bite-policy-gradient-and-reinforce/</link><pubDate>Sat, 08 Mar 2025 13:24:13 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-policy-gradient-and-reinforce/</guid><description>Abstract Link to heading Till now we have considered only learning the Value or Q function and estimating the policy from those. In the next few posts, we are going to look into directly learning the policy. Why directly learn the policy? First, Q learning has a lot of issues involving the Deadly Triad; second, if we have continuous actions we cannot really use it; and lastly, Q learning always learns a deterministic policy, and in cases of partially observed stochastic environments (which is nearly always what we have), having a stochastic policy is proven to be better.</description></item><item><title>RL Bite: Learning the Q Function</title><link>https://n1o.github.io/posts/rl-bite-learning-the-q-function/</link><pubDate>Mon, 03 Mar 2025 08:59:26 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-learning-the-q-function/</guid><description>Abstract Link to heading We already know how to learn the Value function, however we also know that the Value function by itself is not enough since it averages over all possible actions, instead of taking into consideration specific actions the agent should take. We can derive the Q function from the Value function, however we can also try to directly learn it. Especially directly approximating the Q function with Deep Neural Networks has been a huge success when applied to RL Agents playing Atari computer games.</description></item><item><title>RL Bite: Computing the Value Function</title><link>https://n1o.github.io/posts/rl-bite-computing-value-functions/</link><pubDate>Tue, 18 Feb 2025 06:20:35 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-computing-value-functions/</guid><description>Abstract Link to heading In the last RL-Bite I wrote about Bellman&amp;rsquo;s Equations and the Value Function and now we will figure out how we actually apply these equations to compute the Value Function!
Known World Model Link to heading Let&amp;rsquo;s start with the simple case, and make an assumption that the underlying World Model of the Markov Decision Process is known, and we have finite discrete states. If we include that the Discount Factor $\gamma &amp;lt; 1$, we can find the Optimal Value function exactly using:</description></item><item><title>RL Bite: Bellmans Equations and Value Functions</title><link>https://n1o.github.io/posts/rl-bite-bellmans-equations-and-value-functions/</link><pubDate>Wed, 12 Feb 2025 07:00:52 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-bellmans-equations-and-value-functions/</guid><description>Value Based Reinforced Learning Link to heading In value based Reinforced Learning we learn a Value Function:
$$ V_{\pi}(s) = E_{\pi}[G_0|s_0 = s] = E_{\pi}[\sum_{t=0}^T \gamma^t r_t|s_0 = s] $$
$G_t$ is the Total Return at time t, this is just the sum of Rewards an Agent gets walking the trajectory T (fancy name but this is just a sequence of actions the agent takes) $\gamma^t$ is the Discount Factor, long story short this is between $&amp;lt;0,1&amp;gt;$, the closer this value is to zero, the more the agent will focus on the immediate reward, the closer it is to 1 the more it will take future rewards into account.</description></item><item><title>RL Bite: Exploitation vs Exploration</title><link>https://n1o.github.io/posts/rl-bite-exploration-vs-exploitation/</link><pubDate>Wed, 05 Feb 2025 09:51:24 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-exploration-vs-exploitation/</guid><description>Explore vs Exploit Link to heading In reinforcement learning, we have an agent that has to take actions, for which it receives a reward. Here we have a dilemma: do we choose an action that gives the biggest reward or do we explore new actions that may lead to regions with even higher payouts?
Greedy Policy Link to heading This is a simple one, we always take the option that gives us the highest payout:</description></item></channel></rss>