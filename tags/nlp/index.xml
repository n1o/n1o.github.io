<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP on Data, Code and Breaking Stuff</title><link>https://n1o.github.io/tags/nlp/</link><description>Recent content in NLP on Data, Code and Breaking Stuff</description><generator>Hugo</generator><language>en</language><lastBuildDate>Thu, 08 Aug 2024 09:57:32 +0200</lastBuildDate><atom:link href="https://n1o.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>From Mamba to Mamba-2</title><link>https://n1o.github.io/posts/from-mamba-to-mamba2/</link><pubDate>Thu, 08 Aug 2024 09:57:32 +0200</pubDate><guid>https://n1o.github.io/posts/from-mamba-to-mamba2/</guid><description>Abstract Link to heading This is not my first gig where I write about State Space Models. I already mentioned them here and here. Now what is the deal with this Mamba(2) thing? They are proving to be an alternative to the strong Transformer++ architecture (Transformer++ models like LLaMa are based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention). Hold on, if this Transformer++ models work well, why do we need altneratives?</description></item><item><title>Butterflies, Monarchs, Hyenas, and Lightning Fast BERT</title><link>https://n1o.github.io/posts/butterflies-monarchs-hyenas-and-lightning-fast-bert/</link><pubDate>Fri, 12 Jul 2024 13:36:49 +0200</pubDate><guid>https://n1o.github.io/posts/butterflies-monarchs-hyenas-and-lightning-fast-bert/</guid><description>Abstract Link to heading I have been working on a project of creating my own Large Language Model, as I am huge fan of T5, or to be more concrete I recognize the added value of having an Encoder-Decoder architecture. The biggest challenge, at least in my opinion, in training an LLM is the sheer computational costs required to do so. I was originally planning to take the Encoder introduced by ColT5 but than I came across of M2 BERT and suddenly I went down the rabbit hole of Structured Matrices, Butterflies, Monarch and Hyeans.</description></item><item><title>BinT5 and HexT5 or T5 and Binary Reverse Engineering</title><link>https://n1o.github.io/posts/t5-and-reverse-engineering/</link><pubDate>Wed, 12 Jun 2024 14:07:17 +0200</pubDate><guid>https://n1o.github.io/posts/t5-and-reverse-engineering/</guid><description>Abstract Link to heading For a while now I have a new passion and that is binary reverse engineering and vulnerability exploitation. This interest has led me to create CodeBreakers a platform dedicated to applying machine learning to reverse engineering, vulnerability detection, exploitation, and other cybersecurity-related applications.
I found two notable research papers where T5 has been applied to reverse engineering are BinT5 and HexT5. Before we dive deep into the details of these papers, let&amp;rsquo;s first explore the basics of reverse engineering.</description></item><item><title>CodeT5 and CodeT5+</title><link>https://n1o.github.io/posts/code-t5-plus/</link><pubDate>Sat, 01 Jun 2024 08:46:43 +0200</pubDate><guid>https://n1o.github.io/posts/code-t5-plus/</guid><description>Abstract Link to heading In a previous post, T5 the Old New Thing, we briefly touched upon CodeT5 and CodeT5+. Now, we aim to dive deeper into these topics.
I have previously explored CodeBERT and GraphCodeBERT. These models, based on BERT and RoBERTa architectures, excel at code understanding and retrieval tasks. However, they fall short when it comes to code generation tasks. It&amp;rsquo;s worth noting that these models share a common theme: they utilize unique pretraining objectives tailored specifically for source code.</description></item><item><title>Longer Context for T5</title><link>https://n1o.github.io/posts/longer-context-for-t5/</link><pubDate>Mon, 29 Apr 2024 14:10:36 +0200</pubDate><guid>https://n1o.github.io/posts/longer-context-for-t5/</guid><description>Why does T5 need a longer context? Link to heading In my previous post T5 the Old New Thing we already explored why T5 is awesome. But one downside is its limited context length of 512 tokens. However, it does have a limitation - its context length is restricted to 512 tokens. This can&amp;rsquo;t be directly compared to the context length of a decoder-only model, as T5 is an encoder-decoder model.</description></item><item><title>T5 the Old New Thing</title><link>https://n1o.github.io/posts/t5-the-old-new-thing/</link><pubDate>Wed, 06 Mar 2024 12:58:32 +0100</pubDate><guid>https://n1o.github.io/posts/t5-the-old-new-thing/</guid><description>Why T5 Link to heading A couple of weeks ago I run into the following paper Tiny Titans. It compares multiple smallish (up to 1B parameters) open source LLMs with bigger proprietary ones on meeting summarization. TLDR; the small models tend to perform worse in zero-shot setting as well after fine-tunnig than big ones. Except for FLAN-T5-Large which after finetuning performs way beyond its league, beating even the biggest proprietary models (GPT-3.</description></item><item><title>Paper overview: Hungry Hungry Hippos: Towards Language Modeling with State Space Models</title><link>https://n1o.github.io/posts/hungry-hungry-hippos/</link><pubDate>Mon, 06 Feb 2023 09:39:03 +0100</pubDate><guid>https://n1o.github.io/posts/hungry-hungry-hippos/</guid><description>High level overview Link to heading By combining State Space Models (SSMs) with Attention, we get a model that generates text more efficiently, with a speed increase of approximately 1.6 times. Additionally, this approach requires less paremters, enabling the development of larger models on existing hardware.
Language modeling requirements Link to heading The Transformer architecture, which forms the basis of ChatGPT, is riding high on the hype train due to its impressive performance.</description></item></channel></rss>