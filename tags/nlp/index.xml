<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP on Data, Code and Breaking Stuff</title><link>https://n1o.github.io/tags/nlp/</link><description>Recent content in NLP on Data, Code and Breaking Stuff</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 06 Feb 2023 09:39:03 +0100</lastBuildDate><atom:link href="https://n1o.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Paper overview: Hungry Hungry Hippos: Towards Language Modeling with State Space Models</title><link>https://n1o.github.io/posts/hungry-hungry-hippos/</link><pubDate>Mon, 06 Feb 2023 09:39:03 +0100</pubDate><guid>https://n1o.github.io/posts/hungry-hungry-hippos/</guid><description>High level overview Link to heading By combining State Space Models (SSMs) with Attention, we get a model that generates text more efficiently, with a speed increase of approximately 1.6 times. Additionally, this approach requires less paremters, enabling the development of larger models on existing hardware.
Language modeling requirements Link to heading The Transformer architecture, which forms the basis of ChatGPT, is riding high on the hype train due to its impressive performance.</description></item></channel></rss>