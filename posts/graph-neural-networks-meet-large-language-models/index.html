<!doctype html><html lang=en><head><title>Graph Neural Networks meet Large Language Models · Data Artificer and code:Breaker
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="n1o_c0rTx"><meta name=description content="Abstract Link to heading I am a huge fan of Graph Neural Networks (GNNs), and I am (a bit less) a fan of Large Language Models (LLMs), however they are hard to ignore. Both have different strengths, while GNNs excel when it comes to problems that have an inherent structure, LLMs thrive in cases where we treat everything as a sequence of tokens (maybe Bytes in the future). A natural question arises, what if we can combine these two?"><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="Graph Neural Networks meet Large Language Models"><meta name=twitter:description content="Abstract Link to heading I am a huge fan of Graph Neural Networks (GNNs), and I am (a bit less) a fan of Large Language Models (LLMs), however they are hard to ignore. Both have different strengths, while GNNs excel when it comes to problems that have an inherent structure, LLMs thrive in cases where we treat everything as a sequence of tokens (maybe Bytes in the future). A natural question arises, what if we can combine these two?"><meta property="og:url" content="https://n1o.github.io/posts/graph-neural-networks-meet-large-language-models/"><meta property="og:site_name" content="Data Artificer and code:Breaker"><meta property="og:title" content="Graph Neural Networks meet Large Language Models"><meta property="og:description" content="Abstract Link to heading I am a huge fan of Graph Neural Networks (GNNs), and I am (a bit less) a fan of Large Language Models (LLMs), however they are hard to ignore. Both have different strengths, while GNNs excel when it comes to problems that have an inherent structure, LLMs thrive in cases where we treat everything as a sequence of tokens (maybe Bytes in the future). A natural question arises, what if we can combine these two?"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-12-16T09:17:05+01:00"><meta property="article:modified_time" content="2024-12-16T09:17:05+01:00"><link rel=canonical href=https://n1o.github.io/posts/graph-neural-networks-meet-large-language-models/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://n1o.github.io/>Data Artificer and code:Breaker
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Writing</a></li><li class=navigation-item><a class=navigation-link href=/awesome-t5/>Awesome T5</a></li><li class=navigation-item><a class=navigation-link href=/awesome-ssm/>Awesome SSM</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/graph-neural-networks-meet-large-language-models/>Graph Neural Networks meet Large Language Models</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2024-12-16T09:17:05+01:00>December 16, 2024
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
9-minute read</span></div></div></header><div class=post-content><h1 id=abstract>Abstract
<a class=heading-link href=#abstract><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>I am a huge fan of Graph Neural Networks (GNNs), and I am (a bit less) a fan of Large Language Models (LLMs), however they are hard to ignore. Both have different strengths, while GNNs excel when it comes to problems that have an inherent structure, LLMs thrive in cases where we treat everything as a sequence of tokens (maybe Bytes in the future). A natural question arises, what if we can combine these two? It turns out yes, initially I encountered the <a href=https://arxiv.org/abs/2309.15427 class=external-link target=_blank rel=noopener>Graph Neural Prompting</a> paper during my <a href=https://n1o.github.io/awesome-t5/ class=external-link target=_blank rel=noopener>T5</a> journey (still not finished, just making a couple of detours) which is a cool idea on how to merge knowledge graphs and T5. As it turns out there is already comprehensive research done on how to merge GNNs and LLMs in the following paper: <a href=https://arxiv.org/abs/2311.12399 class=external-link target=_blank rel=noopener>A Survey of Graph Meets Large Language Model: Progress and Future Directions</a> and its companion github repo: <a href=https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks class=external-link target=_blank rel=noopener>Awesome-LLMs-in-Graph-tasks</a> and I decided to create this blog post to clarify how the merging of these two technologies is done.</p><h1 id=why-to-combine-gnns-and-llms-or-llms-and-gnns>Why to combine GNNs and LLMs (or LLMs and GNNs)
<a class=heading-link href=#why-to-combine-gnns-and-llms-or-llms-and-gnns><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p><img src=/images/llms-in-graphs.png></p><p>To answer this question we start by asking two additional ones:</p><ol><li>How can LLM help with a Graph problem?</li><li>How can GNN help an LLM?</li></ol><p>To answer those questions let us first analyze the pros/cons of both GNNs and LLMs. GNNs excel when it comes to capturing structural information, this can be the structure of a document, images on the side, text divided into different sections, or source code, which has an inherent graph/tree like structure (I already did write about <a href=https://codebreakers.re/articles/llm-and-security/galla-graph-aligned-llm class=external-link target=_blank rel=noopener>GALa</a> which merges GNN and LLM for Source code understanding and Generation), however they need a way to capture semantic information into their initial node embeddings. On the other hand, LLMs excel at capturing semantic information of text (or source code), however they fail (were not designed to is a better word) to capture complex hierarchical dependencies.</p><p>Just from the description above, we can clearly see a simple, but efficient solution where we use LLMs to generate the initial node embeddings for GNNs. Indeed this is powerful, and it is a widely used and somewhat proven approach. However it has a shortcoming, we only use the LLM to enhance the GNN, however with this we hardly use the LLM to its full potential.</p><h1 id=gnns-and-pretraining>GNNs and Pretraining
<a class=heading-link href=#gnns-and-pretraining><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>With the rise of Pretrained Language Models (PLM) and their ease of adapting them to task specific problems (transfer learning) it is just natural that research tried to achieve something similar with GNNs, however this story is somewhat different and way more complicated to achieve. In GNNs we nearly always are interested in node embeddings, we start with some initial and after N rounds of training and given the actual graph structure and model layers we end up with some final embeddings. The final embeddings then can be used for node classification, where we put an additional model on top of the node embeddings, for link prediction, where we take a pair of nodes and their embeddings and use again an additional model to produce a binary variable if there is a link (if we would have different link types we can go with softmax (for mutually exclusive)). For Graph classification as whole the standard solution is to pool all the node embeddings to generate one graph embedding, the pooling can be taking an average over all embeddings, to more complicated hierarchical embeddings.</p><p>For pretraining GNNs there are two common approaches:</p><ol><li><strong>Reconstruction</strong>, this is a classical approach where we have an encoder that embeds the graph into a latent representation, and a decoder that learns to reconstruct the original graph from the embedding, here are some examples:<ul><li><a href=https://arxiv.org/abs/2205.10803 class=external-link target=_blank rel=noopener>GraphMAE</a>, which uses a Masked Graph Autoencoder</li><li><a href=https://www.semanticscholar.org/paper/S2GAE%3A-Self-Supervised-Graph-Autoencoders-are-with-Tan-Liu/355cf4ef8c666898ceed76ea7950c3df176900fc class=external-link target=_blank rel=noopener>S2GAE</a>, again leverages a Graph Autoencoder but uses it to mask and reconstruct edges</li><li><a href=https://arxiv.org/abs/2406.05391 class=external-link target=_blank rel=noopener>DUPLEX</a> is an example for directed graphs, we learn a complex embedding, where the real part is used to encode the existence of an edge and the imaginary part the orientation of this edge</li></ul></li><li><strong>Contrastive objective</strong>, this is very much the same as any contrastive learning objective, we try to create graph embeddings that for similar graphs are close, we can then later leverage the embeddings for downstream tasks</li></ol><h1 id=gnns--llms--better-together>GNNs + LLMs = Better Together
<a class=heading-link href=#gnns--llms--better-together><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>There are 3 main approaches to combine LLMs and GNNs (or GNNs with LLMs):</p><ol><li>LLM as an Enhancer</li><li>LLM as a Predictor</li><li>GNN-LLM Alignment</li></ol><h2 id=llm-as-an-enhancer>LLM as an Enhancer
<a class=heading-link href=#llm-as-an-enhancer><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p><img src=/images/llm-as-an-enhancer.png>
We already explored this approach a bit in the part where I talked about using LLMs generating the initial node embeddings, however there is more to it and we are going to explore it in more detail. In this setting we always work with Text Attributed Graphs (TAG), these are graphs that have text node features.</p><h3 id=embedding-based-enhancement>Embedding based enhancement
<a class=heading-link href=#embedding-based-enhancement><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Yes, you guessed right, we use an LLM to generate a node embedding! However there are some extra details we need to explore. First we may not want to embed the whole text, but we may decide to extract only relevant information, this approach is explored in depth in the <a href=https://arxiv.org/pdf/2309.02848 class=external-link target=_blank rel=noopener>G-PROMPT</a> paper. Also we need to consider if we train the embedding model, this can be beneficial since it may learn to extract better features, on the other hand, it is way more resource hungry.</p><h3 id=explanation-based-enhancement>Explanation based enhancement
<a class=heading-link href=#explanation-based-enhancement><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Technically this also is an embedding based enhancement, but we do not embed the actual text itself but an explanation of the text that has been produced by a language model. Why would you do that? It is simple - we can use proprietary language models to generate the explanation. Is this actually a good approach? Well yes and no. Yes since the proprietary language models tend to be excellent and they may provide extra information, but no since it can balloon up the costs significantly, since most API based LLMs are expensive.</p><h2 id=llm-as-a-predictor>LLM as a Predictor
<a class=heading-link href=#llm-as-a-predictor><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p><img src=/images/llm-as-predictor.png></p><p>Here we want to use LLM to make predictions about graph related tasks. This is a very active area of research, and overall I believe it is one that makes most sense, especially the GNN-based predictions approach.</p><h3 id=gnn-based-predictions>GNN-based Predictions
<a class=heading-link href=#gnn-based-predictions><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Here we have a GNN model which produces node embeddings, we then take these node embeddings and embed them as tokens for an LLM. Before we go into examples let&rsquo;s face the biggest problem of this approach: It can be used only with open weight models! Again a reason why I believe open source LLMs will dominate in the future, they are the most flexible and they can be used in scenarios that they weren&rsquo;t originally designed for. As for the models here are some:</p><ul><li>GALLa, already mentioned somewhere above, it uses DUPLEX to generate node embeddings of a source code snippet, which are then passed to an LLM which can be then used to answer source code related questions, or even improve code translation (Python -> Java, Java -> Python) performance</li><li>Graph Neural Prompting, again mentioned somewhere above</li><li><a href=https://www.semanticscholar.org/paper/Best-of-Both-Worlds%3A-Advantages-of-Hybrid-Graph-Behrouz-Parviz/876d88fbe0563544ce8761026b2333db71668c89 class=external-link target=_blank rel=noopener>Best of Both worlds</a>, a rather recent paper, where the authors split a graph into subgraphs which are embedded, later they use an Attention-SSM hybrid to process this sequence</li></ul><h3 id=flatten-based-predictions>Flatten-based Predictions
<a class=heading-link href=#flatten-based-predictions><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>This is a somewhat less compelling alternative to GNN-based predictions, and the whole idea is to take a graph and represent it as, well text. So there is this Graph Markup language called <a href=https://en.wikipedia.org/wiki/Graph_Modelling_Language class=external-link target=_blank rel=noopener>Graph Modeling Language (GML)</a>, which we can directly use. A somewhat alternative approach is to use a model like <a href=https://codebreakers.re/articles/llm-and-security/bert-codebert-and-graphcodebert class=external-link target=_blank rel=noopener>GraphCodeBERT</a>, which encodes graph structure as extra tokens with a specially designed attention mask.</p><p>Modeling graphs with GML has one upside, it can be used with any LLM also proprietary, however embedding large graphs will be a challenge, since it will require a lot of tokens, and still the relationships are human readable, but understanding complex hierarchies and structures will be challenging even for powerful LLMs.</p><h4 id=remark-to-special-attention-mask>Remark to special Attention Mask
<a class=heading-link href=#remark-to-special-attention-mask><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>Since Coding LLMs have a special place in my heart, I ran across <a href=https://www.semanticscholar.org/paper/CodeSAM%3A-Source-Code-Representation-Learning-by-Mathai-Sedamaki/1bff70f012e7f6a3fc868e1b12256dc44f79c7f3 class=external-link target=_blank rel=noopener>CodeSAM</a>, where the authors build on top of the idea introduced in GraphCodeBERT, with minor modifications. They do not introduce special tokens encoding the Control Flow between variables, but they instead modify the attention mask. Actually they leverage two different attention masks, one that captures the AST and one for the CFD, and they alternate between these two across different layers. Very cool idea!</p><h2 id=gnn-llm-alignment>GNN-LLM Alignment
<a class=heading-link href=#gnn-llm-alignment><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p><img src=/images/gnn_llm_aligment.png></p><p>The high-level idea is to align (for example, minimize the distance between) the embeddings of both the GNN and the LLM. Why is this useful? Imagine you train an LLM and a GNN at the same time and align their hidden representations - the GNN will try to capture the hierarchical representation while the LLM focuses on the semantics. There will be some information sharing, where hierarchical information will bleed through into the LLM and some semantics will be captured by the GNN. In general, we have two main classes of alignment:</p><ol><li><strong>Symmetric</strong>, where both LLM and GNN are equally important</li><li><strong>Asymmetric</strong>, where one modality plays support to the other</li></ol><h3 id=symmetric>Symmetric
<a class=heading-link href=#symmetric><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Here we can split into two cases:</p><ol><li>Naive Concatenation, which is somewhat anticlimactic - we just train two separate models and concatenate their resulting modalities. With this late fusion, we have no information exchange between models prior to the concatenation.</li><li>Contrastive objective, which is the canonical example of GNN-LLM alignment, where we pull the individual embeddings together in their latent representations. The canonical paper is <a href=https://arxiv.org/abs/2305.14321 class=external-link target=_blank rel=noopener>ConGraT</a>, in which the authors work with Text Attributed Graphs. They use contrastive pretraining to align the latent space of any GNN with any LLM (Encoder only or Decoder Only) and later use it for downstream tasks like node classification, link prediction, Community Detection and Language Modeling where they leverage only the aligned LLM.</li></ol><h3 id=asymmetric>Asymmetric
<a class=heading-link href=#asymmetric><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>We have 3 main cases:</p><ol><li><strong>Graph Nested Transformers</strong>, where the canonical example is <a href=https://arxiv.org/abs/2105.02605 class=external-link target=_blank rel=noopener>Graph-former</a>. Here we embed a GNN into each transformer layer, where the role of the GNN is to further massage the output of the feed-forward networks.</li><li><strong>Graph Aware Distillation</strong>, which we can understand by looking at <a href=https://arxiv.org/abs/2304.10668 class=external-link target=_blank rel=noopener>Grad</a>. It consists of a GNN teacher model whose responsibility is to generate soft-labels for an LLM. We share parameters between the teacher GNN and the student LLM. Parameter sharing results in information bleeding between the GNN and LLM, forcing the LLM to pick up structural information and the GNN to better handle semantic information.</li><li><strong>Iterative Updates</strong>, demonstrated in <a href=https://arxiv.org/abs/2310.12580 class=external-link target=_blank rel=noopener>THLM</a>. Here we have a heterogeneous GNN that enhances an LLM, each pretrained using different strategies and producing labels for the other. After pretraining is done, we discard the GNN and finetune the LLM for graph-aware tasks, leveraging the information gained during joint pretraining. We can clearly see that the GNN plays a supporting role.</li></ol><h1 id=final-remarks>Final Remarks
<a class=heading-link href=#final-remarks><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>This just scratches the surface. If you are interested, I recommend visiting <a href=https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks class=external-link target=_blank rel=noopener>Awesome-LLMs-in-Graph-tasks</a> on Github. An interesting new approach is combining GNNs with LLM Agents for planning: <a href=https://arxiv.org/abs/2405.19119 class=external-link target=_blank rel=noopener>Can Graph Learning Improve Planning in LLM-based Agents?</a>. Also follow <a href=https://logconference.org/ class=external-link target=_blank rel=noopener>Learning On Graph Conference</a> - they do a great job informing the public about the latest research in the field of GNNs, and since Language Models cannot be ignored, there tends to be significant overlap.</p></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2020 -
2024
n1o_c0rTx
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ")}</script></body></html>