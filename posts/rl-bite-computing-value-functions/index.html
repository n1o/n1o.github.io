<!doctype html><html lang=en><head><title>RL Bite: Computing the Value Function · Data Artificer and code:Breaker
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="n1o_c0rTx"><meta name=description content="Abstract Link to heading In the last RL-Bite I wrote about Bellman&rsquo;s Equations and the Value Function and now we will figure out how we actually apply these equations to compute the Value Function!
Known World Model Link to heading Let&rsquo;s start with the simple case, and make an assumption that the underlying World Model of the Markov Decision Process is known, and we have finite discrete states. If we include that the Discount Factor $\gamma < 1$, we can find the Optimal Value function exactly using:"><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="RL Bite: Computing the Value Function"><meta name=twitter:description content="Abstract Link to heading In the last RL-Bite I wrote about Bellman’s Equations and the Value Function and now we will figure out how we actually apply these equations to compute the Value Function!
Known World Model Link to heading Let’s start with the simple case, and make an assumption that the underlying World Model of the Markov Decision Process is known, and we have finite discrete states. If we include that the Discount Factor $\gamma < 1$, we can find the Optimal Value function exactly using:"><meta property="og:url" content="https://n1o.github.io/posts/rl-bite-computing-value-functions/"><meta property="og:site_name" content="Data Artificer and code:Breaker"><meta property="og:title" content="RL Bite: Computing the Value Function"><meta property="og:description" content="Abstract Link to heading In the last RL-Bite I wrote about Bellman’s Equations and the Value Function and now we will figure out how we actually apply these equations to compute the Value Function!
Known World Model Link to heading Let’s start with the simple case, and make an assumption that the underlying World Model of the Markov Decision Process is known, and we have finite discrete states. If we include that the Discount Factor $\gamma < 1$, we can find the Optimal Value function exactly using:"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-18T06:20:35+01:00"><meta property="article:modified_time" content="2025-02-18T06:20:35+01:00"><meta property="article:tag" content="RL Bite"><meta property="article:tag" content="Value Based RL"><meta property="article:tag" content="Temporal Difference"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-policy-gradient-and-reinforce/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-learning-the-q-function/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-bellmans-equations-and-value-functions/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-exploration-vs-exploitation/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-bellmans-equations-and-value-functions/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-policy-gradient-and-reinforce/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-learning-the-q-function/"><link rel=canonical href=https://n1o.github.io/posts/rl-bite-computing-value-functions/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://n1o.github.io/>Data Artificer and code:Breaker
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Writing</a></li><li class=navigation-item><a class=navigation-link href=/awesome-t5/>Awesome T5</a></li><li class=navigation-item><a class=navigation-link href=/awesome-ssm/>Awesome SSM</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/rl-bite-computing-value-functions/>RL Bite: Computing the Value Function</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2025-02-18T06:20:35+01:00>February 18, 2025
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
6-minute read</span></div><div class=categories><i class="fa-solid fa-folder" aria-hidden=true></i>
<a href=/categories/rl-bite/>RL Bite</a>
<span class=separator>•</span>
<a href=/categories/value-based-rl/>Value Based RL</a>
<span class=separator>•</span>
<a href=/categories/temporal-difference/>Temporal Difference</a></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/rl-bite/>RL Bite</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/value-based-rl/>Value Based RL</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/temporal-difference/>Temporal Difference</a></span></div></div></header><div class=post-content><h1 id=abstract>Abstract
<a class=heading-link href=#abstract><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>In the last <a href=/categories/rl-bite/>RL-Bite</a> I wrote about <a href=/posts/rl-bite-bellmans-equations-and-value-functions/>Bellman&rsquo;s Equations and the Value Function</a> and now we will figure out how we actually apply these equations to compute the Value Function!</p><h1 id=known-world-model>Known World Model
<a class=heading-link href=#known-world-model><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Let&rsquo;s start with the simple case, and make an assumption that the underlying World Model of the Markov Decision Process is known, and we have finite discrete states. If we include that the Discount Factor $\gamma &lt; 1$, we can find the Optimal Value function exactly using:</p><ul><li>Dynamic Programming</li><li>Linear Programming</li></ul><p>However, the computational complexity in both cases is polynomial in the size of possible states and actions and we may struggle because of the Curse of Dimensionality if this space is too large. In these cases we need to use approximate variants: Approximate Dynamic Programming or Approximate Linear Programming.</p><h2 id=value-iteration>Value Iteration
<a class=heading-link href=#value-iteration><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>At the core, Value Iteration leverages Dynamic Programming, the update equation is:</p><p>$$ V_{k+1}(s) = \max_a [ R(s, a) + \gamma \sum_{s^{\prime}} p(s^{\prime}|s, a) V_k(s^{\prime}) ]$$</p><p>Where we apply Bellman&rsquo;s Backup, with $V_k$ the current estimate of the Value function.</p><p>The benefit of this approach is that it <strong>contracts</strong>, which means that with each iteration the error decreases:</p><p>$$ \max_s |V_{k+1}(s) - V_{\star}(s)| \leq \gamma \max_s |V_k(s) - V_{\star}(s)|$$</p><p>In practice we stop if we get close enough to the Optimal Value Function, and again we reach for the Bellman&rsquo;s Equations to extract the Optimal Policy.</p><h2 id=real-time-dynamic-programming-rtdp>Real-time dynamic programming (RTDP)
<a class=heading-link href=#real-time-dynamic-programming-rtdp><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Real-time Dynamic Programming is a modification of Value Iteration, where we consider only a subset of states we are interested in:</p><p>$$ V_{k+1}(s) = \max_a E_{p_s(s^{\prime}|s,a)}[ R(s, a) + \gamma V_k(s^{\prime}) ]$$</p><ul><li>here we have $p_s(s^{\prime}|s,a)$ this is just the transition matrix of the MDP</li><li>$R(s,a) = -1$ for the states we are not interested in</li></ul><h2 id=policy-iteration>Policy Iteration
<a class=heading-link href=#policy-iteration><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Again a Dynamic Programming approach to compute the optimal policy $\pi_{\star}$ by searching the space of deterministic policy where we repeat two steps until we converge. These steps are:</p><ol><li><strong>Policy Evaluation</strong>
The idea is to compute the value function for the current policy $\pi$ as $v(s) = V_{\pi}(s)$, here $v$ is a vector indexed by state, $r(s) = \sum_{a} \pi(a|s)R(s,a)$ is the reward vector and $T(s^{\prime}|s) = \sum_{a}\pi(a|s)p(s^{\prime}|s,a)$ is the state transition matrix. With this we express the Bellman&rsquo;s Optimality Equations in matrix form:</li></ol><p>$$ v = r + \gamma Tv $$</p><p>This is a system of equations with $|S|$ unknowns and we can solve it by using matrix inversion:</p><p>$$v = (I - \gamma T)^{-1} r$$</p><p>Alternatively we can use Value Iteration $v_{t+1} = r + \gamma T v_{t}$ until it converges</p><ol start=2><li><strong>Policy Improvement</strong></li></ol><p>In this step we have a policy $\pi$ with a value function $V_{\pi}$, this can be used to derive a better policy $\pi^{\prime}$ as follows:</p><p>$$ \pi^{\prime}(s) = \arg \max_{a}{ R(s,a) + \gamma E[V_{\pi}(s^{\prime})] }$$</p><p>Given the <strong>Policy Improvement Theorem</strong> we are guaranteed that $V_{\pi^{\prime}} \ge V_{\pi}$</p><h3 id=summary>Summary
<a class=heading-link href=#summary><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>The process can be summarized as:
$$ \pi_0 \xrightarrow{E} V_{\pi_0} \xrightarrow{I} \pi_1 \xrightarrow{E} V_{\pi_1} \dots \xrightarrow{I} \pi_* \xrightarrow{E} V_*$$</p><h2 id=comparison-of-vi-and-policy-iteration>Comparison of VI and Policy Iteration
<a class=heading-link href=#comparison-of-vi-and-policy-iteration><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Policy Iteration and Value Iteration are extremely similar, the difference is that in Policy Evaluation we average over all actions according to the policy, while in VI we take the maximum over all actions, the following 2mage gives a nice summary:</p><p><img src=/images/policy_interation_vs_value_iteration.png></p><h1 id=unknown-world-model>Unknown World Model
<a class=heading-link href=#unknown-world-model><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Until now the Agent had access to the underlying world model, here we can agree, that this is not realistic and we will focus on the case where the Agent only observes samples produced by the environment.</p><p>$$(s&rsquo; , r) \sim p(s&rsquo;,r|s,a) $$</p><p>Luckily for us this is enough to learn an Optimal Value Function.</p><h2 id=monte-carlo-estimate>Monte Carlo Estimate
<a class=heading-link href=#monte-carlo-estimate><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>This can be also known as <strong>policy rollout</strong>, and the idea is that we take actions according to the policy (until we reach an absorbing state, alternatively the discount factor forces the rewards to zero), until we reach an end, and average out the discounted rewards. We use this average to update the estimated value function:</p><p>$$V(s_t) \leftarrow V(s_t) + \eta [G_t - V(s_t)]$$</p><h3 id=high-variance>High Variance
<a class=heading-link href=#high-variance><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>As with other Monte Carlo methods they tend to have High Variance. Why is this the case? First each time we unroll a trajectory we work with a stochastic state transition function, and we need to sum up a lot of random rewards. This process is then repeated multiple times and we take an average at the end. Summing and averaging a lot of random variables is noisy!</p><h2 id=temporal-difference-td-learning>Temporal Difference (TD) Learning
<a class=heading-link href=#temporal-difference-td-learning><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>This is an alternative to Monte Carlo Estimate, that promises to be more efficient. The idea is that we incrementally reduce the Bellman&rsquo;s Error each time by making a single state transition: $(s_t, a_t, r_t, s_{t+1})$ with $a_t \sim \pi_{s_t}$. This can be used to estimate $V(s_t)$ as:</p><p>$$ V(s_t) \leftarrow V(s_t) + \eta [r_t + \gamma V(s_{t+1}) - V(s_t)]$$</p><ul><li>$q_t = r_t + \gamma V(s_{t+1}) \approx G_{t:t+1}$ - this is the one step Reward To Go</li><li>$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) = q_t - V(s_t)$ this is the <strong>TD Error</strong></li></ul><h3 id=parametric-form>Parametric Form
<a class=heading-link href=#parametric-form><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>We can reexpress the equation above as:</p><p>$$ w \leftarrow w + \eta [r_t + \gamma V_w(s_{t+1}) - V_w(s_t)] \nabla_w V_w(s_t)$$</p><p>From this it should be obvious that it is a special case of Monte Carlo Estimate, where we do just one step.
And it looks like Gradient Descent but it is not, it is <strong>bootstrap</strong>.</p><h3 id=convergence>Convergence
<a class=heading-link href=#convergence><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>There is some good news: for the tabular case this is guaranteed to converge to the correct value function. The bad news is that if we use non-linear function approximators it may diverge.</p><h3 id=connection-to-bellmans-backup>Connection to Bellman&rsquo;s Backup
<a class=heading-link href=#connection-to-bellmans-backup><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>TD learning is a technique that uses sampled transitions to approximate the Bellman&rsquo;s Backup. Giving us an approximate way to achieve the optimal Value Function.</p><h2 id=combination-of-mc-and-td>Combination of MC and TD
<a class=heading-link href=#combination-of-mc-and-td><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>It is evident that MC and TD are very similar, where TD is essentially MC but with only one step look ahead and we can combine these approaches, by approximating the <strong>n-step return</strong> as:</p><p>$$G_{t:t+n} = r_t + \gamma r_{t+1} + \dots + \gamma^{n-1} r_{t+n-1} + \gamma^n V(s_{t+n}) $$</p><p>The update rule for TD becomes:</p><p>$$ w \leftarrow w + \eta [G_{t:t+n} - V_w(s_t)] \nabla_w V_w(s_t)$$</p><h3 id=tdlambda>TD($\lambda$)
<a class=heading-link href=#tdlambda><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>This is a special case for MC + TD, where we do not specify how far we want to do a lookup but we take a weighted average of all possible values, this average is geometric, so we can sum even infinite long lookaheads:</p><p>$$G_t^\lambda = (1 - \lambda) \sum_{n=1}^\infty \lambda^{n-1} G_{t:t+n}$$</p><ul><li>$\lambda \in [0,1]$<ul><li>if we set it to $0$ we get plain TD learning</li><li>if we set it to $1$ we get regular MC Estimation</li></ul></li></ul></div><footer><section class=see-also></section><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2020 -
2025
n1o_c0rTx
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ")}</script></body></html>