<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Artificial Intelligence and Machine Learning Research</title><link>https://n1o.github.io/posts/</link><description>Recent content in Posts on Artificial Intelligence and Machine Learning Research</description><generator>Hugo</generator><language>en</language><lastBuildDate>Thu, 10 Apr 2025 09:00:56 +0200</lastBuildDate><atom:link href="https://n1o.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>RL Bite: Monte Carlo Search Tree</title><link>https://n1o.github.io/posts/rl-bite-monte-carlo-search-tree/</link><pubDate>Thu, 10 Apr 2025 09:00:56 +0200</pubDate><guid>https://n1o.github.io/posts/rl-bite-monte-carlo-search-tree/</guid><description>Abstract Link to heading Let&amp;rsquo;s talk a bit about Model-Based Reinforcement Learning. The idea is that our RL Agent not just learns a policy to follow or/and a value function (Q function) but also tries to model the environment it is in. This is done by learning the transition dynamics $p(s&amp;rsquo;|s,a)$ (also known as World Model) and a Reward function $\hat{R}(s,a)$. Once we have our world model, we can use it to simulate data and learn the model&amp;rsquo;s policy on the simulations.</description></item><item><title>RL Bite: Monotonic Policy Improvement and Deriving Proximal Policy Optimization (PPO)</title><link>https://n1o.github.io/posts/rl-bite-policy-improvement/</link><pubDate>Tue, 01 Apr 2025 08:44:11 +0200</pubDate><guid>https://n1o.github.io/posts/rl-bite-policy-improvement/</guid><description>Abstract Link to heading A while ago we looked into Policy Gradient and Reinforce. Policy gradient is versatile and under mild conditions it is guaranteed to converge to a local minimum (if we choose the correct policy and step size). This is already a huge step up when compared to Q Learning, which may just diverge. However, we may still want stronger guarantees like monotonic improvement at each step.</description></item><item><title>RL Bite: Policy Gradient and Reinforce</title><link>https://n1o.github.io/posts/rl-bite-policy-gradient-and-reinforce/</link><pubDate>Sat, 08 Mar 2025 13:24:13 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-policy-gradient-and-reinforce/</guid><description>Abstract Link to heading Till now we have considered only learning the Value or Q function and estimating the policy from those. In the next few posts, we are going to look into directly learning the policy. Why directly learn the policy? First, Q learning has a lot of issues involving the Deadly Triad; second, if we have continuous actions we cannot really use it; and lastly, Q learning always learns a deterministic policy, and in cases of partially observed stochastic environments (which is nearly always what we have), having a stochastic policy is proven to be better.</description></item><item><title>RL Bite: Learning the Q Function</title><link>https://n1o.github.io/posts/rl-bite-learning-the-q-function/</link><pubDate>Mon, 03 Mar 2025 08:59:26 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-learning-the-q-function/</guid><description>Abstract Link to heading We already know how to learn the Value function, however we also know that the Value function by itself is not enough since it averages over all possible actions, instead of taking into consideration specific actions the agent should take. We can derive the Q function from the Value function, however we can also try to directly learn it. Especially directly approximating the Q function with Deep Neural Networks has been a huge success when applied to RL Agents playing Atari computer games.</description></item><item><title>TLDR; Graph Contrastive Learning: Representation Scattering</title><link>https://n1o.github.io/posts/tldr-graph-contrastive-repr-representation-shattering/</link><pubDate>Sun, 23 Feb 2025 13:04:42 +0100</pubDate><guid>https://n1o.github.io/posts/tldr-graph-contrastive-repr-representation-shattering/</guid><description>Source Link to heading Paper link: https://openreview.net/pdf?id=R8SolCx62K Source Code: https://github.com/hedongxiao-tju/SGRL Abstract Link to heading Contrastive Learning (CL) is one of my favorite techniques, it is a self-supervised approach for learning latent representations with a special property: Similar elements have representations that are closer together and elements that are different are farther from each other. The paper: Exploitation of a Latent Mechanism in Graph Contrastive Learning Representation Scattering takes a very novel approach to CL and it gives a nice theoretical foundation of CL and Graph!</description></item><item><title>RL Bite: Computing the Value Function</title><link>https://n1o.github.io/posts/rl-bite-computing-value-functions/</link><pubDate>Tue, 18 Feb 2025 06:20:35 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-computing-value-functions/</guid><description>Abstract Link to heading In the last RL-Bite I wrote about Bellman&amp;rsquo;s Equations and the Value Function and now we will figure out how we actually apply these equations to compute the Value Function!
Known World Model Link to heading Let&amp;rsquo;s start with the simple case, and make an assumption that the underlying World Model of the Markov Decision Process is known, and we have finite discrete states. If we include that the Discount Factor $\gamma &amp;lt; 1$, we can find the Optimal Value function exactly using:</description></item><item><title>TLDR; HC-GAE The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation Learning</title><link>https://n1o.github.io/posts/tldr-hc-gae/</link><pubDate>Sun, 16 Feb 2025 14:48:13 +0100</pubDate><guid>https://n1o.github.io/posts/tldr-hc-gae/</guid><description>Source Link to heading Paper link: https://arxiv.org/abs/2405.14742 Source Code: https://github.com/JonathanGXu/HC-GAE Abstract Link to heading Graph Representation Learning is an essential topic in Graph ML, and it is all about compressing a whole Graph (arbitrarily large) into a fixed representation. Usually these techniques leverage Graph Auto Encoders, which are trained in a self-supervised fashion. This is all good; however, they usually focus on node feature reconstruction, and they tend to lose the topological information that the input Graph encodes.</description></item><item><title>RL Bite: Bellmans Equations and Value Functions</title><link>https://n1o.github.io/posts/rl-bite-bellmans-equations-and-value-functions/</link><pubDate>Wed, 12 Feb 2025 07:00:52 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-bellmans-equations-and-value-functions/</guid><description>Value Based Reinforced Learning Link to heading In value based Reinforced Learning we learn a Value Function:
$$ V_{\pi}(s) = E_{\pi}[G_0|s_0 = s] = E_{\pi}[\sum_{t=0}^T \gamma^t r_t|s_0 = s] $$
$G_t$ is the Total Return at time t, this is just the sum of Rewards an Agent gets walking the trajectory T (fancy name but this is just a sequence of actions the agent takes) $\gamma^t$ is the Discount Factor, long story short this is between $&amp;lt;0,1&amp;gt;$, the closer this value is to zero, the more the agent will focus on the immediate reward, the closer it is to 1 the more it will take future rewards into account.</description></item><item><title>TLDR; Duplex: Dual GAT for Complex Embeddings of Directed Graphs</title><link>https://n1o.github.io/posts/tldr-duplex/</link><pubDate>Mon, 10 Feb 2025 09:22:15 +0100</pubDate><guid>https://n1o.github.io/posts/tldr-duplex/</guid><description>Source Link to heading Paper link: https://arxiv.org/abs/2406.05391 Source Code: https://github.com/alipay/DUPLEX Abstract Link to heading I am a huge fan of Graph Machine Learning, it has a lot of cool applications, and I am particularly interested in Source Code understanding and Vulnerability Detection, where Graph Neural Networks (GNN) are unambiguous. One of the obvious downsides of general GNNs is that they mostly focus on undirected graphs, which makes their approach somewhat limiting for Digraphs (fancy name for directed graphs).</description></item><item><title>RL Bite: Exploitation vs Exploration</title><link>https://n1o.github.io/posts/rl-bite-exploration-vs-exploitation/</link><pubDate>Wed, 05 Feb 2025 09:51:24 +0100</pubDate><guid>https://n1o.github.io/posts/rl-bite-exploration-vs-exploitation/</guid><description>Explore vs Exploit Link to heading In reinforcement learning, we have an agent that has to take actions, for which it receives a reward. Here we have a dilemma: do we choose an action that gives the biggest reward or do we explore new actions that may lead to regions with even higher payouts?
Greedy Policy Link to heading This is a simple one, we always take the option that gives us the highest payout:</description></item><item><title>Graph Neural Networks meet Large Language Models</title><link>https://n1o.github.io/posts/graph-neural-networks-meet-large-language-models/</link><pubDate>Mon, 16 Dec 2024 09:17:05 +0100</pubDate><guid>https://n1o.github.io/posts/graph-neural-networks-meet-large-language-models/</guid><description>Abstract Link to heading I am a huge fan of Graph Neural Networks (GNNs), and I am (a bit less) a fan of Large Language Models (LLMs), however they are hard to ignore. Both have different strengths, while GNNs excel when it comes to problems that have an inherent structure, LLMs thrive in cases where we treat everything as a sequence of tokens (maybe Bytes in the future). A natural question arises, what if we can combine these two?</description></item><item><title>Hymba, a new breed of SSM-Attention Hybrids</title><link>https://n1o.github.io/posts/hymba-new-ssm-att-hybrid-breed/</link><pubDate>Mon, 02 Dec 2024 07:00:00 +0100</pubDate><guid>https://n1o.github.io/posts/hymba-new-ssm-att-hybrid-breed/</guid><description>Abstract Link to heading State space models are really close to my heart, I even have a dedicated page about them. But when it comes to Language Models they lack some performance and that gave rise to SSM-Attention Hybrids. Until now it was conventional that in hybrid layers you sequentially combine Mamba(2) with Attention layers. Nvidia introduced Hymba - this paper changes the game by using Attention and Mamba2 in the same layer, each of them processing the same input tokens.</description></item><item><title>Transform any LLMs to a powerful Encoder</title><link>https://n1o.github.io/posts/from-llms-to-sentence-embeddings/</link><pubDate>Mon, 25 Nov 2024 09:00:33 +0100</pubDate><guid>https://n1o.github.io/posts/from-llms-to-sentence-embeddings/</guid><description>Abstract Link to heading In the last two years there has been a surge of Large Language Models. This is understandable, since LLMs are amazing at generating text, and a lot of things can be viewed as text. However, generation is not always all we need - sometimes we want to have semantically rich representations. In general, LLMs are not the best tool to get semantically rich representations, and even today models like BERT are used to achieve state of the art text embeddings.</description></item><item><title>2025 Year of Zig</title><link>https://n1o.github.io/posts/2025-yearl-of-zig/</link><pubDate>Wed, 20 Nov 2024 08:31:40 +0100</pubDate><guid>https://n1o.github.io/posts/2025-yearl-of-zig/</guid><description>Intro Link to heading Programming languages come and go and during my 20 years of coding I have used many of them to at least some degree (more than just hello world). Thanks to ThePrimeagen I decided to take Zig for a spin. And boy I really like it! Just look at the mascot:
Come on, a crocodile with a jetpack? How cool is that?
Why Zig Link to heading Sure I would not invest a lot of time into anything that a full time YouTuber recommends.</description></item><item><title>Distilling State Space Models from Transformers</title><link>https://n1o.github.io/posts/distilling-ssm-from-transformers/</link><pubDate>Mon, 28 Oct 2024 10:12:27 +0100</pubDate><guid>https://n1o.github.io/posts/distilling-ssm-from-transformers/</guid><description>Abstract Link to heading It is notoriously expensive to train a Language Model from scratch, making independent research impossible and trying out new architectures extremely risky. Because of these costs, Transformer++ models like LLaMa, based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention, are the de facto standard, not because they are the best, but because they are proven to work!</description></item><item><title>Illusion of State in SSMs like Mamba</title><link>https://n1o.github.io/posts/ssm-the-illusion/</link><pubDate>Mon, 14 Oct 2024 10:52:09 +0200</pubDate><guid>https://n1o.github.io/posts/ssm-the-illusion/</guid><description>Abstract Link to heading Last time we looked into the weak points of State Space Models (Mamba, Mamba2), especially when compared with Attention-Based models (LLama, GPT-like). They lack in terms of in-context learning. To alleviate this, we focused on SSM-Transformer Hybrids and introduced multiple models that do this differently. Here we look into the expressivity of State Space Models from two formal perspectives. The first is from the perspective of Circuit complexity and later from the perspective of Formal Languages.</description></item><item><title>Mamba(2) and Transformer Hybrids: An Overview</title><link>https://n1o.github.io/posts/ssm-transformer-hybrids-guide/</link><pubDate>Wed, 18 Sep 2024 11:26:25 +0200</pubDate><guid>https://n1o.github.io/posts/ssm-transformer-hybrids-guide/</guid><description>Abstract Link to heading We have already looked into Mamba and Mamba2. In terms of efficiency, with their linear complexity and the absence of Key-Value cache, they are a significant improvement over Attention-based models in terms of throughput and memory usage. However, not everything is perfect. Transformers have a certain advantage when it comes to in-context learning. In-context learning is the ability to adapt the model without retraining it. This is done by providing relevant (or not) context to the model in the form of a prompt.</description></item><item><title>Hydra a Double Headed Mamba</title><link>https://n1o.github.io/posts/hydra-a-double-headed-mamba/</link><pubDate>Thu, 29 Aug 2024 11:53:51 +0200</pubDate><guid>https://n1o.github.io/posts/hydra-a-double-headed-mamba/</guid><description>Abstract Link to heading State Space Models are awesome, models like Mamba and Mamba2 boast unparalleled performance especially when it comes to long sequences. The only downside is that they are causal, which means they model one token at a time, looking only at past tokens. Bidirectional models like Bert, CodeBERT and GraphCodeBERT have been shown to excel when it comes to code understanding. One way to put it is that by looking into the past and the future simultaneously we can get a better understanding of what is happening.</description></item><item><title>From Mamba to Mamba-2</title><link>https://n1o.github.io/posts/from-mamba-to-mamba2/</link><pubDate>Thu, 08 Aug 2024 09:57:32 +0200</pubDate><guid>https://n1o.github.io/posts/from-mamba-to-mamba2/</guid><description>Abstract Link to heading This is not my first gig where I write about State Space Models. I already mentioned them here and here. Now what is the deal with this Mamba(2) thing? They are proving to be an alternative to the strong Transformer++ architecture (Transformer++ models like LLaMa are based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention). Hold on, if this Transformer++ models work well, why do we need altneratives?</description></item><item><title>Butterflies, Monarchs, Hyenas, and Lightning Fast BERT</title><link>https://n1o.github.io/posts/butterflies-monarchs-hyenas-and-lightning-fast-bert/</link><pubDate>Fri, 12 Jul 2024 13:36:49 +0200</pubDate><guid>https://n1o.github.io/posts/butterflies-monarchs-hyenas-and-lightning-fast-bert/</guid><description>Abstract Link to heading I have been working on a project of creating my own Large Language Model, as I am huge fan of T5, or to be more concrete I recognize the added value of having an Encoder-Decoder architecture. The biggest challenge, at least in my opinion, in training an LLM is the sheer computational costs required to do so. I was originally planning to take the Encoder introduced by ColT5 but than I came across of M2 BERT and suddenly I went down the rabbit hole of Structured Matrices, Butterflies, Monarch and Hyeans.</description></item><item><title>BinT5 and HexT5 or T5 and Binary Reverse Engineering</title><link>https://n1o.github.io/posts/t5-and-reverse-engineering/</link><pubDate>Wed, 12 Jun 2024 14:07:17 +0200</pubDate><guid>https://n1o.github.io/posts/t5-and-reverse-engineering/</guid><description>Abstract Link to heading For a while now I have a new passion and that is binary reverse engineering and vulnerability exploitation. This interest has led me to create CodeBreakers a platform dedicated to applying machine learning to reverse engineering, vulnerability detection, exploitation, and other cybersecurity-related applications.
I found two notable research papers where T5 has been applied to reverse engineering are BinT5 and HexT5. Before we dive deep into the details of these papers, let&amp;rsquo;s first explore the basics of reverse engineering.</description></item><item><title>CodeT5 and CodeT5+</title><link>https://n1o.github.io/posts/code-t5-plus/</link><pubDate>Sat, 01 Jun 2024 08:46:43 +0200</pubDate><guid>https://n1o.github.io/posts/code-t5-plus/</guid><description>Abstract Link to heading In a previous post, T5 the Old New Thing, we briefly touched upon CodeT5 and CodeT5+. Now, we aim to dive deeper into these topics.
I have previously explored CodeBERT and GraphCodeBERT. These models, based on BERT and RoBERTa architectures, excel at code understanding and retrieval tasks. However, they fall short when it comes to code generation tasks. It&amp;rsquo;s worth noting that these models share a common theme: they utilize unique pretraining objectives tailored specifically for source code.</description></item><item><title>Longer Context for T5</title><link>https://n1o.github.io/posts/longer-context-for-t5/</link><pubDate>Mon, 29 Apr 2024 14:10:36 +0200</pubDate><guid>https://n1o.github.io/posts/longer-context-for-t5/</guid><description>Why does T5 need a longer context? Link to heading In my previous post T5 the Old New Thing we already explored why T5 is awesome. But one downside is its limited context length of 512 tokens. However, it does have a limitation - its context length is restricted to 512 tokens. This can&amp;rsquo;t be directly compared to the context length of a decoder-only model, as T5 is an encoder-decoder model.</description></item><item><title>T5 the Old New Thing</title><link>https://n1o.github.io/posts/t5-the-old-new-thing/</link><pubDate>Wed, 06 Mar 2024 12:58:32 +0100</pubDate><guid>https://n1o.github.io/posts/t5-the-old-new-thing/</guid><description>Why T5 Link to heading A couple of weeks ago I run into the following paper Tiny Titans. It compares multiple smallish (up to 1B parameters) open source LLMs with bigger proprietary ones on meeting summarization. TLDR; the small models tend to perform worse in zero-shot setting as well after fine-tunnig than big ones. Except for FLAN-T5-Large which after finetuning performs way beyond its league, beating even the biggest proprietary models (GPT-3.</description></item><item><title>Nixos for Hobby Project</title><link>https://n1o.github.io/posts/nixos-for-hobby-project/</link><pubDate>Wed, 13 Sep 2023 13:09:10 +0200</pubDate><guid>https://n1o.github.io/posts/nixos-for-hobby-project/</guid><description>Lately, I&amp;rsquo;ve embarked on a side project: CodeBreakers. It&amp;rsquo;s nothing too fancy, just a website where I plan to release videos and articles about my latest passionsâ€”Reverse Engineering and Binary Exploitation.
Creating a website isn&amp;rsquo;t all that complex, and I&amp;rsquo;ve done it a couple of times before. The main challenge was deciding where and how to host it. Initially, I considered using Fly ((which is a fantastic PaaS product with many built-in features), but ultimately, I opted for Hetzner and rented a virtual server.</description></item><item><title>Paper overview: Hungry Hungry Hippos: Towards Language Modeling with State Space Models</title><link>https://n1o.github.io/posts/hungry-hungry-hippos/</link><pubDate>Mon, 06 Feb 2023 09:39:03 +0100</pubDate><guid>https://n1o.github.io/posts/hungry-hungry-hippos/</guid><description>High level overview Link to heading By combining State Space Models (SSMs) with Attention, we get a model that generates text more efficiently, with a speed increase of approximately 1.6 times. Additionally, this approach requires less paremters, enabling the development of larger models on existing hardware.
Language modeling requirements Link to heading The Transformer architecture, which forms the basis of ChatGPT, is riding high on the hype train due to its impressive performance.</description></item><item><title>Paper overview: Continuous-Time Modeling of Counterfactual Outcomes Using Neural Controlled Differential Equations</title><link>https://n1o.github.io/posts/continuous-time-modeling-of-counterfatual-outcomes/</link><pubDate>Mon, 16 Jan 2023 17:37:36 +0100</pubDate><guid>https://n1o.github.io/posts/continuous-time-modeling-of-counterfatual-outcomes/</guid><description>The problem it solves Link to heading Imagine you have an irregullary sampled time series, where at various time points we perform interventions. These interventions may influence the dynamics of the timeseries. The question we want to answer is:
If I perform a hypothetical sequence of interventions how will my time series evolve?
An example and some details Link to heading Example Link to heading As a medical doctor in a hospital, if a patient has a high fever and is at risk of dying, a common practice is to measure their levels of C-reactive protein (CRP) to determine if they need antibiotics.</description></item><item><title>Hierarchical Probabilistic Matrix Factorization</title><link>https://n1o.github.io/posts/pooled_matrix_factorization/</link><pubDate>Thu, 17 Dec 2020 10:19:42 +0100</pubDate><guid>https://n1o.github.io/posts/pooled_matrix_factorization/</guid><description>Probabilistic Matrix factorization is a simple but useful model for matrix imputation. The main idea is to decompose a tall and wide matrix into a product of two matrices, one tall and thin and one short and wide.
$$ R_{n\times m} = U_{m \times d} \cdot V_{d \times n} $$
If you are a Bayesian, you can express this model as:
$$ R_{ij} \sim \mathcal{N}(u_i \cdot v_j^T, \sigma) $$ $$ u_i \sim \mathcal{N}(\mu_u, \Sigma_u) $$ $$ v_j \sim \mathcal{N}(\mu_v, \Sigma_v) $$</description></item></channel></rss>