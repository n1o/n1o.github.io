<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Data, Code and Breaking Stuff</title><link>https://n1o.github.io/posts/</link><description>Recent content in Posts on Data, Code and Breaking Stuff</description><generator>Hugo</generator><language>en</language><lastBuildDate>Wed, 06 Mar 2024 12:58:32 +0100</lastBuildDate><atom:link href="https://n1o.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>T5 the Old New Thing</title><link>https://n1o.github.io/posts/t5-the-old-new-thing/</link><pubDate>Wed, 06 Mar 2024 12:58:32 +0100</pubDate><guid>https://n1o.github.io/posts/t5-the-old-new-thing/</guid><description>Why T5 Link to heading A couple of weeks ago I run into the following paper Tiny Titans. It compares multiple smallish (up to 1B parameters) open source LLMs with bigger proprietary ones on meeting summarization. TLDR; the small models tend to perform worse in zero-shot setting as well after fine-tunnig than big ones. Except for FLAN-T5-Large which after finetuning performs way beyond its league, beating even the biggest proprietary models (GPT-3.</description></item><item><title>Nixos for Hobby Project</title><link>https://n1o.github.io/posts/nixos-for-hobby-project/</link><pubDate>Wed, 13 Sep 2023 13:09:10 +0200</pubDate><guid>https://n1o.github.io/posts/nixos-for-hobby-project/</guid><description>Lately, I&amp;rsquo;ve embarked on a side project: CodeBreakers. It&amp;rsquo;s nothing too fancy, just a website where I plan to release videos and articles about my latest passionsâ€”Reverse Engineering and Binary Exploitation.
Creating a website isn&amp;rsquo;t all that complex, and I&amp;rsquo;ve done it a couple of times before. The main challenge was deciding where and how to host it. Initially, I considered using Fly ((which is a fantastic PaaS product with many built-in features), but ultimately, I opted for Hetzner and rented a virtual server.</description></item><item><title>Paper overview: Hungry Hungry Hippos: Towards Language Modeling with State Space Models</title><link>https://n1o.github.io/posts/hungry-hungry-hippos/</link><pubDate>Mon, 06 Feb 2023 09:39:03 +0100</pubDate><guid>https://n1o.github.io/posts/hungry-hungry-hippos/</guid><description>High level overview Link to heading By combining State Space Models (SSMs) with Attention, we get a model that generates text more efficiently, with a speed increase of approximately 1.6 times. Additionally, this approach requires less paremters, enabling the development of larger models on existing hardware.
Language modeling requirements Link to heading The Transformer architecture, which forms the basis of ChatGPT, is riding high on the hype train due to its impressive performance.</description></item><item><title>Paper overview: Continuous-Time Modeling of Counterfactual Outcomes Using Neural Controlled Differential Equations</title><link>https://n1o.github.io/posts/continuous-time-modeling-of-counterfatual-outcomes/</link><pubDate>Mon, 16 Jan 2023 17:37:36 +0100</pubDate><guid>https://n1o.github.io/posts/continuous-time-modeling-of-counterfatual-outcomes/</guid><description>The problem it solves Link to heading Imagine you have an irregullary sampled time series, where at various time points we perform interventions. These interventions may influence the dynamics of the timeseries. The question we want to answer is:
If I perform a hypothetical sequence of interventions how will my time series evolve?
An example and some details Link to heading Example Link to heading As a medical doctor in a hospital, if a patient has a high fever and is at risk of dying, a common practice is to measure their levels of C-reactive protein (CRP) to determine if they need antibiotics.</description></item><item><title>Hierarchical Probabilistic Matrix Factorization</title><link>https://n1o.github.io/posts/pooled_matrix_factorization/</link><pubDate>Thu, 17 Dec 2020 10:19:42 +0100</pubDate><guid>https://n1o.github.io/posts/pooled_matrix_factorization/</guid><description>Probabilistic Matrix factorization is a simple but useful model for matrix imputation. The main idea is to decompose a tall and wide matrix into a product of two matrices, one tall and thin and one short and wide.
$$ R_{n\times m} = U_{m \times d} \cdot V_{d \times n} $$
If you are a Bayesian, you can express this model as:
$$ R_{ij} \sim \mathcal{N}(u_i \cdot v_j^T, \sigma) $$ $$ u_i \sim \mathcal{N}(\mu_u, \Sigma_u) $$ $$ v_j \sim \mathcal{N}(\mu_v, \Sigma_v) $$</description></item></channel></rss>