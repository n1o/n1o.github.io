<!doctype html><html lang=en><head><title>TLDR; Duplex: Dual GAT for Complex Embeddings of Directed Graphs · Data Artificer and code:Breaker
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="n1o_c0rTx"><meta name=description content="Source Link to heading Paper link: https://arxiv.org/abs/2406.05391 Source Code: https://github.com/alipay/DUPLEX Abstract Link to heading I am a huge fan of Graph Machine Learning, it has a lot of cool applications, and I am particularly interested in Source Code understanding and Vulnerability Detection, where Graph Neural Networks (GNN) are unambiguous. One of the obvious downsides of general GNNs is that they mostly focus on undirected graphs, which makes their approach somewhat limiting for Digraphs (fancy name for directed graphs)."><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="TLDR; Duplex: Dual GAT for Complex Embeddings of Directed Graphs"><meta name=twitter:description content="Source Link to heading Paper link: https://arxiv.org/abs/2406.05391 Source Code: https://github.com/alipay/DUPLEX Abstract Link to heading I am a huge fan of Graph Machine Learning, it has a lot of cool applications, and I am particularly interested in Source Code understanding and Vulnerability Detection, where Graph Neural Networks (GNN) are unambiguous. One of the obvious downsides of general GNNs is that they mostly focus on undirected graphs, which makes their approach somewhat limiting for Digraphs (fancy name for directed graphs)."><meta property="og:url" content="https://n1o.github.io/posts/tldr-duplex/"><meta property="og:site_name" content="Data Artificer and code:Breaker"><meta property="og:title" content="TLDR; Duplex: Dual GAT for Complex Embeddings of Directed Graphs"><meta property="og:description" content="Source Link to heading Paper link: https://arxiv.org/abs/2406.05391 Source Code: https://github.com/alipay/DUPLEX Abstract Link to heading I am a huge fan of Graph Machine Learning, it has a lot of cool applications, and I am particularly interested in Source Code understanding and Vulnerability Detection, where Graph Neural Networks (GNN) are unambiguous. One of the obvious downsides of general GNNs is that they mostly focus on undirected graphs, which makes their approach somewhat limiting for Digraphs (fancy name for directed graphs)."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-10T09:22:15+01:00"><meta property="article:modified_time" content="2025-02-10T09:22:15+01:00"><meta property="article:tag" content="TLDR"><meta property="article:tag" content="GNN"><meta property="article:tag" content="Graph-Attention"><meta property="article:tag" content="Directed-Graphs"><meta property="og:see_also" content="https://n1o.github.io/posts/tldr-graph-contrastive-repr-representation-shattering/"><meta property="og:see_also" content="https://n1o.github.io/posts/tldr-hc-gae/"><meta property="og:see_also" content="https://n1o.github.io/posts/tldr-graph-contrastive-repr-representation-shattering/"><meta property="og:see_also" content="https://n1o.github.io/posts/tldr-hc-gae/"><link rel=canonical href=https://n1o.github.io/posts/tldr-duplex/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://n1o.github.io/>Data Artificer and code:Breaker
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Writing</a></li><li class=navigation-item><a class=navigation-link href=/awesome-t5/>Awesome T5</a></li><li class=navigation-item><a class=navigation-link href=/awesome-ssm/>Awesome SSM</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/tldr-duplex/>TLDR; Duplex: Dual GAT for Complex Embeddings of Directed Graphs</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2025-02-10T09:22:15+01:00>February 10, 2025
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
5-minute read</span></div><div class=categories><i class="fa-solid fa-folder" aria-hidden=true></i>
<a href=/categories/tldr/>TLDR</a>
<span class=separator>•</span>
<a href=/categories/gnn/>GNN</a>
<span class=separator>•</span>
<a href=/categories/graph-attention/>Graph-Attention</a>
<span class=separator>•</span>
<a href=/categories/directed-graphs/>Directed-Graphs</a></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/tldr/>TLDR</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/gnn/>GNN</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/graph-attention/>Graph-Attention</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/directed-graphs/>Directed-Graphs</a></span></div></div></header><div class=post-content><h1 id=source>Source
<a class=heading-link href=#source><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><ul><li>Paper link: <a href=https://arxiv.org/abs/2406.05391 class=external-link target=_blank rel=noopener>https://arxiv.org/abs/2406.05391</a></li><li>Source Code: <a href=https://github.com/alipay/DUPLEX class=external-link target=_blank rel=noopener>https://github.com/alipay/DUPLEX</a></li></ul><h1 id=abstract>Abstract
<a class=heading-link href=#abstract><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>I am a huge fan of Graph Machine Learning, it has a lot of cool applications, and I am particularly interested in Source Code understanding and Vulnerability Detection, where Graph Neural Networks (GNN) are unambiguous. One of the obvious downsides of general GNNs is that they mostly focus on undirected graphs, which makes their approach somewhat limiting for Digraphs (fancy name for directed graphs). This TLDR; is about DUPLEX (I already wrote about its application in <a href=https://codebreakers.re/articles/llm-and-security/galla-graph-aligned-llm class=external-link target=_blank rel=noopener>GaLLa</a>) which is a cool technique to learn node representations in a self-supervised way (can be extended with arbitrary objectives).</p><h1 id=tldr-duplex>TLDR; DUPLEX
<a class=heading-link href=#tldr-duplex><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>It is a technique for Digraphs where we learn low-dimensional Node representations that can be used for downstream tasks. To fully capture the directionality and the information flow we make the learned Node representations complex valued and we learn them with a dual Graph Attention Network (GAT) encoder. We then reconstruct the learned complex node embedding using two parameter-free decoders.</p><p><img alt=Overview src=/images/duplex_overview.png></p><h1 id=hermitian-adjacency-matrix-ham>Hermitian Adjacency Matrix (HAM)
<a class=heading-link href=#hermitian-adjacency-matrix-ham><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Normally we can use the Adjacency matrix of a Graph to describe the connection (edges) between nodes, however this does not tell us anything useful about the direction of these edges. To capture the direction we are going to use Hermitian Adjacency Matrix (HAM) where its entry for a pair of nodes $u,v$ is $H_{u,v} \in { i, -i, 1, 0}$ which represents a forward, reverse, bidirectional and no edge between these nodes.</p><p>$$ H = A_s \odot \exp(i \frac{\pi}{w} \Theta)$$</p><ul><li>$i$ is the imaginary unit</li><li>$\pi$ is just 3.14&mldr;&mldr; pi :)</li><li>$\odot$ is the Hadamard (element wise) product</li><li>$A_s$ is the undirected symmetric Adjacency matrix</li></ul><p>$$ \Theta(u, v) = \begin{cases} 1, & \text{if } (u, v) \in \mathcal{E}, \\ -1, & \text{if } (v, u) \in \mathcal{E}, \\ 0, & \text{otherwise} \end{cases}$$</p><p>So the HAM can represent all the possible link directions, but it can also be decomposed as:</p><p>$$H = X^T \tilde{X}$$</p><p>$$x_{u} = a_{u} \odot \exp (i \frac{\pi}{2} \theta_{u})$$
$$\tilde{x_u} = a_{u} \odot \exp (-i \frac{\pi}{2} \theta_{u}) $$</p><ul><li>$a_u$ is the amplitude and $\theta_u$ is the phase of $x_u$</li><li>$x_u \in C^{d \times 1}$ is complex embedding and $\tilde{x}_u$ is its complex conjugate</li></ul><p>Now what we want is to learn for each node is to learn $a_u$ and $\theta_u$ from which we can then construct the complex embedding!</p><h1 id=dual-gat-encoder>Dual GAT Encoder
<a class=heading-link href=#dual-gat-encoder><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>From above it should be obvious we need:</p><ol><li><strong>Amplitude Encoder</strong></li><li><strong>Phase Encoder</strong></li></ol><p>Both of them will use GAT under the hood for message passing and we need an extra <strong>Fusion Layer</strong> to share information between them.</p><h2 id=amplitude-encoder>Amplitude Encoder
<a class=heading-link href=#amplitude-encoder><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Here we learn an embedding $a_u$ for node $u$, which captures only the connection information (we do not care about the direction)</p><p>$$ a_{u}^{\prime} = \phi (\sum_{v \in \mathcal{N}(u)} f_a(a_{u}, a_{v}) \psi_a(a_{v} ) )$$</p><ul><li>$N(u)$ is the neighborhood of u</li><li>$\phi$ is the activation function here we use ReLU</li><li>$f_a, \psi_a$ is the learnable attention mechanism</li></ul><h2 id=phase-encoder>Phase Encoder
<a class=heading-link href=#phase-encoder><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>We learn an embedding $\theta_u$, the approach is similar to amplitude but with the difference that here we care about the direction information.</p><p><img alt="Phase Encoder" src=/images/duplex_phase_encoder.png></p><ul><li>the important difference is that there is a subtraction between the in-neighborhood information and out-neighborhood information, this is due to their asymmetry</li></ul><h2 id=fusion-layer>Fusion Layer
<a class=heading-link href=#fusion-layer><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>We combine the information from the amplitude and phase embedding to update the <strong>amplitude embedding</strong> (only)</p><p><img alt="Fusion Layer" src=/images/duplex_fusion_layer.png></p><ul><li>just a sum of two attention layers passed through a non-linearity, the first GAT derives the key using the amplitude embedding and the second uses phase embedding as key, in both cases we take the whole node neighborhood into account discarding the direction information</li></ul><p>This is an example of &ldquo;mid-fusion&rdquo;, where we integrate embeddings at the network&rsquo;s intermediate layers. We do this instead of early fusion because if there are no node attributes it would introduce only random noise and late-fusion (at the terminal layer) can dilute the unique attributes of the amplitude and phase embeddings.</p><h3 id=notes>Notes
<a class=heading-link href=#notes><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>We technically can replace GAT with any other Spatial GNN, also Mamba!</p><h1 id=two-parameter-free-decoders>Two Parameter free Decoders
<a class=heading-link href=#two-parameter-free-decoders><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>We learned an amplitude embedding $a_u$ and a phase embedding $\theta_u$ which we can use to construct the complex embedding $x_u$. From these 3 embeddings (well actually we only use $x_u$ and $a_u$) we are going to train two decoders:</p><ol><li><strong>Direction aware Decoder</strong></li><li><strong>Connection aware Decoder</strong></li></ol><p>Each has its own supervised loss function, with the total loss of the model defined as the sum of individual losses $\mathcal{L} = \mathcal{L}_d + \lambda \mathcal{L}_c$.</p><h3 id=direction-aware-decoder>Direction aware decoder
<a class=heading-link href=#direction-aware-decoder><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>This decoder focuses on reconstructing the complex-valued HAM of the Digraph:</p><p><img alt="Direction Aware Decoder" src=/images/duplex_direction_aware_decoder.png></p><ul><li>this calculates the probability of having an edge between (u,v) and the edge type $r$</li></ul><p>With the loss is defined as:</p><p><img alt="Direction Aware Decoder Loss" src=/images/duplex_direction_aware_decoder_loss.png></p><ul><li>here $x_u, \bar{x}_u$ are node embeddings in polar form</li></ul><h3 id=connection-aware-decoder>Connection aware decoder
<a class=heading-link href=#connection-aware-decoder><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>This decoder focuses only on the existence of connections, it can be viewed as an auxiliary to the Direction aware decoder:</p><p><img alt="Connection Aware Decoder" src=/images/duplex_connection_aware_decoder_loss.png></p><ul><li><p>$\sigma$ is the sigmoid function with $\hat{A}$ is the estimated undirected Adjacency matrix</p></li><li><p>the loss is the same negative sum log likelihood as in direction aware.</p></li></ul><h1 id=final-remarks>Final Remarks
<a class=heading-link href=#final-remarks><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>First of all, in terms of performance it is state of the art when it comes to Digraphs (most papers are! at least during the time of their publishing). The biggest benefit is using GAT for the encoders, this gives a huge benefit since it is enough to aggregate neighborhood information making the model scale to graphs that are similar to the ones inside the training set. Second, the self-supervised method allows pretraining in absence of any labeled data and we can then build models on top of these representations, where we concatenate the phase and amplitude embedding. In case we have labels we can easily extend the learning objective to take them into account!</p></div><footer><section class=see-also><h3 id=see-also-in-tldr>See also in TLDR
<a class=heading-link href=#see-also-in-tldr><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><nav><ul><li><a href=/posts/tldr-graph-contrastive-repr-representation-shattering/>TLDR; Graph Contrastive Learning: Representation Scattering</a></li><li><a href=/posts/tldr-hc-gae/>TLDR; HC-GAE The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation Learning</a></li></ul></nav><h3 id=see-also-in-gnn>See also in GNN
<a class=heading-link href=#see-also-in-gnn><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><nav><ul><li><a href=/posts/tldr-graph-contrastive-repr-representation-shattering/>TLDR; Graph Contrastive Learning: Representation Scattering</a></li><li><a href=/posts/tldr-hc-gae/>TLDR; HC-GAE The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation Learning</a></li></ul></nav></section><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2020 -
2025
n1o_c0rTx
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ")}</script></body></html>