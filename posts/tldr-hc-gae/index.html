<!doctype html><html lang=en><head><title>TLDR; HC-GAE The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation Learning · Data Artificer and code:Breaker
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="n1o_c0rTx"><meta name=description content="Source Link to heading Paper link: https://arxiv.org/abs/2405.14742 Source Code: https://github.com/JonathanGXu/HC-GAE Abstract Link to heading Graph Representation Learning is an essential topic in Graph ML, and it is all about compressing a whole Graph (arbitrarily large) into a fixed representation. Usually these techniques leverage Graph Auto Encoders, which are trained in a self-supervised fashion. This is all good; however, they usually focus on node feature reconstruction, and they tend to lose the topological information that the input Graph encodes."><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="TLDR; HC-GAE The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation Learning"><meta name=twitter:description content="Source Link to heading Paper link: https://arxiv.org/abs/2405.14742 Source Code: https://github.com/JonathanGXu/HC-GAE Abstract Link to heading Graph Representation Learning is an essential topic in Graph ML, and it is all about compressing a whole Graph (arbitrarily large) into a fixed representation. Usually these techniques leverage Graph Auto Encoders, which are trained in a self-supervised fashion. This is all good; however, they usually focus on node feature reconstruction, and they tend to lose the topological information that the input Graph encodes."><meta property="og:url" content="https://n1o.github.io/posts/tldr-hc-gae/"><meta property="og:site_name" content="Data Artificer and code:Breaker"><meta property="og:title" content="TLDR; HC-GAE The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation Learning"><meta property="og:description" content="Source Link to heading Paper link: https://arxiv.org/abs/2405.14742 Source Code: https://github.com/JonathanGXu/HC-GAE Abstract Link to heading Graph Representation Learning is an essential topic in Graph ML, and it is all about compressing a whole Graph (arbitrarily large) into a fixed representation. Usually these techniques leverage Graph Auto Encoders, which are trained in a self-supervised fashion. This is all good; however, they usually focus on node feature reconstruction, and they tend to lose the topological information that the input Graph encodes."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-16T14:48:13+01:00"><meta property="article:modified_time" content="2025-02-16T14:48:13+01:00"><meta property="article:tag" content="TLDR"><meta property="article:tag" content="GNN"><meta property="article:tag" content="Subgraph"><meta property="article:tag" content="Hierarchical Graph Auto-Encoder"><meta property="og:see_also" content="https://n1o.github.io/posts/tldr-duplex/"><meta property="og:see_also" content="https://n1o.github.io/posts/tldr-duplex/"><link rel=canonical href=https://n1o.github.io/posts/tldr-hc-gae/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://n1o.github.io/>Data Artificer and code:Breaker
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Writing</a></li><li class=navigation-item><a class=navigation-link href=/awesome-t5/>Awesome T5</a></li><li class=navigation-item><a class=navigation-link href=/awesome-ssm/>Awesome SSM</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/tldr-hc-gae/>TLDR; HC-GAE The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation Learning</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2025-02-16T14:48:13+01:00>February 16, 2025
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
4-minute read</span></div><div class=categories><i class="fa-solid fa-folder" aria-hidden=true></i>
<a href=/categories/tldr/>TLDR</a>
<span class=separator>•</span>
<a href=/categories/subgraph/>Subgraph</a>
<span class=separator>•</span>
<a href=/categories/gnn/>GNN</a>
<span class=separator>•</span>
<a href=/categories/hierarchical-graph-auto-encoder/>Hierarchical Graph Auto-Encoder</a></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/tldr/>TLDR</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/gnn/>GNN</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/subgraph/>Subgraph</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/hierarchical-graph-auto-encoder/>Hierarchical Graph Auto-Encoder</a></span></div></div></header><div class=post-content><h1 id=source>Source
<a class=heading-link href=#source><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><ul><li>Paper link: <a href=https://arxiv.org/abs/2405.14742 class=external-link target=_blank rel=noopener>https://arxiv.org/abs/2405.14742</a></li><li>Source Code: <a href=https://github.com/JonathanGXu/HC-GAE class=external-link target=_blank rel=noopener>https://github.com/JonathanGXu/HC-GAE</a></li></ul><h1 id=abstract>Abstract
<a class=heading-link href=#abstract><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Graph Representation Learning is an essential topic in Graph ML, and it is all about compressing a whole Graph (arbitrarily large) into a fixed representation. Usually these techniques leverage Graph Auto Encoders, which are trained in a self-supervised fashion. This is all good; however, they usually focus on node feature reconstruction, and they tend to lose the topological information that the input Graph encodes.</p><h1 id=hierarchical-cluster-based-graph-auto-encoder-hc-gae><strong>H</strong>ierarchical <strong>C</strong>luster-based <strong>G</strong>raph <strong>A</strong>uto <strong>E</strong>ncoder (HC-GAE)
<a class=heading-link href=#hierarchical-cluster-based-graph-auto-encoder-hc-gae><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>An approach that learns graph representations by encoding node features but also the topology of the Graph. This is done by an encoder-decoder architecture, where the encoder operates in multiple steps, in each step we compress the input graph into a collection of subgraphs. The goal of the decoder is to reverse this process and recover the original graph, with the correct topology and the correct node features.</p><p><img src=/images/hc_gae.png></p><h2 id=encoder>Encoder
<a class=heading-link href=#encoder><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Encoder consists of a bunch of layers, where each layer can be characterized by two processes:</p><ol><li>Subgraph Assignment</li><li>Coarsening</li></ol><h3 id=subgraph-assignment>Subgraph Assignment
<a class=heading-link href=#subgraph-assignment><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>The idea is that we have an input graph which is compressed into an output graph, by assigning one or more nodes from the input graph to a single node in the output graph. The assignment works in two steps:</p><ol><li>Soft Assignment
$$ S_{soft} =
\begin{cases}
softmax(GNN(X^{(l)}, A^{(l)})) & \text{if } l = 1 \
softmax(X^{(l)}) & \text{if } l > 1
\end{cases}
$$</li></ol><ul><li>$S_{soft} \in R^{n_{(l)} \times n_{(l+1)}}$</li></ul><p>This just calculates the probability that a node $i$ from the input graphs will belong to node $j$ in the output graph</p><ol start=2><li>Hard Assignment
$$
S^{(l)}(i, j) =
\begin{cases}
1 & \text{if } S_{soft}(i, j) = \max_{\forall j \in n_{l+1}} [S_{soft}(i, :)] \
0 & \text{otherwise}
\end{cases}
$$
Nothing fancy we just take the maximum, this enforces that the input graph is partitioned into a bunch of Subgraphs.</li></ol><p>Another view on this problem is that we learn a mapping between two Graph Adjacency matrices:</p><p>$$A^{(l+1)} = S^{(l)^T} A^{(l)}S^{(l)}$$</p><h3 id=coarsening>Coarsening
<a class=heading-link href=#coarsening><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Once we have the learned subgraph partitioning, we learn the node representations for coarsened graph:</p><p>$$Z_j^{(l)} = A_j^{(l)}X_j^{(l)}W_j^{(l)} $$</p><p>This is just Graph Convolution Network (GCN), where we aggregate information from a neighborhood, and since we operate on Subgraphs we do not need to worry about over-smoothing. Given the learned representations we derive the node features:</p><p>$$X^{(l+1)} = Reorder[\underset{j=1}{\overset{n_{l+1}}{\parallel}} s_j^{(l)^\top}] Z^{(l)}$$</p><ul><li>$s_j^{(l)} = softmax(A_j^{(l)} X_j^{(l)} D_j^{(l)})$ these are just mixing weights</li><li>we need to REORDER so we use the correct weight with the correct embedding</li></ul><h3 id=final-graph-representation>Final Graph Representation
<a class=heading-link href=#final-graph-representation><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>For the final graph representation we usually pool the remaining node representations with some sort of pooling like Mean, Max, Min pooling or other.</p><h2 id=decoder>Decoder
<a class=heading-link href=#decoder><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The decoder reverses the graph compression in multiple layers. The key distinction is that we use only soft assignment (with hard assignment we would end up with a bunch of subgraphs) and it is done by learning the re-assignment matrix:</p><p>$$ \bar{S}^{l&rsquo;} \in R^{n_{(l&rsquo;)} \times n_{(l&rsquo; +1)}}$$</p><ul><li>in this case $n_{(l&rsquo;)} &lt; n_{(l&rsquo;+1)}$
$$ \bar{S}^{(l&rsquo;)} = softmax(GNN_{l&rsquo;, re}(X&rsquo;^{(l&rsquo;)}, A&rsquo;^{(l&rsquo;)})) $$</li></ul><p>And we reconstruct the latent representation of individual nodes:</p><p>$$ \bar{Z}^{(l&rsquo;)} = GNN_{l&rsquo;, emb}(X&rsquo;^{(l&rsquo;)}, A&rsquo;^{(l&rsquo;)}) $$</p><ul><li>$GNN_{re}, GNN_{emb}$ are two GNN decoders that do not share parameters</li></ul><p>Now we can compute $A^{(l&rsquo;)}$ the same fashion as in the encoder, but here we increase the dimensions with each layer.</p><p>$$ A&rsquo;^{(l&rsquo;+1)} = \bar{S}^{(l&rsquo;)^\top} A&rsquo;^{(l&rsquo;)} \bar{S}^{(l&rsquo;)} $$</p><p>And the reconstructed node features:
$$ X&rsquo;^{(l&rsquo;+1)} = \bar{S}^{(l&rsquo;)^\top} \bar{Z}^{(l&rsquo;)} $$</p><h2 id=loss>Loss
<a class=heading-link href=#loss><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The loss is a bit tricky, we have a local loss, this covers the information in the subgraphs (needs to capture each layer, where the coarsening happens) and a global loss that captures the information in the whole graph.</p><p>$$ L_{local} = \sum_{l=1}^{L} \sum_{j=1}^{n_{(l+1)}} KL[q(Z_j^{(l)} | X_j^{(l)}, A_j^{(l)}) || p(Z^{(l)})]$$
$$ L_{global} = -\sum_{l=1}^{L} E_{q(X^{(L)}, A^{(L)})|X^{(l)}, A^{(l)}} [\log p(X&rsquo;^{(L-l+2)}, A&rsquo;^{(L-l+2)} | X^{(L)}, A^{(L)})]$$</p><p>$$ L_{HC-GAE} = L_{local} + L_{global}$$</p><ul><li>$Z^{(l)}$ a Gaussian prior, introduced</li></ul><h1 id=final-remarks>Final Remarks
<a class=heading-link href=#final-remarks><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>The overall approach of continually compressing the graph, each time splitting a graph into subgraphs and aggregating the information in them is a great way to avoid oversmoothing. What I find personally compelling is the application to domains where there are naturally occurring subgraphs. At <a href=https://codebreakers.re/ class=external-link target=_blank rel=noopener>code:Breakers</a> I do a lot of AI stuff around source code and cybersecurity. If you think about it, source code is inherently a huge graph, which nicely aggregates: individual statements into control flow, control flow into functions, functions into classes, those into modules. With HC-GAE I can force this natural aggregation into the training objective, and not just that, introduce some extra aggregation along the way to make the final representation as effective as possible.</p></div><footer><section class=see-also><h3 id=see-also-in-tldr>See also in TLDR
<a class=heading-link href=#see-also-in-tldr><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><nav><ul><li><a href=/posts/tldr-duplex/>TLDR; Duplex: Dual GAT for Complex Embeddings of Directed Graphs</a></li></ul></nav><h3 id=see-also-in-gnn>See also in GNN
<a class=heading-link href=#see-also-in-gnn><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><nav><ul><li><a href=/posts/tldr-duplex/>TLDR; Duplex: Dual GAT for Complex Embeddings of Directed Graphs</a></li></ul></nav></section><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2020 -
2025
n1o_c0rTx
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ")}</script></body></html>