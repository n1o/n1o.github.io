<!doctype html><html lang=en><head><title>RL Bite: Bellmans Equations and Value Functions · Data Artificer and code:Breaker
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="n1o_c0rTx"><meta name=description content="Value Based Reinforced Learning Link to heading In value based Reinforced Learning we learn a Value Function:
$$ V_{\pi}(s) = E_{\pi}[G_0|s_0 = s] = E_{\pi}[\sum_{t=0}^T \gamma^t r_t|s_0 = s] $$
$G_t$ is the Total Return at time t, this is just the sum of Rewards an Agent gets walking the trajectory T (fancy name but this is just a sequence of actions the agent takes) $\gamma^t$ is the Discount Factor, long story short this is between $<0,1>$, the closer this value is to zero, the more the agent will focus on the immediate reward, the closer it is to 1 the more it will take future rewards into account."><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="RL Bite: Bellmans Equations and Value Functions"><meta name=twitter:description content="Value Based Reinforced Learning Link to heading In value based Reinforced Learning we learn a Value Function:
$$ V_{\pi}(s) = E_{\pi}[G_0|s_0 = s] = E_{\pi}[\sum_{t=0}^T \gamma^t r_t|s_0 = s] $$
$G_t$ is the Total Return at time t, this is just the sum of Rewards an Agent gets walking the trajectory T (fancy name but this is just a sequence of actions the agent takes) $\gamma^t$ is the Discount Factor, long story short this is between $<0,1>$, the closer this value is to zero, the more the agent will focus on the immediate reward, the closer it is to 1 the more it will take future rewards into account."><meta property="og:url" content="https://n1o.github.io/posts/rl-bite-bellmans-equations-and-value-functions/"><meta property="og:site_name" content="Data Artificer and code:Breaker"><meta property="og:title" content="RL Bite: Bellmans Equations and Value Functions"><meta property="og:description" content="Value Based Reinforced Learning Link to heading In value based Reinforced Learning we learn a Value Function:
$$ V_{\pi}(s) = E_{\pi}[G_0|s_0 = s] = E_{\pi}[\sum_{t=0}^T \gamma^t r_t|s_0 = s] $$
$G_t$ is the Total Return at time t, this is just the sum of Rewards an Agent gets walking the trajectory T (fancy name but this is just a sequence of actions the agent takes) $\gamma^t$ is the Discount Factor, long story short this is between $<0,1>$, the closer this value is to zero, the more the agent will focus on the immediate reward, the closer it is to 1 the more it will take future rewards into account."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-12T07:00:52+01:00"><meta property="article:modified_time" content="2025-02-12T07:00:52+01:00"><meta property="article:tag" content="RL Bite"><meta property="article:tag" content="Value Based RL"><meta property="article:tag" content="Bellman"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-monte-carlo-search-tree/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-policy-improvement/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-policy-gradient-and-reinforce/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-learning-the-q-function/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-computing-value-functions/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-exploration-vs-exploitation/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-computing-value-functions/"><link rel=canonical href=https://n1o.github.io/posts/rl-bite-bellmans-equations-and-value-functions/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://n1o.github.io/>Data Artificer and code:Breaker
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Writing</a></li><li class=navigation-item><a class=navigation-link href=/awesome-t5/>Awesome T5</a></li><li class=navigation-item><a class=navigation-link href=/awesome-ssm/>Awesome SSM</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/rl-bite-bellmans-equations-and-value-functions/>RL Bite: Bellmans Equations and Value Functions</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2025-02-12T07:00:52+01:00>February 12, 2025
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
3-minute read</span></div><div class=categories><i class="fa-solid fa-folder" aria-hidden=true></i>
<a href=/categories/rl-bite/>RL Bite</a>
<span class=separator>•</span>
<a href=/categories/value-based-rl/>Value Based RL</a>
<span class=separator>•</span>
<a href=/categories/bellman/>Bellman</a></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/rl-bite/>RL Bite</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/value-based-rl/>Value Based RL</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/bellman/>Bellman</a></span></div></div></header><div class=post-content><h1 id=value-based-reinforced-learning>Value Based Reinforced Learning
<a class=heading-link href=#value-based-reinforced-learning><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>In value based Reinforced Learning we learn a <strong>Value Function</strong>:</p><p>$$ V_{\pi}(s) = E_{\pi}[G_0|s_0 = s] = E_{\pi}[\sum_{t=0}^T \gamma^t r_t|s_0 = s] $$</p><ul><li>$G_t$ is the <strong>Total Return at time t</strong>, this is just the sum of Rewards an Agent gets walking the trajectory T (fancy name but this is just a sequence of actions the agent takes)<ul><li>$\gamma^t$ is the <strong>Discount Factor</strong>, long story short this is between $&lt;0,1>$, the closer this value is to zero, the more the agent will focus on the immediate reward, the closer it is to 1 the more it will take future rewards into account. In general using $\gamma=1$ is bad if we do not have an explicit terminating state and the agent will run forever (because of this we can say that the discount factor forces the agent to finish). If we use $\gamma=0$ then we are essentially blind, only caring for the immediate reward, thus it is best to stay away from the boundaries.</li></ul></li></ul><p>The Value Function measures the expected total return if we start at $s$ and we transition across states based on the policy $\pi$</p><h2 id=q-function-aka-action-value-function>Q Function a.k.a: Action-Value Function
<a class=heading-link href=#q-function-aka-action-value-function><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>We will talk mostly about Value Functions, however they are a bit constrained as they do not take the action an agent can take. The value function can be easily extended:</p><p>$$Q_{\pi}(s,a) = E_{\pi}[G_0, s_0 = s, a_0 = a] = E_{\pi}[\sum_{t=0}^T \gamma|s_0 = s, a_0 = a] $$</p><p>And a Q function is born, which is also known as <strong>Action-value Function</strong>, since we take the action also into account. Learning Q functions is a bit more involved, but way more useful, and I will make a separate post just about them.</p><h1 id=bellmans-equations>Bellman&rsquo;s Equations
<a class=heading-link href=#bellmans-equations><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Bellman&rsquo;s equations give us a framework to learn an optimal policy $\pi^*$ for a Value Function. All the equations are true for the Q function as well!</p><h2 id=optimal-policy>Optimal Policy
<a class=heading-link href=#optimal-policy><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The idea is that there exists an optimal policy $ \pi^{\star}$ for which $V_{\pi^{\star}} \ge V_{\pi}$. For a given Markov Decision Process (MDP) there can be multiple optimal policies, but for a <strong>finite MDP</strong> there has to be at least one <strong>Deterministic</strong> optimal policy.</p><h2 id=equations>Equations
<a class=heading-link href=#equations><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>$$V_{\star}(s) = \max_a R(s,a) + \gamma E_{p_S(s&rsquo;|s,a)} [\max_{a&rsquo;}V_{*}(s&rsquo;,a&rsquo;)]$$</p><p>$$Q_{\star}(s,a) = R(s,a) + \gamma E_{p_S(s&rsquo;|s,a)} [\max_{a&rsquo;}Q_{*}(s&rsquo;,a&rsquo;)]$$</p><p>These beauties above are the <strong>Bellman&rsquo;s Optimality Equations</strong>, and they are satisfied when we follow the Optimal Policy. This means if we do not have an optimal policy we have an error, meaning $V_{\pi^{\star}} = V_{\star} + \delta(s)$, this difference $\delta(s)$ is called <strong>Bellman&rsquo;s Error</strong>.</p><h2 id=bellmans-operator>Bellman&rsquo;s Operator
<a class=heading-link href=#bellmans-operator><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Bellman&rsquo;s Operator is an update rule that is used to derive a new Value Function by minimizing the Bellman&rsquo;s Error.</p><p>$$V&rsquo;(s) = B_M^\pi V(s) \triangleq E_{\pi(a|s)} [R(s, a) + \gamma E_{T(s&rsquo;|s, a)} [V(s&rsquo;)]]$$</p><h2 id=bellmans-backup>Bellman&rsquo;s Backup
<a class=heading-link href=#bellmans-backup><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p><strong>Bellman&rsquo;s Backup</strong> is just repeated (iterative) application of Bellman&rsquo;s Operator to a state, that is guaranteed to converge to the Optimal Value Function:</p><p>$$ \pi_{\star}(s) = \arg \max_a Q_*(s, a)$$</p><p>$$ = \arg \max_a [R(s, a) + \gamma E_{p_S(s&rsquo;|s, a)} [V_*(s&rsquo;)]] $$</p><h2 id=summary>Summary
<a class=heading-link href=#summary><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>This is a lot to take in, but a summary is:
To define an Optimal Value function, which is a Value Function that has the highest average Reward for a given Markov Decision Process (Agent), we can use Bellman&rsquo;s Equations. These equations hold only if the Value Function is optimal. The difference between the Optimal Value Function and the (Not-Optimal) Value Function is the Bellman&rsquo;s Error. To minimize this Bellman&rsquo;s Error we define the Bellman&rsquo;s Operator, which is just a rule how to derive a new Value Function while minimizing Bellman&rsquo;s Error, and by repeating Bellman&rsquo;s Operator we converge to the Optimal Value Function.</p></div><footer><section class=see-also></section><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2020 -
2025
n1o_c0rTx
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ")}</script></body></html>