<!doctype html><html lang=en><head><title>RL Bite: Learning the Q Function · Data Artificer and code:Breaker
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="n1o_c0rTx"><meta name=description content="Abstract Link to heading We already know how to learn the Value function, however we also know that the Value function by itself is not enough since it averages over all possible actions, instead of taking into consideration specific actions the agent should take. We can derive the Q function from the Value function, however we can also try to directly learn it. Especially directly approximating the Q function with Deep Neural Networks has been a huge success when applied to RL Agents playing Atari computer games."><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="RL Bite: Learning the Q Function"><meta name=twitter:description content="Abstract Link to heading We already know how to learn the Value function, however we also know that the Value function by itself is not enough since it averages over all possible actions, instead of taking into consideration specific actions the agent should take. We can derive the Q function from the Value function, however we can also try to directly learn it. Especially directly approximating the Q function with Deep Neural Networks has been a huge success when applied to RL Agents playing Atari computer games."><meta property="og:url" content="https://n1o.github.io/posts/rl-bite-learning-the-q-function/"><meta property="og:site_name" content="Data Artificer and code:Breaker"><meta property="og:title" content="RL Bite: Learning the Q Function"><meta property="og:description" content="Abstract Link to heading We already know how to learn the Value function, however we also know that the Value function by itself is not enough since it averages over all possible actions, instead of taking into consideration specific actions the agent should take. We can derive the Q function from the Value function, however we can also try to directly learn it. Especially directly approximating the Q function with Deep Neural Networks has been a huge success when applied to RL Agents playing Atari computer games."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-03T08:59:26+01:00"><meta property="article:modified_time" content="2025-03-03T08:59:26+01:00"><meta property="article:tag" content="RL Bite"><meta property="article:tag" content="Q-Function"><meta property="article:tag" content="Neural Networks"><meta property="article:tag" content="Temporal Difference"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-computing-value-functions/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-bellmans-equations-and-value-functions/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-exploration-vs-exploitation/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-computing-value-functions/"><link rel=canonical href=https://n1o.github.io/posts/rl-bite-learning-the-q-function/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://n1o.github.io/>Data Artificer and code:Breaker
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Writing</a></li><li class=navigation-item><a class=navigation-link href=/awesome-t5/>Awesome T5</a></li><li class=navigation-item><a class=navigation-link href=/awesome-ssm/>Awesome SSM</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/rl-bite-learning-the-q-function/>RL Bite: Learning the Q Function</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2025-03-03T08:59:26+01:00>March 3, 2025
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
7-minute read</span></div><div class=categories><i class="fa-solid fa-folder" aria-hidden=true></i>
<a href=/categories/rl-bite/>RL Bite</a>
<span class=separator>•</span>
<a href=/categories/q-function/>Q-Function</a>
<span class=separator>•</span>
<a href=/categories/neural-networks/>Neural Networks</a>
<span class=separator>•</span>
<a href=/categories/temporal-difference/>Temporal Difference</a></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/rl-bite/>RL Bite</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/q-function/>Q-Function</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/neural-networks/>Neural Networks</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/temporal-difference/>Temporal Difference</a></span></div></div></header><div class=post-content><h1 id=abstract>Abstract
<a class=heading-link href=#abstract><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>We already know how to learn the Value function, however we also know that the Value function by itself is not enough since it averages over all possible actions, instead of taking into consideration specific actions the agent should take. We can derive the Q function from the Value function, however we can also try to directly learn it. Especially directly approximating the Q function with Deep Neural Networks has been a huge success when applied to RL Agents playing Atari computer games.</p><h1 id=sarsa>SARSA
<a class=heading-link href=#sarsa><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Sarsa is an on-policy method to learn the Q-function, thus it uses only the policy to choose the next action. Since it is an on-policy approach it may fail to learn the optimal policy $Q_{*}$, and later we are going to extend it to be off-policy, where we get stronger guarantees.</p><h2 id=equations>Equations
<a class=heading-link href=#equations><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>$$Q(s, a) \leftarrow Q(s, a) + \eta [r + \gamma Q(s^{\prime}, a^{\prime}) - Q(s, a)] $$</p><p>This is just the Temporal Difference Learning, where the agent follows its policy $\pi$ at every step generating $a^{\prime} \sim \pi(s^{\prime})$ yielding the following $(s,a,r,s^{\prime}, a^{\prime})$ (hence the name SARSA).</p><p>Once we updated $Q$ we update the policy $\pi$ to be greedy with respect to $Q$.</p><h2 id=convergence>Convergence
<a class=heading-link href=#convergence><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The bad news, in the general case, SARSA is not guaranteed to converge to $Q_{*}$ however for the special, Tabular case (finite discrete states, finite discrete actions), in the limit that every state-action pair is visited infinitely often we are guaranteed to converge. Remember we are on-policy, and we update the Q function (and therefore the policy) only for action-state pairs which we actually visit! This can be done by performing $\epsilon$ greedy exploration-exploitation strategy with gradually vanishing $\epsilon \rightarrow 0$.</p><h1 id=off-policy>Off-policy
<a class=heading-link href=#off-policy><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>As mentioned before SARSA is not guaranteed to converge, well actually Q learning has a lot of issues we are going to discuss later, but first things first. Off-policy approach enables us to use data, to train the agent from anywhere. This means we can use past training data, logs from some online system, imagination is the limit to train an RL Agent.</p><h2 id=tabular-q-learning>Tabular Q Learning
<a class=heading-link href=#tabular-q-learning><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>We make the slightest modification to the SARSA training objective:</p><p>$$Q(s, a) \leftarrow Q(s, a) + \eta [r + \gamma \max_{a^{\prime}} Q(s^{\prime}, a^{\prime}) - Q(s, a)] $$</p><ul><li>$a^{\prime} = \arg \max_{a}Q(s^{\prime}, a)$ replaces $a^{\prime} \sim \pi(s^{\prime})$</li></ul><p>As before we need to visit every state-action pair at once, however since we use an off-policy approach: $(s,a,r,s^{\prime})$ can come from anywhere, which speeds up things!</p><h2 id=q-learning-as-function-approximation>Q-Learning as Function approximation
<a class=heading-link href=#q-learning-as-function-approximation><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Here our Q function has some parameters we are going to optimize: $Q_w(s,a)$, where $w$ are those parameters. These parameters are then usually optimized with Stochastic Gradient Descent and our loss is:</p><p>$$ L(w | s, a, r, s^{\prime}) = ((r + \gamma \max_{a^{\prime}} Q_w(s^{\prime}, a^{\prime})) - Q_w(s, a))^2$$</p><h3 id=deep-q-learning>Deep Q Learning
<a class=heading-link href=#deep-q-learning><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Here is not too much to say, essentially $w$ are weights of a deep neural network. This may sound disappointing, and yes it is. I wanted to dive more in depth here, and I&rsquo;m going to give you a tease of the current latest and greatest at the end of the post. However I want to focus more on the problems and potential pitfalls that are present in Q learning!</p><h2 id=problems>Problems
<a class=heading-link href=#problems><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Off-policy methods and using Function approximation introduces a bunch of problems! These problems are:</p><ol><li>The Deadly Triad</li><li>Optimizer&rsquo;s Curse (Maximization Bias)</li></ol><h1 id=deadly-triad>Deadly Triad
<a class=heading-link href=#deadly-triad><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>In the context of Q-learning where we use Function Approximations (tabular case does not suffer from this, well it does, but it is less problematic), we may end up in a situation where the network is chasing its own tail. Let&rsquo;s explain, essentially if we use TD learning, we use the Q function itself to estimate the new Q function, because of this we may end up in a situation that the algorithm is trying to adjust its estimate based on its own, potentially flawed estimate, creating a self-reinforcing feedback loop, causing divergence. There are actually 3 components, when we may experience instability during Q learning:</p><ol><li><strong>Using Function Approximations</strong>, since we use SGD to update the weights, if we have a biased state-action pair (or multiple) by updating the weights we influence all possible state-action pairs. In the tabular case the updates to one do not influence the other.</li><li><strong>Using Bootstrapping</strong>, by using bootstrapping we do not minimize a fixed objective, we instead create a learning target using our own estimates. This can create a feedback loop pushing the estimates to infinity.</li><li><strong>Using Off-policy Learning</strong>, since the data we use does not come from the policy that we optimize we may diverge from the policy we want to learn. In on-policy methods this is not an issue, since by optimizing the Q function we also optimize the learned policy creating a self-consistency feedback.</li></ol><h2 id=how-to-fix>How to fix?
<a class=heading-link href=#how-to-fix><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Here are some of the methods:</p><ol><li><strong>Layer-norm</strong>, simply add a layer norm to the penultimate layer just before the linear head. This will force the weights $w$ to stay small, and it should be enough.</li><li><strong>Target Networks</strong></li></ol><h3 id=target-networks>Target Networks
<a class=heading-link href=#target-networks><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Here we have 2 Q functions, $Q_{w^-}$ and $Q_{w}$, where we train only $Q_{w}$ and periodically we set $w^{-} \leftarrow sg(w)$, here sg is stop gradient. Essentially $w^{-}$ are frozen all the time!</p><h1 id=optimizers-curse-maximization-bias>Optimizer&rsquo;s Curse (Maximization Bias)
<a class=heading-link href=#optimizers-curse-maximization-bias><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>There is non-trivial randomness involved in Q learning, since we use function approximations, we do not know the world model and actions $a$ we choose are stochastic. Because of this there is quite a chance that greedily picking an action that should pay most, we pick the wrong action due to random noise. This makes the learning way harder.</p><h2 id=double-q-learning>Double Q-Learning
<a class=heading-link href=#double-q-learning><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Double Q learning is an approach to avoid maximization bias where we have two separate Q-functions one is used to select a greedy action and the other for estimating the corresponding Q-value.</p><p>$$ Q_i(s, a) \leftarrow Q_i(s, a) + \eta (q_i(s, a) - Q_i(s, a)) $$
$$ q_i(s, a) = r + \gamma Q_i(s&rsquo;, \underset{a&rsquo;}{\text{argmax}} Q_{-i}(s&rsquo;, a&rsquo;))$$</p><p>For a given a transition tuple: $(s,a,r,s^{\prime})$ we use $Q_1$ to evaluate the action that $Q_2$ chooses.</p><p>And of course this can be applied to Deep Q learning as well.</p><p><img src=/images/double_q_learning.png></p><h1 id=bigger-better-faster>Bigger, Better, Faster
<a class=heading-link href=#bigger-better-faster><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>This relates to the <a href=https://arxiv.org/abs/2305.19452 class=external-link target=_blank rel=noopener>BBF</a> paper, which is the current benchmark for playing Atari games with Deep Q Networks (see we got there!). The authors propose the following tricks, ordered with decreasing importance:</p><ol><li>Larger CNN: Use a larger CNN with residual connections, specifically a modified Impala network</li><li>Increase UTD Ratio: Increase the update-to-data (UTD) ratio (number of Q-function updates per observation) to improve sample efficiency</li><li>Soft Reset Weights: Periodically perform a soft reset of (some) network weights to prevent loss of elasticity, using the SR-SPR method</li><li><strong>N-step Returns:</strong> Use n-step returns, gradually decreasing n from 10 to 3, to reduce bias.</li><li>Weight Decay: Add weight decay.</li><li>Self-Predictive Loss: Add a self-predictive representation loss to increase sample efficiency.</li><li>Increase Discount Factor: Gradually increase the discount factor (γ) from 0.97 to 0.997 to encourage longer-term planning.</li><li>Drop Noisy Nets: Remove noisy nets (as they increase memory usage without helping).</li><li>Dueling DQN: Use dueling DQN</li></ol><ul><li>In Dueling DQN we do not learn the Q function directly, but instead we learn a value function and advantage function and use them to derive the Q function.</li></ul><ol start=10><li>Distributional DQN: Use distributional DQN</li></ol><ul><li>Distributional DQN predicts not just a single point estimate of the (discounted) return, but the whole distribution.</li></ul><h1 id=final-remarks>Final Remarks
<a class=heading-link href=#final-remarks><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Q learning is popular and in some cases it is the state of the art solution, however as we are going to see in further posts, in some cases it may be advantageous to directly learn the agent&rsquo;s policy, or even go down and learn the full model.</p></div><footer><section class=see-also></section><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2020 -
2025
n1o_c0rTx
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ")}</script></body></html>