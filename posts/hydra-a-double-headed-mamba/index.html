<!doctype html><html lang=en><head><title>Hydra a Double Headed Mamba · Data Artificer and code:Breaker
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="n1o_c0rTx"><meta name=description content="
  Abstract
  
    
    Link to heading
  

State Space Models are awesome, models like Mamba and Mamba2 boast unparalleled performance especially when it comes to long sequences. The only downside is that they are causal, which means they model one token at a time, looking only at past tokens. Bidirectional models like Bert, CodeBERT and GraphCodeBERT have been shown to excel when it comes to code understanding. One way to put it is that by looking into the past and the future simultaneously we can get a better understanding of what is happening. Hydra is a bidirectional extension of Mamba2 that builds upon solid mathematical foundations, instead of just naively taking two Mamba(2)&rsquo;s, flipping one of them, and somehow combining them."><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="Hydra a Double Headed Mamba"><meta name=twitter:description content="Abstract Link to heading State Space Models are awesome, models like Mamba and Mamba2 boast unparalleled performance especially when it comes to long sequences. The only downside is that they are causal, which means they model one token at a time, looking only at past tokens. Bidirectional models like Bert, CodeBERT and GraphCodeBERT have been shown to excel when it comes to code understanding. One way to put it is that by looking into the past and the future simultaneously we can get a better understanding of what is happening. Hydra is a bidirectional extension of Mamba2 that builds upon solid mathematical foundations, instead of just naively taking two Mamba(2)’s, flipping one of them, and somehow combining them."><meta property="og:url" content="https://n1o.github.io/posts/hydra-a-double-headed-mamba/"><meta property="og:site_name" content="Data Artificer and code:Breaker"><meta property="og:title" content="Hydra a Double Headed Mamba"><meta property="og:description" content="Abstract Link to heading State Space Models are awesome, models like Mamba and Mamba2 boast unparalleled performance especially when it comes to long sequences. The only downside is that they are causal, which means they model one token at a time, looking only at past tokens. Bidirectional models like Bert, CodeBERT and GraphCodeBERT have been shown to excel when it comes to code understanding. One way to put it is that by looking into the past and the future simultaneously we can get a better understanding of what is happening. Hydra is a bidirectional extension of Mamba2 that builds upon solid mathematical foundations, instead of just naively taking two Mamba(2)’s, flipping one of them, and somehow combining them."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-29T11:53:51+02:00"><meta property="article:modified_time" content="2024-08-29T11:53:51+02:00"><meta property="article:tag" content="NLP"><meta property="article:tag" content="SSM"><meta property="og:see_also" content="https://n1o.github.io/posts/from-mamba-to-mamba2/"><link rel=canonical href=https://n1o.github.io/posts/hydra-a-double-headed-mamba/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://n1o.github.io/>Data Artificer and code:Breaker
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Writing</a></li><li class=navigation-item><a class=navigation-link href=/awesome-t5/>Awesome T5</a></li><li class=navigation-item><a class=navigation-link href=/awesome-ssm/>Awesome SSM</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/hydra-a-double-headed-mamba/>Hydra a Double Headed Mamba</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2024-08-29T11:53:51+02:00>August 29, 2024
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
5-minute read</span></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/nlp/>NLP</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/ssm/>SSM</a></span></div></div></header><div class=post-content><h1 id=abstract>Abstract
<a class=heading-link href=#abstract><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>State Space Models are awesome, models like Mamba and Mamba2 boast unparalleled performance especially when it comes to long sequences. The only downside is that they are causal, which means they model one token at a time, looking only at past tokens. Bidirectional models like <a href=https://codebreakers.re/articles/detail/bert-codebert-and-graphcodebert/ class=external-link target=_blank rel=noopener>Bert, CodeBERT and GraphCodeBERT</a> have been shown to excel when it comes to code understanding. One way to put it is that by looking into the past and the future simultaneously we can get a better understanding of what is happening. Hydra is a bidirectional extension of Mamba2 that builds upon solid mathematical foundations, instead of just naively taking two Mamba(2)&rsquo;s, flipping one of them, and somehow combining them.</p><h1 id=quasi-separable-matrices>Quasi-Separable Matrices
<a class=heading-link href=#quasi-separable-matrices><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>We build upon research done in <a href=/posts/from-mamba-to-mamba2/>State Space Duality</a> and the Mamba2 model. The main idea of Mamba2 is to express a State Space Model as:</p><p>$$ y = M \cdot x $$</p><ul><li>$M = SSS(A,B,C)$ is a Sequentially Semi-Separable Matrix</li></ul><p>As a reminder, an N Semi-Separable Matrix is a lower triangular matrix where every submatrix contained in the lower triangular part is at most rank N. We can express any N Semi-Separable matrix as an N Sequentially Semi-Separable matrix. This just means that every entry in $M_{ij}$ is a product of vectors and matrices:</p><p>$$M_{ij} = C_j^A_j \cdots A_{i+1}B_i$$</p><ul><li>$B_0,\cdots, B_{T-1}, C_0, \cdots, C_{T-1} \in R^{N}$ are vectors</li><li>$A_0, \cdots, A_{T-1} \in R^{N,N}$ are matrices</li></ul><p>Since Semi-Separable matrices are lower triangular matrices, we can view them as a sort of causal attention mask. Since we are also interested in the future tokens, we need an matrix, that is non-zero above the main diagonal as well as below.</p><h2 id=definition>Definition
<a class=heading-link href=#definition><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Quasi-Separable Matrices consist of a lower and upper triangular matrix, with an special vector on the main diagonal.</p><p>$$ m_{ij} = \begin{cases} \xrightarrow[c_i^T ]{} \xrightarrow[A_{i:j}^x]{} \xrightarrow[b_j]{} && \text{ if } i> j \\ \delta_i && \text{ if } i = j \\ \xleftarrow[c_j^T ]{} \xleftarrow[A_{j:i}^x]{} \xleftarrow[b_i]{} && \text{ if } i&lt;j \end{cases} $$</p><ul><li>$\delta_i$ is a scalar</li><li>$b_i, c_i \in R^{N \times 1}$</li><li>$A_i \in R^{N \times N}$</li></ul><p>Here is an image so we can compare the two:</p><p><img alt="Semi Vs Quasi" src=/images/semi_vs_quasi_separable.png></p><h1 id=hydra>Hydra
<a class=heading-link href=#hydra><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Right, so we know we need a Quasi-Separable Matrix, but how do we construct one? The answer is surprisingly simple, we just take two Semi-Separable Matrices and we masage them a bit.</p><p>$$QS(X) = \text{shift}(\text{SS}(X)) + \text{flip}(\text{shift}(\text{SS}(\text{flip}(X)))) + DX$$</p><ul><li>$SS$ is a semi-separable matrix, and in our case booth share the parameters ${A_i, b_i, c_i }_L$</li><li>$D = \text{diag}(\delta_1, \cdots, \delta_L)$ are the diagonal parameters of the quasi-separable matrix</li><li>$\text{flip}$ reverses the input</li><li>$\text{shift}$ shifts the sequence to the right by one position, and pads the beginning with zeros</li></ul><h2 id=model>Model
<a class=heading-link href=#model><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>To get the intuition behind the model, we can look at the following pseudo code:</p><div class=highlight><pre tabindex=0 style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fff;font-weight:700>def</span> hydra (
</span></span><span style=display:flex><span>  x , <span style=color:#007f7f># (B ,L ,H*P)</span>
</span></span><span style=display:flex><span>  A <span style=color:#007f7f># (H ,) Parameter</span>
</span></span><span style=display:flex><span>  ):
</span></span><span style=display:flex><span>  x_b = flip (x , dim =<span style=color:#ff0;font-weight:700>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  dt_f , dt_b = proj_dt (x) , proj_dt ( x_b ) <span style=color:#007f7f># (B ,L ,H)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  y_f = SSD( <span style=color:#007f7f># (B ,L ,H*P)</span>
</span></span><span style=display:flex><span>    x,
</span></span><span style=display:flex><span>    discretize_A (A , dt_f ) , <span style=color:#007f7f># (B ,L ,H)</span>
</span></span><span style=display:flex><span>    discretize_bc (x , dt_f ) , <span style=color:#007f7f># (B ,L ,N)</span>
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  y_b = SSD(
</span></span><span style=display:flex><span>    x_b ,
</span></span><span style=display:flex><span>    discretize_A(A , dt_b) , <span style=color:#007f7f># (B ,L ,H)</span>
</span></span><span style=display:flex><span>    discretize_bc(x_b, dt_b) , <span style=color:#007f7f># (B ,L ,N)</span>
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  y_f = shift(y_f , dim =<span style=color:#ff0;font-weight:700>1</span>)
</span></span><span style=display:flex><span>  y_b = flip(shift(y_b , dim =<span style=color:#ff0;font-weight:700>1</span>), dim =<span style=color:#ff0;font-weight:700>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  y = y_f + y_b + x * repeat(
</span></span><span style=display:flex><span>    proj_D (x) ,
</span></span><span style=display:flex><span>    <span style=color:#0ff;font-weight:700>&#34;B L H -&gt; B L (H P)&#34;</span>
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  <span style=color:#fff;font-weight:700>return</span> y
</span></span></code></pre></div><ul><li>more on <a href=/posts/from-mamba-to-mamba2/#pytorch>SSD</a></li><li>the shift is required to make place for the diagonal entry of Matrix D</li></ul><p>Here is the detailed description of a Hydra layer:</p><p><img alt=Hydra src=/images/hydra_model.png></p><p>It is a bit (way) more involved, here is the actual <a href=https://github.com/goombalab/hydra/blob/main/hydra/modules/hydra.py class=external-link target=_blank rel=noopener>[Source Code]</a>.</p><p>We have initial projections of the input, 1D convolutions, discretization, and flipping. We multiply it with the actual Semi-Separable matrix once for the forward and once for the backward direction. We do some elementwise product (selective gating), shifting, and merging the results together. That is followed by a normalization and a residual connection with the original input.</p><h3 id=pretraining>Pretraining
<a class=heading-link href=#pretraining><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Hydra uses the standard masked language modeling objective, where we mask out some tokens and try to predict them.</p><h3 id=remarks>Remarks
<a class=heading-link href=#remarks><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>One thing that stands out is that the implementation, in comparison to BERT, is much more complex.</p><h3 id=cls-token>CLS Token
<a class=heading-link href=#cls-token><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>The authors use a special pooling technique to average out the CLS token.</p><h2 id=efficiency>Efficiency
<a class=heading-link href=#efficiency><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>It is worth mentioning that we share a single Semi-Separable matrix for the forward and backward direction. Because of this, Hydra introduces only a few more parameters (the diagonal matrix D) than Mamba2.</p><p>Unfortunately, the authors do not provide any empirical results on the runtime or memory usage of Hydra. But since it uses the same building block as Mamba2, we can expect it to give us gains especially on long sequences. I would like to see some benchmarks on the runtime and memory, especially compared to FlashAttention</p><h2 id=performance>Performance
<a class=heading-link href=#performance><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>On the reported benchmarks, the performance of Hydra was higher than BERT across all tasks, with a couple of exceptions. Just having an alternative to bidirectional attention that is on par with BERT is a huge win.</p><h1 id=conclusion>Conclusion
<a class=heading-link href=#conclusion><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>It is nice to see adaptation of SSMs also to bidirectional models. The research is only in its early stages, and only time will tell if Hydra will be applied in practice. There is a lot of research on hybrid models that combine State Space Models with Attention; especially the combination of Hydra and M2 could result in a powerful model that can handle extremely long sequences and still remain efficient.</p></div><footer><section class=see-also></section><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2020 -
2024
n1o_c0rTx
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ")}</script></body></html>