<!doctype html><html lang=en><head><title>Hierarchical Probabilistic Matrix Factorization · Data, Engineering and Stuff</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Marek Barak"><meta name=description content="Hierarchical Probabilistic Matrix Factorization. An example implementation in Pyro."><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Hierarchical Probabilistic Matrix Factorization"><meta name=twitter:description content="Hierarchical Probabilistic Matrix Factorization. An example implementation in Pyro."><meta property="og:title" content="Hierarchical Probabilistic Matrix Factorization"><meta property="og:description" content="Hierarchical Probabilistic Matrix Factorization. An example implementation in Pyro."><meta property="og:type" content="article"><meta property="og:url" content="https://n1o.github.io/posts/pooled_matrix_factorization/"><meta property="article:published_time" content="2020-12-17T10:19:42+01:00"><meta property="article:modified_time" content="2020-12-17T10:19:42+01:00"><link rel=canonical href=https://n1o.github.io/posts/pooled_matrix_factorization/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz+OMl98=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.126ad3988d46bdae6217a11105b53c9662bca05f39d42d3c0fb366919d334620.css integrity="sha256-EmrTmI1Gva5iF6ERBbU8lmK8oF851C08D7NmkZ0zRiA=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5><meta name=generator content="Hugo 0.79.1"></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>Data, Engineering and Stuff</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/pooled_matrix_factorization/>Hierarchical Probabilistic Matrix Factorization</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i><time datetime=2020-12-17T10:19:42+01:00>December 17, 2020</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>6-minute read</span></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i><span class=tag><a href=/tags/probabilistic-programming/>probabilistic programming</a></span>
<span class=separator>•</span>
<span class=tag><a href=/tags/pyro/>pyro</a></span></div></div></header><div class=post-content><p>Probabilistic Matrix factorization is a simple but useful model for matrix imputation. The main idea is to decompose a tall and wide matrix into a product of two matrices, one tall and thin and one short and wide.</p><p>$$
R_{n\times m} = U_{m \times d} \cdot V_{d \times n}
$$</p><p><img src=/images/matrix_factorization.svg alt="Matrix Factorization"></p><p>If you are a Bayesian, you can express this model as:</p><p>$$
R_{ij} \sim \mathcal{N}(u_i \cdot v_j^T, \sigma)
$$
$$
u_i \sim \mathcal{N}(\mu_u, \Sigma_u)
$$
$$
v_j \sim \mathcal{N}(\mu_v, \Sigma_v)
$$</p><ul><li>$u_i$ is row in matrix $U$</li><li>$v_j$ is a column in matrix $V$</li><li>$R_{ij}$ is an entry in matrix R</li></ul><p>We can use this model when R is sparse, to try to fill in the missing entries. This is something that often occurs in practice. Let&rsquo;s look into the following example:</p><blockquote><p>You are working for an Advertisement Video on Demand (AVOD) company. The company is young, and there is a large marketing effort to acquire new users. Because of this, there are some challenges:</p><ul><li>Each day the proportion of new users to recurring users is heavily skewed towards new users.</li><li>We expect to have a high churn rate</li></ul><p>Our goal is to build a video recommendation engine.</p></blockquote><p>Building a recommendation engine is essentially the same as performing matrix imputations. Therefore our first goal is to find the right matrix $R$. In our case, $R$ has a row for every user that has watched something, and a column for every video that has been watched by someone. The entries of the matrix are the number of minutes a user spent watching the video. After some data wrangling we end up with this:</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback>video_id	0	1	2	3	4	5	6	7	8	9	...	555	556	557	558	559	560	561	562	563	564
user_id																					
0	1.0	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	...	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
1	1.0	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	...	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
2	NaN	3.0	NaN	NaN	NaN	NaN	NaN	8.0	NaN	NaN	...	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
3	NaN	4.0	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	...	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
4	NaN	1.0	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	...	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
5 rows × 565 columns
</code></pre></div><p>That table has a lot of <code>NaN</code> entries. The fraction of observed entries is:</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#ff0;font-weight:700>1</span> - np.sum(np.sum(pd.isna(pivot))) / (pivot.shape[<span style=color:#ff0;font-weight:700>0</span>] * pivot.shape[<span style=color:#ff0;font-weight:700>1</span>])
<span style=color:#ff0;font-weight:700>0.004108162504450008</span>
</code></pre></div><p>This is to be expected since most users watch only a couple of videos (the most popular ones, or videos that are somehow promoted).</p><p>Our goal is to replace all those <code>NaN</code> numbers with predictions. We could just continue and apply matrix factorization, or even probabilistic matrix factorization, but since we are Bayesian (or at least I am) we can do better and build a hierarchical model. Hierarchical modeling enables data sharing between groups. This allows groups with a small number of observations to borrow statistical strength from groups that have a large number of observations. In our case, it allows users that have seen only a few videos, to borrow information from users that are frequent watchers. To build a hierarchical model we have to define a prior over the distribution of users. Our model turns out to be:</p><p>$$
R_{ij} \sim \mathcal{N}(u_i \cdot v_j^T, \sigma)
$$
$$
v_j \sim \mathcal{N}(\mu_v, \Sigma_v)
$$
$$
u_i \sim \mathcal{N}(\mu_u, \Sigma_u)
$$
$$
\mu_u \sim \mathcal{N}(\mu_0, \Sigma_0)
$$</p><p>We could also define a prior for videos, which would pool less popular videos towards the more popular. But a a bit of good advice is to start simple and grow more complicated in time.</p><p>To get our hands a bit dirty let&rsquo;s look at the model implementation in <a href=http://pyro.ai/>Pyro</a>.</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python> <span style=color:#fff;font-weight:700>def</span> model(alpha, dim, n, m, nan_mask, not_na, data):
        <span style=color:#0ff;font-weight:700>&#34;&#34;&#34;
</span><span style=color:#0ff;font-weight:700>        Perform matrix factorization
</span><span style=color:#0ff;font-weight:700>        R = U @ V.T
</span><span style=color:#0ff;font-weight:700>        &#34;&#34;&#34;</span>
        alpha_loc = torch.tensor(<span style=color:#ff0;font-weight:700>1</span> / <span style=color:#ff0;font-weight:700>25</span>)

        loc_u = pyro.sample(
            <span style=color:#0ff;font-weight:700>&#34;loc_u&#34;</span>,
            dist.MultivariateNormal(
                loc=torch.zeros(dim),
                precision_matrix=torch.eye(dim) * alpha_loc,
            ),
        )
        precission_u = pyro.sample(
            <span style=color:#0ff;font-weight:700>&#34;precission_u&#34;</span>,
            dist.LKJCorrCholesky(
                d=dim, eta=torch.tensor(alpha)
            ),
        )

        observations_scale = pyro.sample(
            <span style=color:#0ff;font-weight:700>&#34;obs_scale&#34;</span>,
            dist.InverseGamma(
                concentration=torch.tensor(<span style=color:#ff0;font-weight:700>1.0</span>),
                rate=torch.tensor(<span style=color:#ff0;font-weight:700>1.0</span>),
            ),
        )

        <span style=color:#fff;font-weight:700>with</span> pyro.plate(<span style=color:#0ff;font-weight:700>&#34;users&#34;</span>, n):
            U = pyro.sample(
                <span style=color:#0ff;font-weight:700>&#34;U&#34;</span>, 
                dist.MultivariateNormal(
                    loc=loc_u, 
                    precision_matrix=precission_u
                )
            )
        <span style=color:#fff;font-weight:700>with</span> pyro.plate(<span style=color:#0ff;font-weight:700>&#34;content&#34;</span>, m):
            V = pyro.sample(
                <span style=color:#0ff;font-weight:700>&#34;V&#34;</span>, dist.MultivariateNormal(
                    loc=torch.zeros(dim), 
                    precision_matrix=torch.eye(dim)
                )
            )
        <span style=color:#fff;font-weight:700>with</span> pyro.plate(<span style=color:#0ff;font-weight:700>&#34;observations&#34;</span>, not_na):
            R = pyro.sample(
                <span style=color:#0ff;font-weight:700>&#34;R&#34;</span>,
                dist.Normal(loc=(U <span style=color:red>@</span> V.T)[~nan_mask], scale=observations_scale),
                obs=data,
            )
</code></pre></div><p>Here we use vectorized operations instead of for loops for performance reasons. To perform inference we use Stochastic Variational Inference (SVI) with an AutoDiagonalNormal guide. For some of you this may sound scary, and I am planning to release a series of posts where we will look into Variational Inference (VI) in depth. To give you the long story short: Bayesian inference is about finding the distribution of parameters of interest. In Variational Inference we approximate this distribution by a set of simpler distribution, and we use optimization to make our approximation tight. Anyway lets get back to code:</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>pyro.clear_param_store()
guide = AutoDiagonalNormal(model)
svi = SVI(model, guide, Adam({<span style=color:#0ff;font-weight:700>&#34;lr&#34;</span>: <span style=color:#ff0;font-weight:700>0.001</span>}), loss=Trace_ELBO())
n, m = train.shape
dim = <span style=color:#ff0;font-weight:700>5</span>
alpha = <span style=color:#ff0;font-weight:700>2.0</span>
iterations = <span style=color:#ff0;font-weight:700>3000</span>
train_loss = []
<span style=color:#fff;font-weight:700>for</span> i in <span style=color:#fff;font-weight:700>range</span>(iterations):
    loss = svi.step(alpha, dim, n, m, nan_mask, not_na, data)
    train_loss.append(loss / <span style=color:#fff;font-weight:700>len</span>(data))
</code></pre></div><p>After the optimization is done, we can use the guide to retrieve our parameters of interest. To perform matrix completion we are interested in matrix V and U.</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>V = guide.median()[<span style=color:#0ff;font-weight:700>&#39;V&#39;</span>]
U = guide.median()[<span style=color:#0ff;font-weight:700>&#39;U&#39;</span>]

R = U <span style=color:red>@</span> V.T
</code></pre></div><p>Matrix R holds the predicted number of minutes for all the users and all the movies. We can take these predictions and store them in a database and serve them to users when needed.</p><p>We managed to get some predictions for users that we have seen before, now we can turn our attention to new users. This may sound complex, but since we have defined a distribution above all the users, we can use this distribution to generate new users.</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>loc = guide.median()[<span style=color:#0ff;font-weight:700>&#39;loc_u&#39;</span>]
precision_matrix=guide.median()[<span style=color:#0ff;font-weight:700>&#39;precission_u&#39;</span>]
U_pooled = dist.MultivariateNormal(loc=loc, precision_matrix=precision_matrix)

generated_users = torch.from_numpy(
    np.array([U_pooled.sample().detach().numpy() <span style=color:#fff;font-weight:700>for</span> _ in <span style=color:#fff;font-weight:700>range</span>(<span style=color:#ff0;font-weight:700>1000</span>)])
)
</code></pre></div><p>Here we generated 1000 users from the posterior distribution. Each user is a sample from a five-dimensional Gaussian. To get their potential number of watched minutes, we have to multiply the generated samples with our matrix V.</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>potential_watched_minutes = (generated_users <span style=color:red>@</span> V.T)
potential_watched_minutes.shape
&gt;&gt;&gt; torch.Size([<span style=color:#ff0;font-weight:700>1000</span>, <span style=color:#ff0;font-weight:700>565</span>])
</code></pre></div><p>The matrix potential_watched_minutes represents the predicted watched minutes for each movie in our repository. Since we do not expect to see any of those generated users, a good strategy is to average them out.</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>average_potential_watched_minutes = potential_watched_minutes.mean(axis = <span style=color:#ff0;font-weight:700>0</span>)
average_potential_watched_minutes.shape
&gt;&gt;&gt; torch.Size([<span style=color:#ff0;font-weight:700>565</span>])

</code></pre></div><p>Here we ended with a vector. The ith entry of the vector is the average number of minutes we expect a random, not seen before user will spend watching the ith movie. We can store this vector (similarly to vector R), save it into a database, and serve it as predictions for a new user.</p><p>To wrap things up, probabilistic programming gives us superpowers. We can handle potential cases we have not seen before, by generating data from our model and use those generated samples to make decisions.</p><p>The full code example can be found <a href=https://github.com/n1o/n1o.github.io/blob/master/notebooks/pooled_matrix_factorization.ipynb>here</a>.</p></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"mbarak-io"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();document.addEventListener('themeChanged',function(e){if(document.readyState=='complete'){DISQUS.reset({reload:true,config:disqus_config});}});</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false},{left:'\\(',right:'\\)',display:false},{left:'\\[',right:'\\]',display:true}]});"></script></section></div><footer class=footer><section class=container>©
202 -
2023
Marek Barak
·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=/js/coder.min.b764d18be7e43ab6b14e5ae4a9ebc7a6d3c333faa951aa702200a71d62264190.js integrity="sha256-t2TRi+fkOraxTlrkqevHptPDM/qpUapwIgCnHWImQZA="></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','G-5WLCXX3LGJ','auto');ga('send','pageview');}</script></body></html>