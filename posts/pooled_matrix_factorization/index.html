<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=content-language content="en"><meta name=author content="Marek Barak"><meta name=description content="Hierarchical Probabilistic Matrix Factorization. An example implementation in Pyro."><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Hierarchical Probabilistic Matrix Factorization"><meta name=twitter:description content="Hierarchical Probabilistic Matrix Factorization. An example implementation in Pyro."><meta property="og:title" content="Hierarchical Probabilistic Matrix Factorization"><meta property="og:description" content="Hierarchical Probabilistic Matrix Factorization. An example implementation in Pyro."><meta property="og:type" content="article"><meta property="og:url" content="http://fullstackdatascience.tech/posts/pooled_matrix_factorization/"><meta property="article:published_time" content="2020-12-17T10:19:42+01:00"><meta property="article:modified_time" content="2020-12-17T10:19:42+01:00"><title>Hierarchical Probabilistic Matrix Factorization · Full Stack DataScience</title><link rel=canonical href=http://fullstackdatascience.tech/posts/pooled_matrix_factorization/><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/normalize.css@8/normalize.min.css><link rel=stylesheet href=/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz+OMl98=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.126ad3988d46bdae6217a11105b53c9662bca05f39d42d3c0fb366919d334620.css integrity="sha256-EmrTmI1Gva5iF6ERBbU8lmK8oF851C08D7NmkZ0zRiA=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><script defer src=https://twemoji.maxcdn.com/v/13.0.1/twemoji.min.js integrity=sha384-5f4X0lBluNY/Ib4VhGx0Pf6iDCF99VGXJIyYy7dDLY5QlEd7Ap0hICSSZA1XYbc4 crossorigin=anonymous></script><meta name=generator content="Hugo 0.79.1"></head><body class=colorscheme-auto onload=twemoji.parse(document.body);><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>Full Stack DataScience</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title>Hierarchical Probabilistic Matrix Factorization</h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i><time datetime=2020-12-17T10:19:42+01:00>December 17, 2020</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>6-minute read</span></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i><a href=/tags/probabilistic-programming/>probabilistic programming</a>
<span class=separator>•</span>
<a href=/tags/pyro/>pyro</a></div></div></header><div><p>Probabilistic Matrix factorization is a simple but useful model for matrix imputation. The main idea is to decompose a tall and wide matrix into a product of two matrices, one tall and thin and one short and wide.</p><p>$$
R_{n\times m} = U_{m \times d} \cdot V_{d \times n}
$$</p><p><img src=/images/matrix_factorization.svg alt="Matrix Factorization"></p><p>If you are a Bayesian, you can express this model as:</p><p>$$
R_{ij} \sim \mathcal{N}(u_i \cdot v_j^T, \sigma)
$$
$$
u_i \sim \mathcal{N}(\mu_u, \Sigma_u)
$$
$$
v_j \sim \mathcal{N}(\mu_v, \Sigma_v)
$$</p><ul><li>$u_i$ is row in matrix $U$</li><li>$v_j$ is a column in matrix $V$</li><li>$R_{ij}$ is an entry in matrix R</li></ul><p>We can use this model when R is sparse, to try to fill in the missing entries. This is something that often occurs in practice. Let&rsquo;s look into the following example:</p><blockquote><p>You are working for an Advertisement Video on Demand (AVOD) company. The company is young, and there is a large marketing effort to acquire new users. Because of this, there are some challenges:</p><ul><li>Each day the proportion of new users to recurring users is heavily skewed towards new users.</li><li>We expect to have a high churn rate</li></ul><p>Our goal is to build a video recommendation engine.</p></blockquote><p>Building a recommendation engine is essentially the same as performing matrix imputations. Therefore our first goal is to find the right matrix $R$. In our case, $R$ has a row for every user that has watched something, and a column for every video that has been watched by someone. The entries of the matrix are the number of minutes a user spent watching the video. After some data wrangling we end up with this:</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback>video_id	0	1	2	3	4	5	6	7	8	9	...	555	556	557	558	559	560	561	562	563	564
user_id																					
0	1.0	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	...	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
1	1.0	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	...	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
2	NaN	3.0	NaN	NaN	NaN	NaN	NaN	8.0	NaN	NaN	...	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
3	NaN	4.0	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	...	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
4	NaN	1.0	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	...	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN
5 rows × 565 columns
</code></pre></div><p>That table has a lot of <code>NaN</code> entries. The fraction of observed entries is:</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#ff0;font-weight:700>1</span> - np.sum(np.sum(pd.isna(pivot))) / (pivot.shape[<span style=color:#ff0;font-weight:700>0</span>] * pivot.shape[<span style=color:#ff0;font-weight:700>1</span>])
<span style=color:#ff0;font-weight:700>0.004108162504450008</span>
</code></pre></div><p>This is to be expected since most users watch only a couple of videos (the most popular ones, or videos that are somehow promoted).</p><p>Our goal is to replace all those <code>NaN</code> numbers with predictions. We could just continue and apply matrix factorization, or even probabilistic matrix factorization, but since we are Bayesian (or at least I am) we can do better and build a hierarchical model. Hierarchical modeling enables data sharing between groups. This allows groups with a small number of observations to borrow statistical strength from groups that have a large number of observations. In our case, it allows users that have seen only a few videos, to borrow information from users that are frequent watchers. To build a hierarchical model we have to define a prior over the distribution of users. Our model turns out to be:</p><p>$$
R_{ij} \sim \mathcal{N}(u_i \cdot v_j^T, \sigma)
$$
$$
v_j \sim \mathcal{N}(\mu_v, \Sigma_v)
$$
$$
u_i \sim \mathcal{N}(\mu_u, \Sigma_u)
$$
$$
\mu_u \sim \mathcal{N}(\mu_0, \Sigma_0)
$$</p><p>We could also define a prior for videos, which would pool less popular videos towards the more popular. But a a bit of good advice is to start simple and grow more complicated in time.</p><p>To get our hands a bit dirty let&rsquo;s look at the model implementation in <a href=http://pyro.ai/>Pyro</a>.</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python> <span style=color:#fff;font-weight:700>def</span> model(alpha, dim, n, m, nan_mask, not_na, data):
        <span style=color:#0ff;font-weight:700>&#34;&#34;&#34;
</span><span style=color:#0ff;font-weight:700>        Perform matrix factorization
</span><span style=color:#0ff;font-weight:700>        R = U @ V.T
</span><span style=color:#0ff;font-weight:700>        &#34;&#34;&#34;</span>
        alpha_loc = torch.tensor(<span style=color:#ff0;font-weight:700>1</span> / <span style=color:#ff0;font-weight:700>25</span>)

        loc_u = pyro.sample(
            <span style=color:#0ff;font-weight:700>&#34;loc_u&#34;</span>,
            dist.MultivariateNormal(
                loc=torch.zeros(dim),
                precision_matrix=torch.eye(dim) * alpha_loc,
            ),
        )
        precission_u = pyro.sample(
            <span style=color:#0ff;font-weight:700>&#34;precission_u&#34;</span>,
            dist.LKJCorrCholesky(
                d=dim, eta=torch.tensor(alpha)
            ),
        )

        observations_scale = pyro.sample(
            <span style=color:#0ff;font-weight:700>&#34;obs_scale&#34;</span>,
            dist.InverseGamma(
                concentration=torch.tensor(<span style=color:#ff0;font-weight:700>1.0</span>),
                rate=torch.tensor(<span style=color:#ff0;font-weight:700>1.0</span>),
            ),
        )

        <span style=color:#fff;font-weight:700>with</span> pyro.plate(<span style=color:#0ff;font-weight:700>&#34;users&#34;</span>, n):
            U = pyro.sample(
                <span style=color:#0ff;font-weight:700>&#34;U&#34;</span>, 
                dist.MultivariateNormal(
                    loc=loc_u, 
                    precision_matrix=precission_u
                )
            )
        <span style=color:#fff;font-weight:700>with</span> pyro.plate(<span style=color:#0ff;font-weight:700>&#34;content&#34;</span>, m):
            V = pyro.sample(
                <span style=color:#0ff;font-weight:700>&#34;V&#34;</span>, dist.MultivariateNormal(
                    loc=torch.zeros(dim), 
                    precision_matrix=torch.eye(dim)
                )
            )
        <span style=color:#fff;font-weight:700>with</span> pyro.plate(<span style=color:#0ff;font-weight:700>&#34;observations&#34;</span>, not_na):
            R = pyro.sample(
                <span style=color:#0ff;font-weight:700>&#34;R&#34;</span>,
                dist.Normal(loc=(U <span style=color:red>@</span> V.T)[~nan_mask], scale=observations_scale),
                obs=data,
            )
</code></pre></div><p>Here we use vectorized operations instead of for loops for performance reasons. To perform inference we use Stochastic Variational Inference (SVI) with an AutoDiagonalNormal guide. For some of you this may sound scary, and I am planning to release a series of posts where we will look into Variational Inference (VI) in depth. To give you the long story short: Bayesian inference is about finding the distribution of parameters of interest. In Variational Inference we approximate this distribution by a set of simpler distribution, and we use optimization to make our approximation tight. Anyway lets get back to code:</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>pyro.clear_param_store()
guide = AutoDiagonalNormal(model)
svi = SVI(model, guide, Adam({<span style=color:#0ff;font-weight:700>&#34;lr&#34;</span>: <span style=color:#ff0;font-weight:700>0.001</span>}), loss=Trace_ELBO())
n, m = train.shape
dim = <span style=color:#ff0;font-weight:700>5</span>
alpha = <span style=color:#ff0;font-weight:700>2.0</span>
iterations = <span style=color:#ff0;font-weight:700>3000</span>
train_loss = []
<span style=color:#fff;font-weight:700>for</span> i in <span style=color:#fff;font-weight:700>range</span>(iterations):
    loss = svi.step(alpha, dim, n, m, nan_mask, not_na, data)
    train_loss.append(loss / <span style=color:#fff;font-weight:700>len</span>(data))
</code></pre></div><p>After the optimization is done, we can use the guide to retrieve our parameters of interest. To perform matrix completion we are interested in matrix V and U.</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>V = guide.median()[<span style=color:#0ff;font-weight:700>&#39;V&#39;</span>]
U = guide.median()[<span style=color:#0ff;font-weight:700>&#39;U&#39;</span>]

R = U <span style=color:red>@</span> V.T
</code></pre></div><p>Matrix R holds the predicted number of minutes for all the users and all the movies. We can take these predictions and store them in a database and serve them to users when needed.</p><p>We managed to get some predictions for users that we have seen before, now we can turn our attention to new users. This may sound complex, but since we have defined a distribution above all the users, we can use this distribution to generate new users.</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>loc = guide.median()[<span style=color:#0ff;font-weight:700>&#39;loc_u&#39;</span>]
precision_matrix=guide.median()[<span style=color:#0ff;font-weight:700>&#39;precission_u&#39;</span>]
U_pooled = dist.MultivariateNormal(loc=loc, precision_matrix=precision_matrix)

generated_users = torch.from_numpy(
    np.array([U_pooled.sample().detach().numpy() <span style=color:#fff;font-weight:700>for</span> _ in <span style=color:#fff;font-weight:700>range</span>(<span style=color:#ff0;font-weight:700>1000</span>)])
)
</code></pre></div><p>Here we generated 1000 users from the posterior distribution. Each user is a sample from a five-dimensional Gaussian. To get their potential number of watched minutes, we have to multiply the generated samples with our matrix V.</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>potential_watched_minutes = (generated_users <span style=color:red>@</span> V.T)
potential_watched_minutes.shape
&gt;&gt;&gt; torch.Size([<span style=color:#ff0;font-weight:700>1000</span>, <span style=color:#ff0;font-weight:700>565</span>])
</code></pre></div><p>The matrix potential_watched_minutes represents the predicted watched minutes for each movie in our repository. Since we do not expect to see any of those generated users, a good strategy is to average them out.</p><div class=highlight><pre style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>average_potential_watched_minutes = potential_watched_minutes.mean(axis = <span style=color:#ff0;font-weight:700>0</span>)
average_potential_watched_minutes.shape
&gt;&gt;&gt; torch.Size([<span style=color:#ff0;font-weight:700>565</span>])

</code></pre></div><p>Here we ended with a vector. The ith entry of the vector is the average number of minutes we expect a random, not seen before user will spend watching the ith movie. We can store this vector (similarly to vector R), save it into a database, and serve it as predictions for a new user.</p><p>To wrap things up, probabilistic programming gives us superpowers. We can handle potential cases we have not seen before, by generating data from our model and use those generated samples to make decisions.</p><p>The full code example can be found <a href=https://github.com/n1o/n1o.github.io/blob/master/notebooks/pooled_matrix_factorization.ipynb>here</a>.</p></div><footer><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='//'+"mbarak-io"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false},{left:'\\(',right:'\\)',display:false},{left:'\\[',right:'\\]',display:true}]});"></script></section></div></main><script src=/js/dark-mode.min.0213e1773e6d1c5a644f847c67a6f8abac49a3776e2976f6008038af8c5b76a1.js></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-188036512-1','auto');ga('send','pageview');}</script><script>(function(f,a,t,h,o,m){a[h]=a[h]||function(){(a[h].q=a[h].q||[]).push(arguments)};o=f.createElement('script'),m=f.getElementsByTagName('script')[0];o.async=1;o.src=t;o.id='fathom-script';m.parentNode.insertBefore(o,m)})(document,window,'//analytics.example.com/tracker.js','fathom');fathom('set','siteId','ABCDE');fathom('trackPageview');</script><script async defer data-domain=example.com src=https://analytics.example.com/js/plausible.js></script><script data-goatcounter=https://code.goatcounter.com/count async src=//gc.zgo.at/count.js></script><script defer src=https://static.cloudflareinsights.com/beacon.min.js data-cf-beacon='{"token": "token"}'></script></body></html>