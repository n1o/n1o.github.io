<!doctype html><html lang=en><head><title>Transform any LLMs to a powerful Encoder · Data Artificer and code:Breaker
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="n1o_c0rTx"><meta name=description content="
  Abstract
  
    
    Link to heading
  

In the last two years there has been a surge of Large Language Models. This is understandable, since LLMs are amazing at generating text, and a lot of things can be viewed as text. However, generation is not always all we need - sometimes we want to have semantically rich representations. In general, LLMs are not the best tool to get semantically rich representations, and even today models like BERT are used to achieve state of the art text embeddings. A natural question is to ask why BERT is so much better at embeddings than LLMs? Here are the two main reasons:"><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="Transform any LLMs to a powerful Encoder"><meta name=twitter:description content="Abstract Link to heading In the last two years there has been a surge of Large Language Models. This is understandable, since LLMs are amazing at generating text, and a lot of things can be viewed as text. However, generation is not always all we need - sometimes we want to have semantically rich representations. In general, LLMs are not the best tool to get semantically rich representations, and even today models like BERT are used to achieve state of the art text embeddings. A natural question is to ask why BERT is so much better at embeddings than LLMs? Here are the two main reasons:"><meta property="og:url" content="https://n1o.github.io/posts/from-llms-to-sentence-embeddings/"><meta property="og:site_name" content="Data Artificer and code:Breaker"><meta property="og:title" content="Transform any LLMs to a powerful Encoder"><meta property="og:description" content="Abstract Link to heading In the last two years there has been a surge of Large Language Models. This is understandable, since LLMs are amazing at generating text, and a lot of things can be viewed as text. However, generation is not always all we need - sometimes we want to have semantically rich representations. In general, LLMs are not the best tool to get semantically rich representations, and even today models like BERT are used to achieve state of the art text embeddings. A natural question is to ask why BERT is so much better at embeddings than LLMs? Here are the two main reasons:"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-11-21T10:44:33+01:00"><meta property="article:modified_time" content="2024-11-21T10:44:33+01:00"><link rel=canonical href=https://n1o.github.io/posts/from-llms-to-sentence-embeddings/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://n1o.github.io/>Data Artificer and code:Breaker
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Writing</a></li><li class=navigation-item><a class=navigation-link href=/awesome-t5/>Awesome T5</a></li><li class=navigation-item><a class=navigation-link href=/awesome-ssm/>Awesome SSM</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/from-llms-to-sentence-embeddings/>Transform any LLMs to a powerful Encoder</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2024-11-21T10:44:33+01:00>November 21, 2024
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
13-minute read</span></div></div></header><div class=post-content><h1 id=abstract>Abstract
<a class=heading-link href=#abstract><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>In the last two years there has been a surge of Large Language Models. This is understandable, since LLMs are amazing at generating text, and a lot of things can be viewed as text. However, generation is not always all we need - sometimes we want to have semantically rich representations. In general, LLMs are not the best tool to get semantically rich representations, and even today models like BERT are used to achieve state of the art text embeddings. A natural question is to ask why BERT is so much better at embeddings than LLMs? Here are the two main reasons:</p><ol><li><strong>Causal vs bidirectional attention</strong>, in causal attention the contextual representation of a token is determined by the current token and all (or a window with sliding window attention) previous tokens. With bidirectional attention, we also allow influence from future tokens. This makes intuitive sense - if we want to see importance, or meaning of a word in a sequence, it makes sense to take into account words that come before and after.</li><li><strong>Next token prediction vs Masked Token Prediction</strong>, decoder models are trained to predict the next token given the previous. This is obviously very efficient if we want to generate semantically valid text. Encoder models are trained on Masked Token prediction, which means we replace a random sample (usually 15%) of tokens with a special [MASK] token. Its goal is to recover the masked token, and it is done by leveraging information from the neighborhood of the masked token.</li></ol><p>It is not hard to see that causal models are meant to be good at predicting future tokens, while encoder models are designed to capture information about their surroundings. A natural question to ask is: <em>&ldquo;If Bidirectional models are so good why do we not stick to them, and train them to get better embedding models?&rdquo;</em>. Again there are a couple of reasons:</p><ol><li><strong>Sample efficiency of LLMs</strong>, let&rsquo;s start here with an example sentence: &ldquo;I think this article is really cool.&rdquo;. It does not matter if this is true or not, we can show how a causal LLM is trained:
$$ p(., &ldquo;I&rdquo;) $$
$$ p(.| &ldquo;think&rdquo;, &ldquo;I&rdquo;)$$
$$ p(.| &ldquo;this&rdquo;, &ldquo;think&rdquo;, &ldquo;I&rdquo;)$$
$$ p(.| &ldquo;article&rdquo;, &ldquo;this&rdquo;, &ldquo;think&rdquo;, &ldquo;I&rdquo;)$$
$$ p(.| &ldquo;is&rdquo;, &ldquo;article&rdquo;, &ldquo;this&rdquo; ,&ldquo;think&rdquo;, &ldquo;I&rdquo;)$$
$$ p(.| &ldquo;really&rdquo; ,&ldquo;is&rdquo;, &ldquo;article&rdquo;, &ldquo;this&rdquo; ,&ldquo;think&rdquo;, &ldquo;I&rdquo;)$$</li></ol><p>We can see that a single sentence produces multiple training examples, here $p(.|)$ says we want to observe the next word. (Sure LLMs operate on tokens not words but the intuition still applies).</p><p>For Masked Token Prediction the examples would be:</p><p>$$ p([MASK]| &ldquo;I&rdquo;, &ldquo;think&rdquo;, &ldquo;this&rdquo;, [MASK], &ldquo;is&rdquo;, &ldquo;really&rdquo;, &ldquo;cool&rdquo;) $$
$$ p([MASK]| &ldquo;I&rdquo;, &ldquo;think&rdquo;, &ldquo;this&rdquo;, &ldquo;article&rdquo;, &ldquo;is&rdquo;, &ldquo;really&rdquo;, [MASK]) $$</p><p>Here we want to predict [MASK], given the surrounding, and usually we use a sentence only a couple of times. Technically we could reuse the sentence as many times as we want, but it is usually not done.</p><ol start=2><li><strong>LLMs are more useful</strong>, here we may argue a bit, but maybe not. ChatGPT, Claude, or other Generative models, have made an impact on knowledge workers, and have potential to transform many aspects of work. On the other hand embedding models are useful but they are constrained to certain subjects like similarity detection, information retrieval and classification, all useful but the overall impact is somewhat less profound.</li></ol><p>Let&rsquo;s recap the text above: LLMs are more efficient to train and are more flexible ultimately yielding them more useful, but for some special cases we need an encoder model to get rich representations. A natural question is: <em>&ldquo;Can we train an LLM and somehow make it so that it can produce rich semantic representations?&rdquo;</em>. The answer is of course we can, and we will cover the current state of the art how to do it:</p><ol><li><a href=https://arxiv.org/abs/2404.05961 class=external-link target=_blank rel=noopener>LLM2Vec: Large Language Models are Secretly Powerful Text Encoders</a></li><li><a href=https://arxiv.org/abs/2405.17428 class=external-link target=_blank rel=noopener>NV-Embed</a></li><li><a href=https://arxiv.org/abs/2410.14578 class=external-link target=_blank rel=noopener>Large Language Models Are Overparameterized Text Encoders</a>, this is a bit different than the papers above and it focuses on reducing the runtime costs of newly created models.</li></ol><h2 id=remark>Remark
<a class=heading-link href=#remark><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>There is also a family of Encoder-Decoder models like <a href>T5</a>. These models are excellent at providing rich embeddings and they are great at generating text. So are they perfect? Yes they are, but (of course there is a but) they are way less efficient if you use them as a conversational agent. In a decoder only language model, you can cache the previous token representation, since they do not change if we keep adding tokens to the end. With encoder-decoder models, we need to always reevaluate the entire input before we pass to the decoder, even if we only add a single token at the end. Ok so this is not 100% true, we could append the extra tokens just to the decoder, but that will limit the expressivity of the model.</p><h2 id=sequence-embeddings>Sequence Embeddings
<a class=heading-link href=#sequence-embeddings><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Before we look into the main research papers, we have to clarify what a sequence embedding is. It is a function that takes an arbitrary long input and transforms it into a fixed length output.</p><p>$$ f: X \in R^{l \times d} \rightarrow Y \in R^d $$</p><ul><li>$l$ is the length of the original sequence</li><li>$d$ is the dimension of this sequence</li></ul><p>In human language, we take an arbitrary long text and we transform it to a fixed sized vector. A common way to get a sequence embedding is just to average individual entries out, however with tokens this is not very fortunate, since different tokens can contribute more to an overall representation. A better way is to take a weighted average, where we use a learnable weight function.</p><h1 id=llm2vec>LLM2Vec
<a class=heading-link href=#llm2vec><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>LLM2Vec is an unsupervised approach that can transform any decoder-only LLMs into a strong text encoder. This is done without any expensive synthetic data in a parameter efficient way, and it works in 3 steps.</p><h2 id=steps>Steps
<a class=heading-link href=#steps><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p><img src=/images/llm_2_vec_steps.png></p><ol><li>Enable bidirectional attention</li><li>Masked next token prediction</li><li>Unsupervised [[Contrastive Training|Contrastive Learning]]</li></ol><h3 id=enable-bidirectional-attention>Enable Bidirectional Attention
<a class=heading-link href=#enable-bidirectional-attention><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>At the beginning of this article we established why causal attention mask is not the best option if we need a rich embedding. Because of this, the first step in LLM2Vec is to replace the attention mask with a bidirectional (all ones) mask. If we use <a href=https://github.com/Dao-AILab/flash-attention class=external-link target=_blank rel=noopener>FlashAttention</a> the following will make the trick:</p><div class=highlight><pre tabindex=0 style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>flash_attn_func(q, k, v, causal=<span style=color:#fff;font-weight:700>False</span>, window_size=(-<span style=color:#ff0;font-weight:700>1</span>, -<span style=color:#ff0;font-weight:700>1</span>)):
</span></span></code></pre></div><p>Later in the benchmarks, we are going to see that changing from causal to bidirectional attention won&rsquo;t really help; actually it will worsen the model&rsquo;s performance. The reason is that the model does not really know what to do with future information.</p><h3 id=masked-next-token-prediction-mntp>Masked Next Token Prediction (MNTP)
<a class=heading-link href=#masked-next-token-prediction-mntp><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>As mentioned above, we need to teach the model to leverage information from future tokens. To do that we train it on <a href=https://www.semanticscholar.org/paper/An-Analysis-and-Mitigation-of-the-Reversal-Curse-Lv-Zhang/175ccc099f2a91005f53d752c09a74b8b91fdc38 class=external-link target=_blank rel=noopener>Masked Next Token Prediction</a> objective. MNTP is similar to Masked Language Modeling, with one key distinction: to predict the masked token at position $i$, we take the representation at the previous token $i-1$ and not the masked position itself.</p><h3 id=unsupervised-contrastive-learning>Unsupervised Contrastive Learning
<a class=heading-link href=#unsupervised-contrastive-learning><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>In the previous step we healed the model - it is no longer confused about what these new tokens are about, and we have a good word level model. But since LLMs were not trained to capture the semantics of a whole sentence, we fill this gap by applying Contrastive learning. The key idea behind contrastive learning is that we want to pull the semantic representations of similar samples/sentences closer together while pushing dissimilar ones farther apart. Here we follow the recipe introduced by <a href=https://arxiv.org/abs/2104.08821 class=external-link target=_blank rel=noopener>SimCE</a>, and we need positive and negative samples. <strong>Positive samples</strong> are generated from the sentence itself by applying a random dropout mask with dropout probability 0.3 and we generate exactly two samples. Dropout probability 0.3 is rather high; usually researchers leverage 0.1 when training bidirectional models with contrastive objective. <strong>Negative samples</strong> are simply other examples in a given mini-batch.</p><h2 id=training>Training
<a class=heading-link href=#training><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>We have our 3 steps, from this Masked Next Token Prediction and Unsupervised Contrastive learning introduce objectives that require training. As a dataset, English Wikipedia is chosen, mainly because the underlying model should be already trained on it. For both objectives we leverage <a href=https://huggingface.co/docs/peft/main/conceptual_guides/lora class=external-link target=_blank rel=noopener>LoRA</a>, which we merge back into the model. As for compute, it requires a single 80GB A100 GPU for around 5 hours, which makes LLM2Vec compelling for individuals and academic institutions.</p><h2 id=results>Results
<a class=heading-link href=#results><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The researchers applied LLM2Vec to Mistral-7B, S-LLama-1.3B and Llama-2-7B and here are the results:</p><h3 id=word-level>Word Level
<a class=heading-link href=#word-level><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p><img src=/images/llm_2_vec_word_level_tasks.png></p><p>We have two baselines, the first is deBERTa-v3-large (dashed lines, and it is a 418M parameter model, however 131M are the initial embeddings) and the embedding produced by a plain causal model (full line).</p><p>In general, swapping causal attention for bidirectional hurts the performance, but not in case of Mistral. Here it is theorized that it may have been trained in some of its pretraining phases with bidirectional attention.</p><p>If we include MNTP we observe increased performance since the model was taught to leverage the bidirectional information. Contrastive learning actually hurts the performance a bit; this makes sense since its contribution is to get a better sentence model and it is not needed for a word model.</p><h3 id=sentence-level>Sentence Level
<a class=heading-link href=#sentence-level><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Here the baseline is a BERT model that was trained in the SimCSE paper and we also explore various pooling methods over individual token representations.</p><p><img src=/images/llm_2_vec_sentence_level_tasks.png></p><p>The results follow similar trend as in Word Level tasks: bidirectional attention without finetuning hurts the performance, but not for Mistral. The best results are achieved by applying all 3 steps with weighted mean pooling.</p><h2 id=further-finetuning-for-mteb>Further Finetuning for MTEB
<a class=heading-link href=#further-finetuning-for-mteb><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>To further evaluate the performance the authors use the <a href=https://arxiv.org/abs/2210.07316 class=external-link target=_blank rel=noopener>MTEB: Massive Text Embedding Benchmark</a> here is the <a href=https://huggingface.co/spaces/mteb/leaderboard class=external-link target=_blank rel=noopener>Leaderboard</a> from Hugging Face where you can check the latest and greatest.</p><p><img src=/images/llm_2_vec_mteb.png></p><h1 id=nv-embed>NV-Embed
<a class=heading-link href=#nv-embed><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>If you checked the leaderboard (well, it depends when you are reading this post), you noticed that NV-embed was in first place. Because of this, we cannot really ignore it. NV-Embed shares a lot of similar design decisions with LLM2Vec - both swap causal for bidirectional attention, both finetune the resulting model. However, NV-Embed only does Contrastive learning. The biggest difference is how they pool individual token representations into a single embedding - NV-Embed introduced a novel technique called Latent Attention.</p><h2 id=latent-attention>Latent Attention
<a class=heading-link href=#latent-attention><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p><img src=/images/nv_embed_latent_attention.png></p><p>Latent Attention is just cross attention defined as:</p><p>$$O \in R^{l \times d} = \text{softmax}(QK^T)V$$</p><ul><li>$Q \in R^{l \times d}$ is the output from the last attention layer with $l$ the sequence length and $d$ the hidden dimension.</li><li>$K = V \in R^{r \times d}$ are trainable parameters &ldquo;dictionary&rdquo;, which is used to obtain better representations, $r$ is the number of latents in the dictionary.</li></ul><p>Once we have our output $O$ we transform it with two linear layers each with GeLU activation, followed by mean pooling to get the final embedding.</p><h3 id=intuition>Intuition
<a class=heading-link href=#intuition><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Let&rsquo;s put this into more human-like words: our latent K,V vectors are learned during training, and they are independent of the input during inference (this makes dictionary actually a great name). Since Q comes from the actual input sequence, we can view the whole latent attention as an average over the latent dictionary weighted by the input sentence. The two MLPs perform channel mixing (mixing along the latent dimensionality of individual entries in the output sequence). At the end, we can just do mean pooling since we already have a weighted mixture of individual tokens from the attention operation.</p><h2 id=training-1>Training
<a class=heading-link href=#training-1><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>We need to perform training since the model does not know what to do with the extra token information that it gets from the bidirectional attention, and we need to learn the latent dictionaries for the latent attention. The training is done in 2 stages:</p><ol><li><p>Contrastive Training
Again we follow the SimCE recipe but with the following modifications: we do not generate extra positive samples with dropout, there is just one. As for negatives, we have in-batch negatives and hard negatives. Hard negatives are negative samples chosen for a given sample.</p></li><li><p>Contrastive Instruction Tuning</p></li></ol><p>The instruction following capacity of LLMs makes them extra compelling. By introducing prompts to embedding models, we can steer the embedding more towards our needs. Since we want to use embedding not just for retrieval but also classification, clustering and other non-retrieval tasks, we include extra training data for this purpose.</p><p>Here is the complete list of possible prompts:</p><p><img src=/images/nv_embed_train_query.png></p><p>During instruction tuning we also do not use in-batch negatives, mainly because a single batch may contain actual samples from the same class and it will mislead the model.</p><h2 id=experiments>Experiments
<a class=heading-link href=#experiments><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The authors build their model on top of Mistral 7B, where the latent attention has 512 latents, and the resulting sequence embedding is 4096 long. As with LLM2Vec, the training was done using LoRA, for NV-Embed with rank 16, alpha 32 and Dropout 0.1.</p><h3 id=mteb-results>MTEB results
<a class=heading-link href=#mteb-results><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>And here are the official results for MTEB. Results are taken directly from the paper. Currently, there is a v2 model on top of the charts, which leverages <a href=https://arxiv.org/abs/2407.15831 class=external-link target=_blank rel=noopener>NV-Retriever</a> to mine the hard negatives.
<img src=/images/nv_embed_mteb_results.png></p><h1 id=large-language-models-are-overparameterized-text-encoders>Large Language Models are Overparameterized Text Encoders
<a class=heading-link href=#large-language-models-are-overparameterized-text-encoders><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Maybe you already noticed, if we derive an encoder model from an LLM, they tend to have a lot of parameters especially if we compare them to existing state of the art - they are even 20x larger. Their size makes them significantly slower and more expensive to run. A common approach to reduce model size is to start dropping layers.</p><p>However not all layers are equal, it is theorized that the lower layers in Large Language Models are responsible for extracting the necessary information from the underlying input and the higher levels are responsible for generating meaningful text, that is predicting the next token. Since we are more interested in having efficient embeddings, we can prune the top layers, but how much is too much?</p><p><img src=/images/l3_prune.png></p><p>The picture above has two local minima, and we can use that to create two models: one that is large but retains most of the LLM&rsquo;s performance and one that has a larger performance drop but is small and cheap to run.</p><h2 id=l3-prune>$L^3$ Prune
<a class=heading-link href=#l3-prune><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>$L^3$ prune is a method that automatically determines the optimal cutoff point. For more information, here is the forked <a href=https://github.com/n1o/l3prune class=external-link target=_blank rel=noopener>source code</a></p><p><img src=/images/l3_pruning_performance.png></p><p>We can clearly see that we can chop off the upper 10-20% of layers and see virtually no performance drop. We can push it further and remove around 70-80% of the top layers and see some minor drop while still maintaining excellent performance.</p><h1 id=final-thoughts>Final Thoughts
<a class=heading-link href=#final-thoughts><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Since my main interests lie more in understanding tasks than in generation problems, this research was very needed. For a very long time, I thought that I would need to pretrain my own embedding model; however, as we all know, that is expensive and generally discouraged. Right now, I see that I can create a powerful embedding model, in my example from <a href=https://github.com/QwenLM/Qwen2.5-Coder class=external-link target=_blank rel=noopener>Qwen-2.5 Coder</a>. What would be extremely nice to see is if I could take the ideas from NV-Embed (mainly because it is the latest research already leveraging ideas from LLM2Vec, however I still see the need to have a word-level pretraining step) and fuse it with <a href=https://arxiv.org/pdf/2205.13147 class=external-link target=_blank rel=noopener>Matryoshka Representation Learning</a> to get a leaner final representation.</p></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2020 -
2024
n1o_c0rTx
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ")}</script></body></html>