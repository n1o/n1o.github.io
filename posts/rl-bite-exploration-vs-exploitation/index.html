<!doctype html><html lang=en><head><title>RL Bite: Exploitation vs Exploration · Data Artificer and code:Breaker
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="n1o_c0rTx"><meta name=description content="Explore vs Exploit Link to heading In reinforcement learning, we have an agent that has to take actions, for which it receives a reward. Here we have a dilemma: do we choose an action that gives the biggest reward or do we explore new actions that may lead to regions with even higher payouts?
Greedy Policy Link to heading This is a simple one, we always take the option that gives us the highest payout:"><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="RL Bite: Exploitation vs Exploration"><meta name=twitter:description content="Explore vs Exploit Link to heading In reinforcement learning, we have an agent that has to take actions, for which it receives a reward. Here we have a dilemma: do we choose an action that gives the biggest reward or do we explore new actions that may lead to regions with even higher payouts?
Greedy Policy Link to heading This is a simple one, we always take the option that gives us the highest payout:"><meta property="og:url" content="https://n1o.github.io/posts/rl-bite-exploration-vs-exploitation/"><meta property="og:site_name" content="Data Artificer and code:Breaker"><meta property="og:title" content="RL Bite: Exploitation vs Exploration"><meta property="og:description" content="Explore vs Exploit Link to heading In reinforcement learning, we have an agent that has to take actions, for which it receives a reward. Here we have a dilemma: do we choose an action that gives the biggest reward or do we explore new actions that may lead to regions with even higher payouts?
Greedy Policy Link to heading This is a simple one, we always take the option that gives us the highest payout:"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-05T09:51:24+01:00"><meta property="article:modified_time" content="2025-02-05T09:51:24+01:00"><meta property="article:tag" content="RL Bite"><link rel=canonical href=https://n1o.github.io/posts/rl-bite-exploration-vs-exploitation/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://n1o.github.io/>Data Artificer and code:Breaker
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Writing</a></li><li class=navigation-item><a class=navigation-link href=/awesome-t5/>Awesome T5</a></li><li class=navigation-item><a class=navigation-link href=/awesome-ssm/>Awesome SSM</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/rl-bite-exploration-vs-exploitation/>RL Bite: Exploitation vs Exploration</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2025-02-05T09:51:24+01:00>February 5, 2025
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
3-minute read</span></div><div class=categories><i class="fa-solid fa-folder" aria-hidden=true></i>
<a href=/categories/rl-bite/>RL Bite</a></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/rl-bite/>RL Bite</a></span></div></div></header><div class=post-content><h1 id=explore-vs-exploit>Explore vs Exploit
<a class=heading-link href=#explore-vs-exploit><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>In reinforcement learning, we have an agent that has to take actions, for which it receives a reward. Here we have a dilemma: do we choose an action that gives the biggest reward or do we explore new actions that may lead to regions with even higher payouts?</p><h1 id=greedy-policy>Greedy Policy
<a class=heading-link href=#greedy-policy><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>This is a simple one, we always take the option that gives us the highest payout:</p><p>$$ a_t = \arg \max_{a} Q(s,a) $$</p><p>The obvious downside is that we do not do any exploration. To fix this there are two possible extensions:</p><ol><li>$\epsilon$ Greedy policy $\pi_{\epsilon}$, this takes the greedy action as above with probability $\epsilon \in (0,1)$ otherwise we choose a random action. This is still not perfect since we equally explore all actions even if they are bad, however this can be solved by annealing $\epsilon \rightarrow 0$ with time.</li><li>$\epsilon z$ Greedy policy. This is an extension of $\epsilon$ greedy where instead of making a single sample in the random direction we take multiple steps, the number of steps is determined by $n \sim z$ and it should help us to escape potential random minima we may get stuck in.</li></ol><h1 id=boltzman-exploration>Boltzman Exploration
<a class=heading-link href=#boltzman-exploration><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Greedy policies above have the downside that they explore actions with equal probability. Boltzman exploration tries to fix it by exploring more promising actions with higher probability.</p><p>$$ \sum_{a&rsquo;}\exp(\hat{R}_t(s_t, a&rsquo;)/\tau)$$</p><p>$$\pi_{\tau}(a|s) = \frac{\exp(\hat{R}<em>t(s_t, a) / \tau )}{\sum</em>{a&rsquo;} }$$</p><ul><li>$\tau > 0$ is a temperature parameter, as it gets closer to 0 we get a greedy distribution and with higher temperatures we get a more uniform distribution.</li></ul><p>This is still not perfect, we explore different actions equally widely, which means that actions that we tried only a few times have some uncertainty in their actual payout. Because of this, we introduce an <strong>exploration bonus</strong> $R_t^b(s,a)$ which is higher for states we visited only a few times and decreases with each visit.</p><h1 id=optimal-explore-exploit-strategy>Optimal Explore-Exploit Strategy
<a class=heading-link href=#optimal-explore-exploit-strategy><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Is there an Optimal Explore-Exploit strategy? Well yes and no. Yes for some special types of models like Context-free Bandits which we can compute with dynamic programming, however for the general case this problem is not tractable.</p><h1 id=upper-confidence-bound-and-thompson-sampling>Upper Confidence Bound and Thompson Sampling
<a class=heading-link href=#upper-confidence-bound-and-thompson-sampling><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>As mentioned above, optimal solution for explore-exploit tradeoff is not tractable, because of this we can introduce an optimistic estimate of the reward function: $\hat{R}$ which we call the <strong>upper confidence bound</strong>:</p><p>$$ \hat{R}_t(s_t,a) \ge R(s_t,a) $$</p><p>This function with high probability overestimates the payout for every action, then we greedily choose actions from this distribution:</p><p>$$ a_t = \arg \max_{a} \hat{R}_t(s,a) $$</p><p>This by itself would not do much, but we can imagine it as a uniform distribution where it overestimates everything by introducing an <strong>exploration bonus</strong> as in Boltzman Exploration (where with high temperature we get the same uniform distribution) for every action.</p><p>With time as we explore we continually decrease our optimism:</p><p>$$ \hat{R}_t - R_t $$</p><h2 id=thompson-sampling>Thompson sampling
<a class=heading-link href=#thompson-sampling><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>It would be a sin to not mention Thompson sampling. It is a Bayesian approach, where we choose an action based on some probability, actions that have high payout are more likely to be chosen. Actions that we have not explored or explored only a little have high variance, thus making them still likely to be explored. Each time we take an action and get a reward we update our belief. We can also view it as assigning an extra probability to each state being optimal and we update this distribution as we sample.</p></div><footer><section class=see-also></section><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2020 -
2025
n1o_c0rTx
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ")}</script></body></html>