<!doctype html><html lang=en><head><title>RL Bite: Policy Gradient and Reinforce · Data Artificer and code:Breaker
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="n1o_c0rTx"><meta name=description content="Abstract Link to heading Till now we have considered only learning the Value or Q function and estimating the policy from those. In the next few posts, we are going to look into directly learning the policy. Why directly learn the policy? First, Q learning has a lot of issues involving the Deadly Triad; second, if we have continuous actions we cannot really use it; and lastly, Q learning always learns a deterministic policy, and in cases of partially observed stochastic environments (which is nearly always what we have), having a stochastic policy is proven to be better."><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="RL Bite: Policy Gradient and Reinforce"><meta name=twitter:description content="Abstract Link to heading Till now we have considered only learning the Value or Q function and estimating the policy from those. In the next few posts, we are going to look into directly learning the policy. Why directly learn the policy? First, Q learning has a lot of issues involving the Deadly Triad; second, if we have continuous actions we cannot really use it; and lastly, Q learning always learns a deterministic policy, and in cases of partially observed stochastic environments (which is nearly always what we have), having a stochastic policy is proven to be better."><meta property="og:url" content="https://n1o.github.io/posts/rl-bite-policy-gradient-and-reinforce/"><meta property="og:site_name" content="Data Artificer and code:Breaker"><meta property="og:title" content="RL Bite: Policy Gradient and Reinforce"><meta property="og:description" content="Abstract Link to heading Till now we have considered only learning the Value or Q function and estimating the policy from those. In the next few posts, we are going to look into directly learning the policy. Why directly learn the policy? First, Q learning has a lot of issues involving the Deadly Triad; second, if we have continuous actions we cannot really use it; and lastly, Q learning always learns a deterministic policy, and in cases of partially observed stochastic environments (which is nearly always what we have), having a stochastic policy is proven to be better."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-08T13:24:13+01:00"><meta property="article:modified_time" content="2025-03-08T13:24:13+01:00"><meta property="article:tag" content="RL Bite"><meta property="article:tag" content="Policy Learning"><meta property="article:tag" content="Temporal Difference"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-learning-the-q-function/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-computing-value-functions/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-bellmans-equations-and-value-functions/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-exploration-vs-exploitation/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-learning-the-q-function/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-computing-value-functions/"><link rel=canonical href=https://n1o.github.io/posts/rl-bite-policy-gradient-and-reinforce/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://n1o.github.io/>Data Artificer and code:Breaker
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Writing</a></li><li class=navigation-item><a class=navigation-link href=/awesome-t5/>Awesome T5</a></li><li class=navigation-item><a class=navigation-link href=/awesome-ssm/>Awesome SSM</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/rl-bite-policy-gradient-and-reinforce/>RL Bite: Policy Gradient and Reinforce</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2025-03-08T13:24:13+01:00>March 8, 2025
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
4-minute read</span></div><div class=categories><i class="fa-solid fa-folder" aria-hidden=true></i>
<a href=/categories/rl-bite/>RL Bite</a>
<span class=separator>•</span>
<a href=/categories/policy-learning/>Policy Learning</a>
<span class=separator>•</span>
<a href=/categories/temporal-difference/>Temporal Difference</a></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/rl-bite/>RL Bite</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/policy-learning/>Policy Learning</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/temporal-difference/>Temporal Difference</a></span></div></div></header><div class=post-content><h1 id=abstract>Abstract
<a class=heading-link href=#abstract><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Till now we have considered only learning the Value or Q function and estimating the policy from those. In the next few posts, we are going to look into directly learning the policy. Why directly learn the policy? First, Q learning has a lot of issues involving the Deadly Triad; second, if we have continuous actions we cannot really use it; and lastly, Q learning always learns a deterministic policy, and in cases of partially observed stochastic environments (which is nearly always what we have), having a stochastic policy is proven to be better.</p><h1 id=policy-gradient>Policy Gradient
<a class=heading-link href=#policy-gradient><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>I teased a bit, yes we are going to use (stochastic) Gradient Descent to optimize (this is also known as <strong>policy search</strong>) the following loss:</p><p>$$ J(\pi) \triangleq E_{\pi} [ \sum_{t=0}^{\infty} \gamma^t R_{t+1} ]$$
$$ = \sum_{t=0}^{\infty} \gamma^t \sum_s \left( \sum_{s_0} p_0(s_0) p^{\pi}(s_0 \rightarrow s, t) \right) \sum_a \pi(a|s) R(s, a)$$
$$ = \sum_s \left( \sum_{s_0} \sum_{t=0}^{\infty} \gamma^t p_0(s_0) p^{\pi}(s_0 \rightarrow s, t) \right) \sum_a \pi(a|s) R(s, a)$$
$$ = \sum_s \rho^{\gamma}_{\pi}(s) \sum_a \pi(a|s) R(s, a)$$</p><ul><li>$\rho_{\pi}^{\gamma}(s) \triangleq \sum_{t=0}^{\infty} \gamma^t \sum_{s_0} p_0(s_0) p^{\pi}(s_0 \rightarrow s, t)$ this measures the time spent in non-terminal states<ul><li>it is <strong>NOT a probability measure</strong>, since it is not normalised but can be normalized by exploiting $\sum_{i=0}^{\infty} \gamma^t = \frac{1}{1 - \gamma}$ if $\gamma &lt; 1$ this yields<ul><li>$p_{\pi}^{\gamma}(s) = (1 - \gamma) \rho_{\pi}^{\gamma}(s) = (1 - \gamma) \sum_{t=0}^{\infty} \gamma^t p_t(s)$</li></ul></li></ul></li><li>$p_t^{\pi}(s) = \sum_{s_0} p_0(s_0) p^{\pi}(s_0 \rightarrow s, t)$ this is the marginal probability of being in state $s$ at time $t$</li><li>$p^{\pi}(s_0 \rightarrow s, t)$ is the probability of going from $s_0$ to $s$ in $t$ steps</li></ul><p>We abuse the notation a bit by treating $\rho$ as a probability measure we get:
$$ E_{\rho_{\pi}^{\gamma}(s)}[f(s)] = \sum_{s} \rho_{\pi}^{\gamma}(s) f(s)$$</p><p>And our final loss is:
$$ J(\pi) = E_{\rho_{\pi}^{\gamma}(s), \pi(a|s)}[R(s, a)]$$</p><h2 id=theorem>Theorem
<a class=heading-link href=#theorem><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Now we differentiate the loss to get:</p><p>$$ \nabla_{\theta} J(\theta) = \sum_{s} \rho_{\pi}^{\gamma}(s) \sum_{a} Q^{\pi}(s, a) \nabla_{\theta} \pi_{\theta}(a|s)$$
$$ = \sum_{s} \rho_{\pi}^{\gamma}(s) \sum_{a} Q^{\pi_{\theta}}(s, a) \pi_{\theta}(a|s) \nabla_{\theta} \log \pi_{\theta}(a|s)$$
$$ = E_{\rho_{\pi}^{\gamma}(s) \pi_{\theta}(a|s)}[Q^{\pi_{\theta}}(s, a) \nabla_{\theta} \log \pi_{\theta}(a|s)]$$</p><ul><li>$\nabla_{\theta} \log \pi_{\theta}(a|s)]$ this is also known as the <strong>score function</strong> and it is totally unrelated to the score function in Denoising Diffusion, which is the gradient with respect to a log probability: $ \nabla_{\theta} \log \pi_{\theta}(a|s) $</li></ul><p>Since it is a gradient, we can follow it to regions with higher reward! Now you can also see that the equation contains the Q function! Even in policy-based methods, we frequently use the Value, Q, or Advantage function (difference between the Value and Q function) since they stabilize the loss.</p><h1 id=reinforce>Reinforce
<a class=heading-link href=#reinforce><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>So naive policy gradient is said to have high variance! This is because we do a Monte Carlo rollout of the policy, which has low bias but, as said, high variance. To reduce the variance we need to introduce a baseline function, let&rsquo;s look at the equations:</p><p>$$ \nabla_{\theta} J(\pi_{\theta}) = \sum_{t=0}^{\infty} \gamma^t E_{p_t(s) \pi_{\theta}(a_t|s_t)} [\nabla_{\theta} \log \pi_{\theta}(a_t|s_t) Q_{\pi_{\theta}}(s_t, a_t)]$$</p><p>$$ \approx \sum_{t=0}^{T-1} \gamma^t G_t \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)$$</p><ul><li>$G_t \triangleq r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots + \gamma^{T-t-1} r_{T-1} = \sum_{k=0}^{T-t-1} \gamma^k r_{t+k} = \sum_{j=t}^{T-1} \gamma^{j-t} r_j$ this is the reward-to-go and is estimated using Markov Chain rollout of the policy, hence the high variance part</li></ul><p>As mentioned before, we introduce a baseline function $b(s)$:</p><p>$$\nabla_{\theta} J(\pi_{\theta}) = E_{\rho_{\theta}(s) \pi_{\theta}(a|s)} [\nabla_{\theta} \log \pi_{\theta}(a|s) (Q_{\pi_{\theta}}(s, a) - b(s))] $$</p><h2 id=baseline-functions>Baseline Functions
<a class=heading-link href=#baseline-functions><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>It can be anything, it has just one requirement:</p><p>$$ E[\nabla_{\theta}b(s)] = 0$$</p><p>A common choice is either the Value Function $b(s) = V_{\pi_{\theta}}(s)$ or Advantage Function $b(s) = Q_{\pi_{\theta}}(a, s)$</p><h2 id=estimator>Estimator
<a class=heading-link href=#estimator><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Thus our update for the parameters $\theta$ becomes:</p><p>$$\theta \leftarrow \theta + \eta \sum_{t=0}^{T-1} \gamma^t (G_t - b(s_t)) \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)$$</p><p>The update has an intuitive explanation:</p><p><em>We compute the sum of discounted future reward induced by a trajectory, compared to a baseline and if it is positive we increase $\theta$ to make this trajectory more likely otherwise we decrease $\theta$, or in simpler terms we reinforce good behavior and penalize bad.</em></p><h2 id=algorithm>Algorithm
<a class=heading-link href=#algorithm><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p><img src=/images/reinforce_algo.png></p><h1 id=final-remarks>Final Remarks
<a class=heading-link href=#final-remarks><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>This is the basis of Policy-Based Reinforcement Learning. In the next posts, we will look into an alternative approach called Actor-Critic methods, where instead of MC policy rollout we use Temporal Difference to estimate $G_t$. However, as it turns out, neither Reinforce nor Actor-Critic methods guarantee monotonic improvement in the learned policy, and we turn to Policy Improvement methods which contain the currently popular Proximal Policy Optimization (PPO) method known from Large Language Models!</p></div><footer><section class=see-also></section><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2020 -
2025
n1o_c0rTx
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ")}</script></body></html>