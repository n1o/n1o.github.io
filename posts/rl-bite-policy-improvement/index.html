<!doctype html><html lang=en><head><title>RL Bite: Monotonic Policy Improvement and Deriving Proximal Policy Optimization (PPO) · Data Artificer and code:Breaker
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="n1o_c0rTx"><meta name=description content="Abstract Link to heading A while ago we looked into Policy Gradient and Reinforce. Policy gradient is versatile and under mild conditions it is guaranteed to converge to a local minimum (if we choose the correct policy and step size). This is already a huge step up when compared to Q Learning, which may just diverge. However, we may still want stronger guarantees like monotonic improvement at each step."><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="RL Bite: Monotonic Policy Improvement and Deriving Proximal Policy Optimization (PPO)"><meta name=twitter:description content="Abstract Link to heading A while ago we looked into Policy Gradient and Reinforce. Policy gradient is versatile and under mild conditions it is guaranteed to converge to a local minimum (if we choose the correct policy and step size). This is already a huge step up when compared to Q Learning, which may just diverge. However, we may still want stronger guarantees like monotonic improvement at each step."><meta property="og:url" content="https://n1o.github.io/posts/rl-bite-policy-improvement/"><meta property="og:site_name" content="Data Artificer and code:Breaker"><meta property="og:title" content="RL Bite: Monotonic Policy Improvement and Deriving Proximal Policy Optimization (PPO)"><meta property="og:description" content="Abstract Link to heading A while ago we looked into Policy Gradient and Reinforce. Policy gradient is versatile and under mild conditions it is guaranteed to converge to a local minimum (if we choose the correct policy and step size). This is already a huge step up when compared to Q Learning, which may just diverge. However, we may still want stronger guarantees like monotonic improvement at each step."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-01T08:44:11+02:00"><meta property="article:modified_time" content="2025-04-01T08:44:11+02:00"><meta property="article:tag" content="RL Bite"><meta property="article:tag" content="Policy Learning"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-policy-gradient-and-reinforce/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-learning-the-q-function/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-computing-value-functions/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-bellmans-equations-and-value-functions/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-exploration-vs-exploitation/"><meta property="og:see_also" content="https://n1o.github.io/posts/rl-bite-policy-gradient-and-reinforce/"><link rel=canonical href=https://n1o.github.io/posts/rl-bite-policy-improvement/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://n1o.github.io/>Data Artificer and code:Breaker
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Writing</a></li><li class=navigation-item><a class=navigation-link href=/awesome-t5/>Awesome T5</a></li><li class=navigation-item><a class=navigation-link href=/awesome-ssm/>Awesome SSM</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/rl-bite-policy-improvement/>RL Bite: Monotonic Policy Improvement and Deriving Proximal Policy Optimization (PPO)</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2025-04-01T08:44:11+02:00>April 1, 2025
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
5-minute read</span></div><div class=categories><i class="fa-solid fa-folder" aria-hidden=true></i>
<a href=/categories/rl-bite/>RL Bite</a>
<span class=separator>•</span>
<a href=/categories/policy-learning/>Policy Learning</a></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/rl-bite/>RL Bite</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/policy-learning/>Policy Learning</a></span></div></div></header><div class=post-content><h1 id=abstract>Abstract
<a class=heading-link href=#abstract><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>A while ago we looked into <a href=/posts/rl-bite-policy-gradient-and-reinforce/>Policy Gradient and Reinforce</a>. Policy gradient is versatile and under mild conditions it is guaranteed to converge to a local minimum (if we choose the correct policy and step size). This is already a huge step up when compared to Q Learning, which may just diverge. However, we may still want stronger guarantees like monotonic improvement at each step.</p><h1 id=the-policy-improvement-lower-bound>The Policy Improvement Lower Bound
<a class=heading-link href=#the-policy-improvement-lower-bound><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>This will be a bit math heavy, but the idea is to figure out the lower bound between the new policy $\pi$ and current policy $\pi_k$. Once we have figured it out we can optimize this lower bound using Total Variational Distance (later we use KL divergence), giving us monotonic policy improvement guarantees at each step.</p><ol><li>Let&rsquo;s derive the Lower Bound:
$$ J(\pi) - J(\pi_k) \geq \frac{1}{1 - \gamma} E_{p_{\pi_k}^{\gamma}(s)\pi_k(a|s)} \left[ \frac{\pi(a|s)}{\pi_k(a|s)} A^{\pi_k}(s, a) \right] - \frac{2 \gamma C^{\pi, \pi_k}}{(1 - \gamma)^2} E_{p_{\pi_k}^{\gamma}(s)}[TV(\pi(\cdot|s), \pi_k(\cdot|s))]$$</li></ol><ul><li>$L(\pi, \pi_k)=E_{p_{\pi_k}^{\gamma}(s)\pi_k(a|s)} \left[ \frac{\pi(a|s)}{\pi_k(a|s)} A^{\pi_k}(s, a) \right]$ this is the surrogate objective</li><li>$\frac{2 \gamma C^{\pi, \pi_k}}{(1 - \gamma)^2} E_{p_{\pi_k}^{\gamma}(s)}[TV(\pi(\cdot|s), \pi_k(\cdot|s))]$ this is a penalty term</li><li>$A^{\pi_k} = Q^{\pi_k}(s,a) - V^{\pi_k}(s)$ is the advantage function</li><li>$\pi^{\gamma}_{\pi_k}$ is the normalized discounted state visitation distribution for $\pi_k$</li><li>$C^{\pi, \pi_k} = \max_s |E_{\pi(a|s)}[A^{\pi_k}(s, a)]|$</li></ul><ol start=2><li>Total Variation Distance</li></ol><p>$$TV(p, q) = \frac{1}{2} ||p - q|| = \frac{1}{2} \sum_{s} |p(s) - q(s)|$$</p><ul><li>$||p-q||$ is the $l_1$ norm</li></ul><ol start=3><li>TV is hard to optimize, because of that we do a slight modification by introducing a trust-region update</li></ol><p>$$ \pi_{k+1} = \underset{\pi}{\text{argmax}} \ L(\pi, \pi_k) \ \text{s.t.} \ E_{p_{\pi_k}^{\gamma}(s)}[TV(\pi, \pi_k)(s)] \leq \epsilon$$</p><p>This is just constrained optimization, where we constrain the worst case performance decline at each update step.</p><h2 id=trust-region>Trust Region?
<a class=heading-link href=#trust-region><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>So what the hell is trust region update? Long story short, take gradient descent or any local descent method (called local since we do an approximation at a local point), in general we need to choose a step size. There are some methods where this step size is not fixed but may increase and decrease. In trust region methods, the step size is adjusted based on the quality of the local approximation. The higher the quality of the local approximation (more similar to the function we approximate) the longer steps we can take and vice versa. Because of this we can define the trust-region as a region where the approximation is very high, and we can make a step right at the border of this region.</p><h1 id=trust-region-policy-optimization-trpo>Trust Region Policy Optimization (TRPO)
<a class=heading-link href=#trust-region-policy-optimization-trpo><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>In practice we use KL Divergence instead of TV!</p><ol><li>We redefine the objective</li></ol><p>$$ \pi_{k+1} = \underset{\pi}{\text{argmax}} \ L(\pi, \pi_k) \ \text{s.t.} \E_{p_{\pi_k}^{\gamma}(s)} [D_{KL}(\pi_k || \pi)(s)] \leq \delta $$</p><ul><li>there is an equivalence between TV and KL divergence if $\delta = \frac{\epsilon^2}{2}$</li></ul><ol start=2><li>To get the loss we perform a first-order expansion of the surrogate objective:</li></ol><p>$$ L(\pi, \pi_k) = E_{p_{\pi_k}^{\gamma}(s) \pi_k(a|s)} \left[ \frac{\pi(a|s)}{\pi_k(a|s)} A^{\pi_k}(s, a) \right] \approx g_k^T (\theta - \theta_k)$$</p><ul><li>here $g_k = \nabla_{\theta}L(\pi_{\theta}, \pi_{k})|_{\theta_k}$</li></ul><ol start=3><li>Approximate the KL term:</li></ol><p>$$ E_{p_{\pi_k}^{\gamma}(s)}[D_{KL}(\pi_k || \pi)(s)] \approx \frac{1}{2} (\theta - \theta_k)^T F_k (\theta - \theta_k)$$</p><ul><li>$F_k = g_kg_k^T$ is the Fisher Information Matrix</li></ul><ol start=4><li>Now we can express the update rule as:</li></ol><p>$$ \theta_{k+1} = \theta_k + \mu_ v_k$$</p><ul><li>with $v_k = F_k^{-1}g_k$ is the Natural gradient Descent<ul><li>in practice we compute $v_k$ by approximately solving the linear system $F_k v = g_k$ using Conjugate Gradient</li></ul></li><li>$\mu_v = \sqrt{\frac{2\delta}{v_k^T F_k v_k}}$</li></ul><h2 id=fisher-information-matrix>Fisher Information Matrix
<a class=heading-link href=#fisher-information-matrix><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>In Machine Learning, we do a lot of Maximum Likelihood Estimation (MLE) $p(x|\theta)$, thus we maximize the likelihood of the data under certain parameters. This is extremely different to a Bayesian approach where we would maximize $p(\theta|x)$ or the parameters given the data we have. We can use the Fisher Information Matrix to measure how stable our MLE estimate is. In our case we can say that we use it to measure how stable our policy estimation/learning is.</p><h2 id=natural-gradient-descent>Natural Gradient Descent
<a class=heading-link href=#natural-gradient-descent><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Natural Gradient Descent is a slight extension of Gradient Descent where we use the Fisher Information Matrix for preconditioning the Gradient.</p><p>$$-\mu \F^{-1}g$$</p><p>So what the hell does this mean? If you look at the equation, and you&rsquo;re kind of familiar with Newton&rsquo;s Method, we swap the inverse Hessian with the inverse Fisher Information Matrix. For those that are not familiar with Newton&rsquo;s Methods: The approach is a second order optimization method. Second order because we do not only use linear approximation given by the gradient, but include curvature from the second derivative in our approximation. Thus in Natural Gradient Descent we replace the second order derivative information with the Fisher Information Matrix.</p><h2 id=conjugate-gradient-descent>Conjugate Gradient Descent
<a class=heading-link href=#conjugate-gradient-descent><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The intuition is as follows: First imagine that we optimize an n dimensional quadratic function, this can be done in n steps, if we use second order optimization. Why n steps? Well first by using second order information, we estimate the curvature, but since our function is quadratic, this approximation is perfect. Because of this we can individually optimize each dimension and arrive at a global minimum (a quadratic function is convex we have no local minima).</p><h1 id=proximal-policy-optimization-ppo>Proximal Policy Optimization (PPO)
<a class=heading-link href=#proximal-policy-optimization-ppo><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>PPO by itself is very anticlimactic. Why? Because it is again just a simplification, in this case of TRPO (which itself is a simplification)</p><ol><li>Constraint</li></ol><p>$$ E_{p_{\pi_k}^{\gamma}(s)}[TV(\pi, \pi_k)(s)] = \frac{1}{2} E_{(s, a) \sim p^{\gamma}_{\pi_k}} \left[ \left| \frac{\pi(a|s)}{\pi_k(a|s)} - 1 \right| \right]$$</p><p>Here we require that the support of $\pi$ is contained in the support of $\pi_k$ at every state.</p><ol start=2><li>Update Rule</li></ol><p>$$\pi_{k+1} = \underset{\pi}{\text{argmax}} \ E_{(s, a) \sim p^{\gamma}_{\pi_k}} [\min(\rho_k(s, a) A^{\pi_k}(s, a), \tilde{\rho}_k(s, a) A^{\pi_k}(s, a))]$$</p><ul><li>$\rho_k (s,a) = \frac{\pi(a|s)}{\pi_k(a|s)}$ is the likelihood ratio</li><li>$\tilde{\rho_k}(s,a) = clip(\rho_k(s,a), 1 - \epsilon, 1 + \epsilon)$, where clip(x,l,u) = min(max(x, l),u)</li></ul><p>That&rsquo;s it! No magic, just an simplification, of a simplification of an lower-bound to constrain monotonic policy improvement.</p></div><footer><section class=see-also></section><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2020 -
2025
n1o_c0rTx
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ")}</script></body></html>