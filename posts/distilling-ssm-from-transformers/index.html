<!doctype html><html lang=en><head><title>Distilling State Space Models from Transformers · Data Artificer and code:Breaker
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="n1o_c0rTx"><meta name=description content="Abstract Link to heading It is notoriously expensive to train a Language Model from scratch, making independent research impossible and trying out new architectures extremely risky. Because of these costs, Transformer++ models like LLaMa, based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention, are the de facto standard, not because they are the best, but because they are proven to work!"><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="Distilling State Space Models from Transformers"><meta name=twitter:description content="Abstract Link to heading It is notoriously expensive to train a Language Model from scratch, making independent research impossible and trying out new architectures extremely risky. Because of these costs, Transformer++ models like LLaMa, based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention, are the de facto standard, not because they are the best, but because they are proven to work!"><meta property="og:url" content="https://n1o.github.io/posts/distilling-ssm-from-transformers/"><meta property="og:site_name" content="Data Artificer and code:Breaker"><meta property="og:title" content="Distilling State Space Models from Transformers"><meta property="og:description" content="Abstract Link to heading It is notoriously expensive to train a Language Model from scratch, making independent research impossible and trying out new architectures extremely risky. Because of these costs, Transformer++ models like LLaMa, based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention, are the de facto standard, not because they are the best, but because they are proven to work!"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-28T10:12:27+01:00"><meta property="article:modified_time" content="2024-10-28T10:12:27+01:00"><meta property="article:tag" content="NLP"><meta property="article:tag" content="SSM"><meta property="article:tag" content="Transformers"><link rel=canonical href=https://n1o.github.io/posts/distilling-ssm-from-transformers/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://n1o.github.io/>Data Artificer and code:Breaker
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Writing</a></li><li class=navigation-item><a class=navigation-link href=/awesome-t5/>Awesome T5</a></li><li class=navigation-item><a class=navigation-link href=/awesome-ssm/>Awesome SSM</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/distilling-ssm-from-transformers/>Distilling State Space Models from Transformers</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2024-10-28T10:12:27+01:00>October 28, 2024
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
12-minute read</span></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/nlp/>NLP</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/ssm/>SSM</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/transformers/>Transformers</a></span></div></div></header><div class=post-content><h1 id=abstract>Abstract
<a class=heading-link href=#abstract><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>It is notoriously expensive to train a Language Model from scratch, making independent research impossible and trying out new architectures extremely risky. Because of these costs, Transformer++ models like LLaMa, based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention, are the de facto standard, not because they are the best, but because they are proven to work! In previous posts I covered <a href=/posts/ssm-transformer-hybrids-guide/>SSM-Transformer Hybrids</a>. The biggest benefit of hybridization is their reduced inference cost and minimal memory overhead due to reduced KV cache, and combining SSMs can Attention can make the model more expressive. The biggest obstacle of these models is that they need to be pretrained from scratch. For example <a href=https://www.zyphra.com/post/zamba2-7b class=external-link target=_blank rel=noopener>Zamba2 7B</a>, is a 7B model that was trained on 128 H100 GPUs for 50 days, bringing its pretraining costs to around 600K US dollars. This makes it one of the cheapest (but by far not weakest) SSM-Attention hybrids, however if we look into detail the model was trained only on 3T (+100B high quality for annealing) tokens. In comparison Llama3 was trained on 15T tokens, if we would apply the same number of tokens to Zamba we would end up with costs around 3M US dollars. It is not hard to see that these kinds of budgets are out of scope for any individual, but also out of scope for many medium sized research organizations and academia.</p><p>In this post we look into the details of two distillation techniques that take an already pretrained Transformer model and replace some of its parts with a Mamba-like State Space model. The biggest benefit of this approach is that we can drastically decrease the inference costs while avoiding the whole pretraining procedure on trillions of tokens, and still maintain the same performance level by finetuning on a few billion tokens.</p><ul><li><a href=https://arxiv.org/abs/2408.10189 class=external-link target=_blank rel=noopener>(Mohawk) Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models</a></li><li><a href=https://arxiv.org/abs/2408.15237 class=external-link target=_blank rel=noopener>Mamba in the Llama</a></li></ul><h1 id=sequence-and-channel-mixers>Sequence and Channel mixers!
<a class=heading-link href=#sequence-and-channel-mixers><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Let&rsquo;s revisit some key concepts of Transformers and Self attention. So what is self-attention? Remember, for a causal model, every output token is just a weighted sum of the input tokens. Yes you can have more heads, but that will just give you multiple weighted sums, preferably somewhat different (each head should capture different variation in the data). If you read, which I hope you did, my post about <a href=/posts/butterflies-monarchs-hyenas-and-lightning-fast-bert/>Monarch Matrices</a>, there I did a breakdown of an Attention Block into two parts: first part is the Sequence Mixer, and the second the Channel Mixer. Self-Attention is a Sequence Mixer - it may not be the most efficient in terms of computation, being quadratic in computation and needing a KV cache so we do not recompute the token representations over and over each time we sample a new token. The MLP parts of an Attention block serve as Channel Mixers. MLP also plays another crucial role, since it is theorized that it holds most of the LLMs knowledge!</p><h1 id=matrix-orientation-hidden-state-alignment-weight-transfer-and-knowledge-distillation-mohawk>Matrix Orientation, Hidden-State Alignment, Weight-Transfer and Knowledge Distillation (Mohawk)
<a class=heading-link href=#matrix-orientation-hidden-state-alignment-weight-transfer-and-knowledge-distillation-mohawk><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>The goal of Mohawk is to take a Transformer model (in our context this is a Teacher Model), and replace some (or all) self-attention parts with Mamba2 (this we will call Student Model). The knowledge transfer happens in 3 stages, where in each consecutive stage we train more parameters, each responsible for transferring different information.</p><h2 id=stage-1>Stage 1
<a class=heading-link href=#stage-1><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>In the first stage, we focus on aligning the Sequence Mixers between the Student Model and the Teacher model. For example, if we were to distill from Llama3, we would align the output from <a href=https://github.com/meta-llama/llama3/blob/main/llama/model.py#L90C7-L90C16 class=external-link target=_blank rel=noopener>here</a>.</p><p><strong>Objective</strong></p><p>$$\min_{\phi}||\text{TeacherMixer}(u) - \text{StudentMixer}_{\phi}(u) ||_F$$</p><ul><li>$u$ is the output of the preceding layer of the Teacher</li></ul><h3 id=remark>Remark!
<a class=heading-link href=#remark><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>This optimization can run in parallel, and we can precompute the Teacher part in advance. It is crucial that both teacher and student mixer have the same preceding transformations, which means they take input and produce outputs of the same shape.</p><h3 id=modifications-to-mamba>Modifications to Mamba
<a class=heading-link href=#modifications-to-mamba><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Mamba and Mamba2 are richer than the self-attention operation since they can also act as Channel Mixers. Because of this, the authors made one modification to Mamba, which is replacing the local convolution with the identity operation. This nullifies its effect, and according to the following <a href=/posts/ssm-the-illusion/>research</a>, it is not really needed since Mamba itself is expressive enough to capture this information.</p><h2 id=stage-2>Stage 2
<a class=heading-link href=#stage-2><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>As mentioned in stage 1, the role of self-attention in an attention block is to perform sequence mixing. In stage 2, we go a step further and align the whole Attention block.</p><p><strong>Objective</strong>
$$ \min_{\phi}||\text{AttentionBlock}(u) - \text{StudentMixtureBlock}_{\phi} (u)||_2$$</p><p>Here it is worth noting that Mamba(2) is also the channel Mixer, thus the student block essentially stays the same between stages 1 and 2. We can view Stage 2 as a correction to Stage 1, which by itself pushes the attention weights in a slightly wrong direction. As with Stage 1, we can train all the blocks in parallel, on materialized data.</p><h3 id=modifications-to-mamba-1>Modifications to Mamba
<a class=heading-link href=#modifications-to-mamba-1><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>The authors decided to remove the Normalization layers, and they set the gate to 1 to cancel out its initial effect.</p><p><img alt="Here sigma is the gate, by forcing it to 1 we open it" src=/images/mamba_layer.png></p><h2 id=stage-3>Stage 3
<a class=heading-link href=#stage-3><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Here we are going to train the student model as a whole, but first we transfer the remaining parts from the teacher model. These remaining parts are:</p><ul><li>MLPs, which we will freeze since they should contain most of the model&rsquo;s learned knowledge</li><li>initial embedding layer</li><li>final layer normalization</li><li>language modeling heads</li><li>input normalization of each block</li></ul><p><strong>Objective</strong></p><p>$$ \min_{\phi}L_{CE}(\text{TeacherModel}(x), \text{StudentModel}_{\phi}(x)) $$</p><p>This is a very common approach where the student tries to mimic the distribution of the teacher.</p><h2 id=phi-mamba>Phi Mamba
<a class=heading-link href=#phi-mamba><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>To showcase the effectiveness of this approach, the authors distilled two hybrid models. Since they used the stellar Phi-1.5 as the teacher model, they coined the models Phi-Mamba.</p><h3 id=architecture>Architecture
<a class=heading-link href=#architecture><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Phi-Mamba retains 4 attention layers, with the rest replaced by Mamba2. However, as mentioned above, there are some modifications to the original architecture:</p><ul><li>removed post convolution activation</li><li>set convolution to identity, disabling this feature</li><li>removed pre-output normalization</li><li>changed from multi-value to multi-head to match the behavior of attention</li><li>dropped the discretization by making A purely input dependent</li></ul><p>These modifications are not too major and do not require changes to the SSD algorithm.</p><h3 id=results>Results
<a class=heading-link href=#results><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>The actual training was done on 3 Billion tokens! Yes billions, not trillions! At the beginning of the post we saw that Zamba2 7B was pretrained on 3T tokens with an estimated budget of around 600k. By naively extrapolating, we could say that Mohawk enables distillation at 0.1% of the cost, making it around 600 US Dollars. Yes sure, there are vast differences in the architecture, but this is again one of the first steps in exploring new architectures with shortcuts in pretraining.</p><p><img src=/images/phi_1_5_mamb_2_results.png></p><p>The actual token distribution was 80M for Stage 1, 160M for Stage 2, and 2.78B for Stage 3.</p><h4 id=stage-importance>Stage Importance
<a class=heading-link href=#stage-importance><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>To better grasp the importance of various stages, the authors decided to train 3 models:</p><ul><li>Phi-Mamba is a pure Mamba2 model</li><li>H-Phi-Mamba is the hybrid</li><li>Phi is a pure transformer model, created by randomly reinitializing the student&rsquo;s attention weights</li></ul><p>All the models were distilled with Mohawk on 5B tokens instead of 3B.</p><p><img src=/images/phi_1_5_mamb_2_stage_comparison.png></p><p>We can see that most of the training budget was spent on Stage 3, and we spent relatively little in the first 2 stages. However, we can see massive gains. Even when we just apply stages 2 and 3, we get improvements compared to performing vanilla knowledge distillation. And lastly, even a bit of Stage 1 is key for the student to retain the teacher&rsquo;s performance.</p><h2 id=remarks>Remarks
<a class=heading-link href=#remarks><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The idea of mixer-wise alignment for knowledge distillation is still in its infancy. However, it is an extremely cool concept, and it enables creating new models without the need for expensive pretraining. However, as we can see, there are still strict constraints on how a student model can look, where Mamba2 is used to mimic the behavior of self-attention.</p><h1 id=mamba-in-the-llama>Mamba in the Llama
<a class=heading-link href=#mamba-in-the-llama><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>To a certain degree, the authors build upon the research done in <a href=/posts/from-mamba-to-mamba2/>[State Space Duality]</a>, which focused on the connection between Linear Attention and the Recurrent form of State Space Models.</p><h2 id=from-linear-attention-to-a-linear-rnn>From Linear Attention to a Linear RNN
<a class=heading-link href=#from-linear-attention-to-a-linear-rnn><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>To recap, let&rsquo;s start with standard masked multi-head Attention
$$Q_t = W^Qo_t, K_t = W^Ko_t, V_t = W^Vo_t, \text{ for all t} $$
$$ \alpha_1, \cdots, \alpha_T = \text{softmax}([Q^T_qK_q, \cdots, Q^TK_T]/ \sqrt{D}) $$
$$ y_t = \sum_{s=1}^t m_{s,t}a_sV_s $$</p><ul><li>$m_{s,t} = 1(s \le t)$ is our causal mask</li></ul><p>By dropping softmax, we can reexpress it as:</p><p>$$ y_t = \sum{s=1}^tm_{s,t}a_s V_s = \frac{1}{\sqrt{D}}Q_t \sum_{s=1}^t(m_{s,t}K_s^TV_s) = \frac{1}{\sqrt{D}} Q_t \sum_{s=1}^tm_{s,t}K_s^TW^vo_s $$</p><p>If we compare it to the definition of a Linear Recurrent Neural Network:</p><p>$$h_{t} = A_t h_{t-1} + B_t x_t $$
$$ y_t = C_t h_t $$</p><p>It&rsquo;s not hard to see that there is a lot of similarity, and we can express linear Attention as a Linear RNN</p><p>$$h_t = m_{t-1,t}h_{t-1} + K_t V_t $$
$$ y_t = \frac{1}{\sqrt{D}}Q_th_t$$
$$ \downarrow $$</p><p>$$ h_t = A_t h_{t-1} + B_t x_t$$
$$ y_t = C_t h_t $$
$$ A_t = m_{t-1,t}, B_t = W^Ko_t, C_t = W^Qo_t, x_t = W^vo^t $$</p><p>However, there is a catch: $h \in R^{N \times 1}$, which means that the hidden state is capable of storing only one scalar over time per hidden dimension, greatly reducing its expressivity! This is one of the main reasons why linear attention did not become more mainstream. Luckily for us, Mamba (and Mamba2) provides an efficient way to expand the hidden state size while still maintaining the nice recurrent form.</p><h2 id=deriving-mamba-from-attention>Deriving Mamba from Attention
<a class=heading-link href=#deriving-mamba-from-attention><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>First, let&rsquo;s recap the Mamba equation:</p><p>$$h^t(k) = A_h(k) + B(k)x(k) $$
$$ y(k) = C(k)h(k) $$
Here A is a diagonal matrix and the rest is a continuous signal</p><p>We can now use V, K, Q from attention to initialize x, B, C of Mamba:</p><p><img src=/images/attention_to_mamba_algorithm.png></p><p>This introduces a couple of extra parameters. First, there is a need for a Neural Network to perform the discretization of the continuous signal, and second, we need the values for A. As it turns out, by reusing attention weights, we greatly jumpstart the model&rsquo;s performance:</p><p><img src=/images/attention_initialized_mamba.png></p><p>This figure compares two models: one is a pure Mamba model and the second is a 50% Hybrid. We compare the Perplexity of both models, and it&rsquo;s clearly obvious that Attention initialization leads to significantly lower perplexity, which is most evident in a pure Mamba model!</p><h2 id=hybrid-model>Hybrid Model
<a class=heading-link href=#hybrid-model><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>We already have an algorithm that is efficient at reusing attention weights, let&rsquo;s see how far we can go and how many attention layers we can transfer.</p><p><img src=/images/attention_to_mamba_initialization_and_training.png></p><p>It is crucial to note that we freeze most of the remaining layers, especially the Fully Connected layers, since they should contain most of the model&rsquo;s knowledge! We only train the transferred weights and the extra parameters.</p><h3 id=knowledge-distillation>Knowledge Distillation
<a class=heading-link href=#knowledge-distillation><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>We can divide Mamba in the Llama&rsquo;s knowledge distillation into two parts:</p><ol><li><strong>Supervised Fine-Tuning</strong></li></ol><p>Here we combine two approaches:</p><ul><li>Word level KL divergence, where the student is forced to match the whole probability distribution of the teacher over the entire set of tokens</li></ul><p>$$ \text{KL}(p(.| \hat{y_{1:t}}, x, \theta_T) || p(.|\hat{y}_{1:t}, x ,\theta))$$</p><ul><li>Sentence Level Knowledge Distillation (<a href=https://arxiv.org/abs/1606.07947 class=external-link target=_blank rel=noopener>SeqKD</a>), where the student is optimized on the output of the teacher ($\hat{y}<em>{1 \cdots t}$, also known as pseudo-labels) instead of the ground truth $y</em>{1,\cdots, t}$.</li></ul><p>$$\sum_{t=1}^T \alpha \log p(\hat{y_{t+1}}| \hat{y}_{1:t}, x, \theta)$$</p><p>The overall objective is just the weighted combination of both:</p><p>$$L(\theta) = - \sum_{t=1}^T \alpha \log p(\hat{y_{t+1}}| \hat{y}_{1:t}, x, \theta) + \beta $$</p><ol start=2><li><strong>Preference Optimization</strong>
By performing supervised finetuning in the first part, we undo the preference optimization performed on the original model. By reintroducing it, we should gain extra performance. The authors leveraged Direct Preference Optimization (DPO), where the teacher acted as the reference model.</li></ol><p>Here is the objective:</p><p>$$ \max_{\theta}E_{x \sim D, y \sim p(y|x;\theta)}[r_{\phi}(x,y)] - \beta KL(p(y|x;\theta) || \theta(y| x; \theta_{\text{Teacher}}) $$</p><ul><li>$r_{\phi}(x,y)$ is a reward function, where $\phi$ is optimized with regard to the reward</li><li>since we use DPO, we do not have a reward model as in reinforced learning, it is just classification since it is basically just supervised learning.</li></ul><h4 id=data>Data
<a class=heading-link href=#data><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>For the teacher&rsquo;s pseudo labels we leverage: UltraChat and UltraFeedback, for the word level objective we use GenQua, InfinityInstruct and OpenHermes 2.5.</p><p>For DPO we use UltraFeedback if the teacher is Zephyr and SimPO and Zephyr if the teacher is Llama3.</p><p>Overall we train on 20B Tokens!</p><h3 id=experiments>Experiments
<a class=heading-link href=#experiments><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>We use two teacher models: Zephyr-7B and Llama3-instruct. In student models, we replace attention by either Mamba or Mamba2 with 50%, 25%, 12.5% or 0% of retained attention layers.</p><h4 id=results-1>Results
<a class=heading-link href=#results-1><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>For a chat Specific benchmark:</p><p><img src=/images/attention_to_mamba_chat_results.png></p><p>For a more general benchmark:</p><p><a href=/images/attention_to_mamba_general_results.png></a></p><p>To a certain degree, the results are somewhat disappointing. It is clearly obvious that replacing Attention with Mamba hurts, especially in cases where we drop most of the attention layers.</p><h1 id=final-impression>Final Impression
<a class=heading-link href=#final-impression><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>My first impression to both distillation methods were disappoint, especially if we look at the performance of hybrid models - the best performance was achieved in cases where we retained more attention layers. Before I thoroughly read the papers I had high hopes that it would be applicable to more experimental models, however in both cases the hybrid models were nearly a 1-to-1 match to their transformer counterparts. To a certain degree this makes sense, as most of the knowledge in Transformer models is kept in the channel mixer (FFN) part. Now Mamba can perform both sequence and channel mixing at the same time, which means that it is able to store a lot of knowledge! This is most obvious in models like Zyphra (2), where we have a lot of stacked Mamba blocks with minimal attention.</p><p>Still, this is one of the first papers that discusses cross-architecture knowledge distillation, and even with the shortcomings, the results are promising.</p></div><footer><section class=see-also></section><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2020 -
2024
n1o_c0rTx
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ")}</script></body></html>