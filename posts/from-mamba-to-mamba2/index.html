<!doctype html><html lang=en><head><title>From Mamba to Mamba-2 · Data, Code and Breaking Stuff
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="n1o_c0rTx"><meta name=description content="Abstract Link to heading This is not my first gig where I write about State Space Models. I already mentioned them here and here. Now what is the deal with this Mamba(2) thing? They are proving to be an alternative to the strong Transformer++ architecture (Transformer++ models like LLaMa are based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention). Hold on, if this Transformer++ models work well, why do we need altneratives?"><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="From Mamba to Mamba-2"><meta name=twitter:description content="Abstract Link to heading This is not my first gig where I write about State Space Models. I already mentioned them here and here. Now what is the deal with this Mamba(2) thing? They are proving to be an alternative to the strong Transformer++ architecture (Transformer++ models like LLaMa are based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention). Hold on, if this Transformer++ models work well, why do we need altneratives?"><meta property="og:url" content="https://n1o.github.io/posts/from-mamba-to-mamba2/"><meta property="og:site_name" content="Data, Code and Breaking Stuff"><meta property="og:title" content="From Mamba to Mamba-2"><meta property="og:description" content="Abstract Link to heading This is not my first gig where I write about State Space Models. I already mentioned them here and here. Now what is the deal with this Mamba(2) thing? They are proving to be an alternative to the strong Transformer++ architecture (Transformer++ models like LLaMa are based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention). Hold on, if this Transformer++ models work well, why do we need altneratives?"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-08T09:57:32+02:00"><meta property="article:modified_time" content="2024-08-08T09:57:32+02:00"><meta property="article:tag" content="NLP"><meta property="article:tag" content="SSM"><link rel=canonical href=https://n1o.github.io/posts/from-mamba-to-mamba2/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://n1o.github.io/>Data, Code and Breaking Stuff
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/awesome-t5/>Awesome T5</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/from-mamba-to-mamba2/>From Mamba to Mamba-2</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2024-08-08T09:57:32+02:00>August 8, 2024
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
18-minute read</span></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/nlp/>NLP</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/ssm/>SSM</a></span></div></div></header><div class=post-content><h1 id=abstract>Abstract
<a class=heading-link href=#abstract><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>This is not my first gig where I write about State Space Models. I already mentioned them <a href=/posts/hungry-hungry-hippos/>here</a> and <a href=/posts/butterflies-monarchs-hyenas-and-lightning-fast-bert/>here</a>. Now what is the deal with this Mamba(2) thing? They are proving to be an alternative to the strong Transformer++ architecture (Transformer++ models like LLaMa are based on Rotary Embedding, SwiGLU, MLP, RMSNorm, without linear bias, sometimes with grouped query attention and/or sliding window attention). Hold on, if this Transformer++ models work well, why do we need altneratives? There are multiple reason:</p><ol><li><p><strong>Performance</strong>: Self-attention with a causal mask has a quadratic bottleneck, and as the sequence length becomes longer, this becomes a problem. Resolving this issue is a field of active research. One possible solution is to use Linear Attention, which we will cover since it is one of the basics Mamba-2 builds upon. Another possibility is to use Sliding Window Attention, which constrains the context for the next token generation to the past N tokens, where N is the window size. This alleviates the memory requirements, though it makes the model less capable. Technically speaking, State Space Models scale linearly in terms of sequence length (quadratically with the state size, but in general, this is fixed).</p></li><li><p><strong>State</strong>: Attention is stateless; there is no hidden state that is sequentially updated. This is both a good and a bad thing. It is good because if the model needs to look something up, it will take into account everything it has seen before. This is super important as it enables in-context learning. It is bad because it has to keep track of everything it has seen before. With state space models, we have a hidden state that is updated every time we have a new input. Because of this, we can view the hidden state as a compressed representation of everything it has observed before. Again, this is both good and bad. It is good because this compressed representation is smaller than the whole sequence, making it more efficient. It is bad because the hidden state has to be large enough to store everything that is important and at the same time remain relatively small to be efficient, AND (it is capitalized for a reason!) the mechanism that updates the state has to do it in a meaningful way (this is something we are going to explore in more detail).</p></li><li><p><strong>Alternatives</strong>: I get it, this is subjective, but if we only do research into Transformers, we may never find anything better.</p></li></ol><p>Before we dive into the details of Mamba-1 and Mamba-2, let me give you a brief summary:</p><p><strong>Mamba-1</strong>: The idea behind Mamba is to make the updating mechanics of its hidden state input-dependent. This may sound intuitive, but previous SSM variants like H3 were Linear Time Invariant, which means that their updating mechanism did not change with time. I the remainder of the post I will use the term Mamba and Mamba-1 interchangeably.</p><p><strong>Mamba-2</strong>: Mamba-2 is generally just a simplification of Mamba, with stronger constraints on the structure of the hidden space update matrix and moving some projections to the beginning of the layer. This enables the usage of common scaling strategies used in transformers, like tensor and sequence parallelism, and the ability to split input sequences across multiple GPUs. Also, the authors build a solid theoretical foundation behind SSMs and Semi-Separable Matrices and prove they have a primal-dual relationship with Linear Attention.</p><h1 id=mamba>Mamba
<a class=heading-link href=#mamba><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><h2 id=structured-state-space-models-s4>Structured State Space Models (S4)
<a class=heading-link href=#structured-state-space-models-s4><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Structured State Space model is defined as a one-dimensional function of sequences $x(t) \in R \rightarrow y(t) \in R$ mapped trough a hidden state $h(t) \in R^N$.</p><p>The actual model consist of four parameters $\Delta, A, B, C$ and we can express it as:</p><p>$$h(t) = Ah(t) + Bx(t) $$
$$y(t) = Ch(t)$$</p><ul><li>$A \in R^{N \times N}$ is contained to be diagonal</li><li>$B \in R^{N \times 1}$</li><li>$C \in R^{1 \times N}$</li></ul><p>Because of the constraints, we can represent all matrices with N numbers. To generalize to a multi-dimensional input, we apply the SSM independently to each channel, making the total memory requirements $O(BLDN)$.</p><p>Since we work with continuous time but process discrete data, we need to discretize the model:</p><p>$$ h_t = \bar{A} h_{t-1} + \bar{B}x_t $$
$$ y_t = Ch_t $$</p><ul><li>$\bar{A} = f_A(\Delta, A)$</li><li>$\bar{B} = f_B(\Delta, A, B)$</li><li>with $f_A, f_B$ being discretization rules. For example we can use <a href=https://en.wikipedia.org/wiki/Zero-order_hold class=external-link target=_blank rel=noopener>Zero-Order hold</a></li></ul><p>To actually compute this model, we use global convolution:</p><p>$$ y = x * \bar{K} $$</p><ul><li>K is our kernel that is implicitly parametrized by an SSM</li></ul><p>$$ \bar{K} = (C\bar{B}, C\bar{AB}, \cdots, C\bar{A}^k\bar{C}, \cdots)$$</p><p>The benefit of this is that we can use Fast Fourier Transform to compute the convolution in $O(N \log N)$ time.</p><h3 id=linear-time-invariance-lti>Linear Time Invariance (LTI)
<a class=heading-link href=#linear-time-invariance-lti><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Just from the definition above, we can see that the $(\Delta, A, B, C)$ do not depend on $x$ nor $t$. This is one of the main drawbacks and the reason why State Space Models were struggling with in-context learning.</p><h2 id=selective-state-space-models-s6>Selective State Space Models (S6)
<a class=heading-link href=#selective-state-space-models-s6><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p><img alt=S6 src=/images/selective_state_space_models.png></p><p>One easy fix is to take the same model as above but make the parameters $\Delta, A, B$ functions of the input:</p><h3 id=algorithm>Algorithm
<a class=heading-link href=#algorithm><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li>we have input $x: (B,L,D)$ (Batch, Length, Dimension)</li><li>output $y: (B, L, D)$</li></ul><ol><li>$A: (D,N) \leftarrow \text{Parameters}$</li><li>$B: (B,L,D) \leftarrow s_B(x)$</li><li>$C: (B,L,D) \leftarrow s_C(x)$</li><li>$\Delta: (B,L,N) \leftarrow \tau_{\Delta}(\text{Parameter} + s_{\Delta}(x))$</li><li>$\bar{A}, \bar{B}: (B, L, D, N) \leftarrow \text{discretize}(A,B)$
6: $y \leftarrow \text{SSM}(\bar{A}, \bar{B}, C)(x)$</li></ol><ul><li>$A$ is still diagonal</li><li>$s_B(x) = \text{Linear}_N(x)$</li><li>$s_C(x) = \text{Linear}_N(x)$</li><li>$s_{\Delta} = \text{Broadcast}_D(\text{Linear}_1(x))$ (we choose this due to a connection to Recurrent Neural Networks)</li><li>$\tau_{\Delta} = \text{softplus}$ (we choose this due to a connection to Recurrent Neural Networks)</li><li>$\text{Linear}_d$ is parametrized projection to dimension d</li></ul><h3 id=selective-scan>Selective Scan
<a class=heading-link href=#selective-scan><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Since the dynamics of the model are dynamic, we cannot use global convolution anymore. Because of this, we define selective scan, which is a hardware-aware algorithm. The actual implementation is rather <a href=https://github.com/state-spaces/mamba/blob/62db608da60f6fc790b8ed9f4b3225e95ca15fde/csrc/selective_scan/selective_scan_fwd_kernel.cuh class=external-link target=_blank rel=noopener>involved</a>. The main idea is that we load the parameters $\Delta, A, B, C$ from HBM to SRAM, perform the discretization and recurrence in SRAM, and write the final output of size (B, L, D) back to main memory (HBM). To reduce memory requirements, the intermediate steps are not stored but recomputed during the backward pass.</p><h3 id=benefits-of-natural-selection>Benefits of (Natural) Selection
<a class=heading-link href=#benefits-of-natural-selection><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Because of the selection mechanism, the model can choose what to store (or not) in its hidden state based on what it currently sees. It may also choose to reset its hidden state and start over. Selection enables the model to have strong in-context learning capabilities.</p><h2 id=mamba-layer>Mamba Layer
<a class=heading-link href=#mamba-layer><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The core of the Mamba architecture is the Mamba layer:</p><p><img alt=Mamba src=/images/mamba_layer.png></p><p>We are already familiar what is happening inside the SSM (Selective Scan) part of the Mamba. Prior to it we have two projections that expand the dimensionality of the input, than we perform short convolution as in M2 Bert with <a href=https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html class=external-link target=_blank rel=noopener>torch.nn.Conv1d</a> on one branch on the other branch we apply just SiLu non-linearity (This is the same as the Gated approach found in other LLMs). After that we perform an additional projection, and we have all the inputs prepared for the SSM block. The output of the SSM block is than multiplied with the residual gate branch and finally we project the dimension back to match the input dimension.</p><h1 id=mamba-2>Mamba-2
<a class=heading-link href=#mamba-2><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Mamba is a cool innovation, and it has led to multiple cool models, especially attention-SSM hybrid models like <a href=https://github.com/microsoft/Samba class=external-link target=_blank rel=noopener>Samba</a> and <a href=https://github.com/Zyphra/transformers_zamba class=external-link target=_blank rel=noopener>Zamba</a>. However, the authors recognize some of its shortcomings. Its biggest weak point compared to Transformers is the lack of research in terms of scaling. For Transformers, we have multiple system optimizations on how to split up a model or how to split up processing long sequences into more GPUs. Here is two of them:</p><ol><li><strong>Tensor Parallelism</strong>: This allows splitting each layer in a large Transformer model onto multiple GPUs on the same node.</li><li><strong>Sequence Parallelism</strong>: This allows splitting a sequence into smaller parts, with each GPU holding one part.</li></ol><p>Mamba-2 is designed in a way that allows for Sequence Parallelism by passing the recurrent state between multiple GPUs. Tensor Parallelism is possible because of independent parallel projections of A, B, C, and X inputs of its SSM part.</p><h2 id=semi-separable-matrices>Semi-Separable Matrices
<a class=heading-link href=#semi-separable-matrices><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>This is a special structured matrix. We say that a lower triangular matrix $M$ is N-semi separable if every submatrix contained in the lower triangular part has rank at most N.</p><p>Here, we are more interested in a special representation of N-semi separable called Sequentially Semi Separable (SSS).</p><h3 id=sequentially-semi-separable-n-sss>Sequentially Semi Separable (N-SSS)
<a class=heading-link href=#sequentially-semi-separable-n-sss><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>A lower triangular matrix $M \in R^{(T,T)}$ has an N-sequentially semiseparable representation if we can write it as:</p><p>$$ M_{ij} = C_j^TA_j \cdots A_{i+1}B_i$$</p><ul><li>$B_0, \cdots, B_{T - 1}, C_0, \cdots, C_{T-1} \in R^N$ are vectors</li><li>$A_0, \cdots, A_{T-1} \in R^{(N,N)} $</li></ul><p>To express it in matrix form we define the SSS operator:</p><p>$$ M = SSS(A_{0:T}, B_{0:T}, C_{0:T})$$</p><p>It turns out that every N-semiseparable matrix M is also an N-sequentially semiseparable matrix. The main const of N-SSS representation that we can compress down the parameters to $O(NT)$</p><h2 id=state-space-duality>State Space Duality
<a class=heading-link href=#state-space-duality><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Let&rsquo;s start by exploring a special case of 1-semiseparable (1-SS or just 1SS). This can be written in the Sequentially Semi-Separable form as:</p><p>$$SSS(a,b,c) = \text{diag}(c) \cdot M \cdot \text{diag}(b) $$</p><ul><li>$M_{ij} = \prod_{t=j}^i a_t = a_{j:i}^{\times}$</li></ul><p>M is an 1-SS</p><p>$$M = 1SS(a_{0:T}) = \begin{bmatrix} 1 \\ a_1 && 1 \\ a_{2}a_1 && a_2 && 1 \\ \vdots && \vdots && \ddots && \ddots \\ a_{T-1}\cdots a_1 && a_{T-1}a_2 && \cdots && a_{T-1} && 1 \end{bmatrix}$$</p><h3 id=state-space-models-are-separable-matrices>State Space Models are Separable Matrices
<a class=heading-link href=#state-space-models-are-separable-matrices><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>We make a special assumption that we have a State Space Model without projections (no B, C) and the state dimension $N = 1$. Then we can express the multiplication $y = Mx$ as a recurrence:</p><p>$$y_t = a_{t:0}x_0 + \cdots + a_{t:t}x_t $$
$$y_t = a_t(a_{t-1:0}x_0 \cdots a_{t-1:t-1}x_{t-1} + a_{t:t}x_t $$
$$y_t = a_t y_{t-1} + x_t$$</p><p>We can generalize this further by expressing any State Space Model as matrix multiplication by an N-semiseparable matrix in a sequentially semiseparable form:</p><p>$$y = SSM(A,B,C)(x) = SSS(A,B,C) \cdot x $$</p><h2 id=linearrecurrent-and-dualquadratic-form>Linear(Recurrent) and Dual(Quadratic) form
<a class=heading-link href=#linearrecurrent-and-dualquadratic-form><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>We already know we can express a State Space model as a matrix multiplication by an N-separable matrix in a sequentially semiseparable form:</p><p>$$ y = SSS(A,B,C) \cdot x $$</p><p>However, if we naively first compute the $SSS$ part and then multiply by $x$, we end up with an $O(T^2)$ complexity. There is a more efficient recurrent way. However, let&rsquo;s break down the quadratic form first, since it has a tight connection to Attention.</p><h3 id=dual-quadratic-form>Dual (Quadratic) Form
<a class=heading-link href=#dual-quadratic-form><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Here, we take a small detour from SSMs and look into Linear Attention. We can express the attention mechanism as:</p><p>$$Y = \text{softmax}(QK^T) V $$</p><p>This is the most common form of attention, called Softmax Attention. By applying a causal mask, we get the following:</p><p>$$Y = (L \circ \text{softmax}(QK^T)) \cdot V $$</p><ul><li>$L$ is an lower triangular matrix with ones on and below the main diagonal</li></ul><p>In linear attention we drop the softmax to get:</p><p>$$Y = (L \circ (QK^T)) \cdot V $$</p><p>This form is way nicer and we can rewrite it using einsum as:</p><p>$$Y = \text{einsum}(TN,SN,SP, TS \rightarrow TP)(Q,K,V,L)$$</p><p>Or we can express it as pairwise matrix multiplication:</p><ol><li>$G = \text{einsum}(TN,SN \rightarrow TS)(Q,K)$ resulting shape (T,S)</li><li>$M = \text{einsum}(TS,TS \rightarrow TS)(G,L)$ resulting shape (T,S)</li><li>$Y = \text{einsum}(TS,SP \rightarrow TP)(M,V)$ resulting shape (T,P)</li></ol><ul><li>T, S are the target source dimensions, for autoregressive self-attention they are the same</li><li>P is the head dimensionality</li></ul><h3 id=linear-recurrent-form>Linear (Recurrent) Form
<a class=heading-link href=#linear-recurrent-form><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Until now, we have just removed the softmax operation. However, we can go further by changing the order of matrix association, resulting in the following:</p><p>$$(QK^T)V = Q(K^TV) $$</p><p>With this, we can re-express the definition of $Y$ as:</p><p>$$ Y = Q \cdot \text{cumsum}(K^TV)$$</p><ul><li>cumsum is just the cumulative sum</li></ul><p>It may seem that we got rid of the causal mask. This is technically not true, since the cumsum operation is a causal operation, and we just hid it. To make this clearer, we can express the same equation using einsum:</p><ol><li>$Z = \text{einsum}(SP,SN \rightarrow SPN)(V,K)$ resulting shape (S,P,N)</li><li>$H = \text{einsum}(TS,SPN \rightarrow TPN)(V,K)$ resulting shape (T,P,N) this being optimized with subquadratic matrix multiplication</li><li>$Y = \text{einsum}(TN,TPN \rightarrow TP)(V,K)$ resulting shape (T,P)</li></ol><p>Lets break down the equation:</p><ol><li>Expands the dimensionality by a factor N</li><li>Uses the mask matrix L explicitly, we flatten the dimensions of (P,N) resulting in multiplying an lower triangular matrix with an vector. This just just an cumulative sum operation:</li></ol><p>$$ y = \begin{bmatrix} 1 \\ \cdots && \ddots \\ 1 && \cdots && 1 \end{bmatrix}x \Leftrightarrow \begin{matrix} y_0 = x_0 \\ y_t = y_{t-1} + x_t\end{matrix} $$</p><ol start=3><li>Contracts the dimensionality back to P</li></ol><h2 id=state-space-models-and-recurrent-linear-attention>State Space Models and Recurrent Linear Attention
<a class=heading-link href=#state-space-models-and-recurrent-linear-attention><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The hints that there should be a connection between the recurrent form of Linear Attention and the State Space Model should be obvious.</p><p>Lets remind us about the definition of the State Space Model using SSS:</p><p>$$ Y = SSS(A,B,C) \cdot x $$</p><p>The SSS matrix M is defined as:</p><ul><li>$M_{ji} = C_j^TA_{j:i}B_i$</li></ul><p>By constraining the A matrix to be diagonal $A = aI$ we can rearrange the terms a bit to get:</p><p>$$ M_{ji} = A_{j:i} \cdot (C_j^TB_i)$$</p><p>The equation for M in matrix form becomes:</p><p>$$L = 1SS(a)$$
$$M = L \circ (CB^T)$$</p><ul><li>$B,C \in R^{(T,N)}$</li></ul><p>Now we can compute $Y = MX$ using einsum as:</p><ol><li>$G = \text{einsum}(TN,SN \rightarrow TS)(C,B)$ resulting shape (T,S)</li><li>$M = \text{einsum}(TS,TS \rightarrow TS)(G,L)$ resulting shape (T,S)</li><li>$Y = \text{einsum}(TS,SP \rightarrow TP)(M,X)$ resulting shape (T,P)</li></ol><p>If we assume that S = T, we end up with the same equations as in the Recurrent form of Linear Attention. And that is it, we have our duality.</p><h2 id=mamba-2-layer>Mamba-2 Layer
<a class=heading-link href=#mamba-2-layer><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>At the beginning, I mentioned that there are few differences between Mamba and Mamba-2. One of them is a stronger constraint on the matrix A, for Mamba-2 it is $A = aI$ in Mamba it was $A = \text{diag}(a)$. The reason to constrain to $A = aI$ is that we can express the SSM as a matrix multiplication of an 1-SS matrix, which is more efficient to compute.</p><p><img alt=S6 src=/images/mamba_2_architecture.png></p><p>In the image above, we can see the differences between Mamba and Mamba-2. While the idea of Mamba was to have a function $X \rightarrow Y$, in Mamba-2, we instead think of a mapping of $A, B, C, X \rightarrow Y$. Because of this, we can parallelize the computation of the projections at the beginning of the block. This enables tensor parallelism and reduces the number of parameters. This is also analogous to Attention, where $X, B, C$ correspond to $Q, K, V$.</p><p>Additionally, Mamba-2 introduces a larger head dimension $P$. While Mamba leverages $P =1 $, Mamba-2 leverages $P = {64, 128}$. Again, this is similar to conventions in Transformer Architecture. What does this head dimension in Mamba mean? If we have a head dimension of 1, we are computing an SSM for each channel independently. By increasing the head dimension, we achieve a sort of weight-tying where we share SSMs across multiple channels.</p><p>Overall, it may seem that Mamba-2 is less expressive than Mamba. However, due to optimizations, we are able to train Mamba-2 models with much larger state dimensions (in Mamba-1 we had $N=16$, whereas in Mamba-2 we can go up to $N=256$ or more), while also being much faster during training.</p><p>The model also adds an additional normalization layer, which improves the stability of larger models. There is nothing more to say about Mamba-2; it is simply a more efficient version of Mamba, incorporating many lessons learned from Transformers and the strong theoretical foundation behind SSMs and Semi-Separable Matrices.</p><h3 id=algorithm-1>Algorithm
<a class=heading-link href=#algorithm-1><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>As with Mamba-1, we cannot use Global Convolution. For Mamba-2, we need an efficient way to compute the matrix $M$. Luckily, the computation is much simpler than for Mamba-1, and we do not need to implement a low-level GPU kernel. The algorithm consists mostly of matrix multiplications.</p><p><img alt="Mamba-2 Blocks" src=/images/mamba_2_diagonal_off_diagonal_blocks.png></p><p>This is an example for $T=9$ where we decompose it into chunks of length $Q = 3$, we can generalize it as:</p><ol><li>$M^{(j,j)} = SSM(A_{jQ:(j+1)Q},B_{jQ(j+1)Q},C_{jQ:(j+1)Q}$ for the diagonal blocks</li><li>$M^{(i,j)} = \begin{bmatrix}C_{jQ}^TA_{jQ:jQ-1} \ \vdots \ C^T_{(j+1)Q-1}A_{(j+1)Q-1:jQ-1}\end{bmatrix}A_{jQ-1:(i+1)Q-1} \begin{bmatrix}B_{iQ}^TA_{(i+1)Q-1:iQ} \ \vdots \ B_{(i+1)Q-1}^T A_{(i+1)Q-1:(i+1)Q-1}\end{bmatrix}^T$ for the off-diagonal low rank blocks</li></ol><h4 id=diagonal-blocks>Diagonal Blocks
<a class=heading-link href=#diagonal-blocks><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>The general idea is that $Q$ is rather small. Because of this, we can use the dual quadratic form of Structured Masked Attention (more on this later) and perform the computation for each block in parallel.</p><h4 id=low-rank-blocks>Low Rank Blocks
<a class=heading-link href=#low-rank-blocks><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>Here, we have three parts (the following example is the breakdown of the leftmost bottom block from the image above):</p><ol><li>$\begin{bmatrix} C_6^T A_{6:5} \\ C_7^TA_{7:5} \\ C_8^TA_{8:5} \end{bmatrix}^T$ this are the left factors (C-block factors)</li><li>$A_{5:2}$ this are the center factors (A-block factors)</li><li>$\begin{bmatrix} B_0^T A_{2:0} \\ B_1^TA_{2:1} \\ B_2^TA_{2:2} \end{bmatrix}^T$ this are the right factors (B-block factors)</li></ol><h4 id=pytorch>Pytorch
<a class=heading-link href=#pytorch><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>Compared to Mamba-1&rsquo;s selective scan the implementation is way more straight forward:</p><div class=highlight><pre tabindex=0 style=color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#fff;font-weight:700>def</span> segsum(x):
</span></span><span style=display:flex><span><span style=color:#0ff;font-weight:700>&#34;&#34;&#34;Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,
</span></span></span><span style=display:flex><span><span style=color:#0ff;font-weight:700>which is equivalent to a scalar SSM.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>T = x.size(-<span style=color:#ff0;font-weight:700>1</span>)
</span></span><span style=display:flex><span>    x_cumsum = torch.cumsum(x, dim=-<span style=color:#ff0;font-weight:700>1</span>)
</span></span><span style=display:flex><span>    x_segsum = x_cumsum[..., :, <span style=color:#fff;font-weight:700>None</span>] - x_cumsum[..., <span style=color:#fff;font-weight:700>None</span>, :]
</span></span><span style=display:flex><span>    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=<span style=color:#fff;font-weight:700>bool</span>), diagonal=<span style=color:#ff0;font-weight:700>0</span>)
</span></span><span style=display:flex><span>    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#fff;font-weight:700>return</span> x_segsum
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#fff;font-weight:700>def</span> ssd(X, A, B, C, block_len=<span style=color:#ff0;font-weight:700>64</span>, initial_states=<span style=color:#fff;font-weight:700>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#0ff;font-weight:700>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#0ff;font-weight:700>    Arguments:
</span></span></span><span style=display:flex><span><span style=color:#0ff;font-weight:700>    X: (batch, length, n_heads, d_head)
</span></span></span><span style=display:flex><span><span style=color:#0ff;font-weight:700>    A: (batch, length, n_heads)
</span></span></span><span style=display:flex><span><span style=color:#0ff;font-weight:700>    B: (batch, length, n_heads, d_state)
</span></span></span><span style=display:flex><span><span style=color:#0ff;font-weight:700>     C: (batch, length, n_heads, d_state)
</span></span></span><span style=display:flex><span><span style=color:#0ff;font-weight:700>    Return:
</span></span></span><span style=display:flex><span><span style=color:#0ff;font-weight:700>    Y: (batch, length, n_heads, d_head)
</span></span></span><span style=display:flex><span><span style=color:#0ff;font-weight:700>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#fff;font-weight:700>assert</span> X.dtype == A.dtype == B.dtype == C.dtype
</span></span><span style=display:flex><span>    <span style=color:#fff;font-weight:700>assert</span> X.shape[<span style=color:#ff0;font-weight:700>1</span>] % block_len == <span style=color:#ff0;font-weight:700>0</span>
</span></span><span style=display:flex><span>    <span style=color:#007f7f># Rearrange into blocks/chunks</span>
</span></span><span style=display:flex><span>    X, A, B, C = [rearrange(x, <span style=color:#0ff;font-weight:700>&#34;b (c l) ... -&gt; b c l ...&#34;</span>, l=block_len) <span style=color:#fff;font-weight:700>for</span> x in (X, A, B, C)]
</span></span><span style=display:flex><span>    A = rearrange(A, <span style=color:#0ff;font-weight:700>&#34;b c l h -&gt; b h c l&#34;</span>)
</span></span><span style=display:flex><span>    A_cumsum = torch.cumsum(A, dim=-<span style=color:#ff0;font-weight:700>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#007f7f># 1. Compute the output for each intra-chunk (diagonal blocks)</span>
</span></span><span style=display:flex><span>    L = torch.exp(segsum(A))
</span></span><span style=display:flex><span>    Y_diag = torch.einsum(<span style=color:#0ff;font-weight:700>&#34;bclhn,bcshn,bhcls,bcshp-&gt;bclhp&#34;</span>, C, B, L, X)
</span></span><span style=display:flex><span>    <span style=color:#007f7f># 2. Compute the state for each intra-chunk</span>
</span></span><span style=display:flex><span>    <span style=color:#007f7f># (right term of low-rank factorization of off-diagonal blocks; B terms)</span>
</span></span><span style=display:flex><span>    decay_states = torch.exp((A_cumsum[:, :, :, -<span style=color:#ff0;font-weight:700>1</span>:] - A_cumsum))
</span></span><span style=display:flex><span>    states = torch.einsum(<span style=color:#0ff;font-weight:700>&#34;bclhn,bhcl,bclhp-&gt;bchpn&#34;</span>, B, decay_states, X)
</span></span><span style=display:flex><span>    <span style=color:#007f7f># 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries</span>
</span></span><span style=display:flex><span>    <span style=color:#007f7f># (middle term of factorization of off-diag blocks; A terms)</span>
</span></span><span style=display:flex><span>    <span style=color:#fff;font-weight:700>if</span> initial_states is <span style=color:#fff;font-weight:700>None</span>:
</span></span><span style=display:flex><span>        initial_states = torch.zeros_like(states[:, :<span style=color:#ff0;font-weight:700>1</span>])
</span></span><span style=display:flex><span>    states = torch.cat([initial_states, states], dim=<span style=color:#ff0;font-weight:700>1</span>)
</span></span><span style=display:flex><span>    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -<span style=color:#ff0;font-weight:700>1</span>], (<span style=color:#ff0;font-weight:700>1</span>, <span style=color:#ff0;font-weight:700>0</span>))))
</span></span><span style=display:flex><span>    new_states = torch.einsum(<span style=color:#0ff;font-weight:700>&#34;bhzc,bchpn-&gt;bzhpn&#34;</span>, decay_chunk, states)
</span></span><span style=display:flex><span>    states, final_state = new_states[:, :-<span style=color:#ff0;font-weight:700>1</span>], new_states[:, -<span style=color:#ff0;font-weight:700>1</span>]
</span></span><span style=display:flex><span>    <span style=color:#007f7f># 4. Compute state -&gt; output conversion per chunk</span>
</span></span><span style=display:flex><span>    <span style=color:#007f7f># (left term of low-rank factorization of off-diagonal blocks; C terms)</span>
</span></span><span style=display:flex><span>    state_decay_out = torch.exp(A_cumsum)
</span></span><span style=display:flex><span>    Y_off = torch.einsum(<span style=color:#0ff;font-weight:700>&#39;bclhn,bchpn,bhcl-&gt;bclhp&#39;</span>, C, states, state_decay_out)
</span></span><span style=display:flex><span>    <span style=color:#007f7f># Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)</span>
</span></span><span style=display:flex><span>    Y = rearrange(Y_diag+Y_off, <span style=color:#0ff;font-weight:700>&#34;b c l h p -&gt; b (c l) h p&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#fff;font-weight:700>return</span> Y, final_state
</span></span></code></pre></div><h4 id=performance>Performance
<a class=heading-link href=#performance><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>Mamba-2, like Mamba-1, with hidden state size ( N ), has the same training speed ( O(TN^2) ) and inference speed ( O(N^2) ). However, the biggest improvement is the use of matrix multiplication in Mamba-2, which is much more efficient than the selective scan in Mamba-1.</p><h1 id=state-space-duality-additional-notes>State Space Duality Additional Notes
<a class=heading-link href=#state-space-duality-additional-notes><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Overall, the State Space Duality paper introduces many concepts; here are arguably the most important ones:</p><h2 id=structured-masked-attention>Structured Masked Attention
<a class=heading-link href=#structured-masked-attention><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>This builds upon the notion of linear attention, where we expressed the causal mask matrix L as a cumulative sum. However, we can generalize the mask matrix L to any matrix that supports fast matrix multiplication.</p><p>![[/images/structured_attention.png]]</p><p>In this case, we view the attention mechanism through the following equations (this is also the quadratic form mentioned earlier):</p><p>$$Y = MV$$</p><p>$$M = QK^T \circ L$$</p><p>Where L is our mask matrix, which we can choose as we like. In the context of State Space duality, we choose it as 1-semiseparable matrix.</p><h2 id=multi-patterns-for-ssms>Multi patterns for SSMs
<a class=heading-link href=#multi-patterns-for-ssms><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Again, this builds upon analogies to Attention, where multihead attention involves applying self-attention multiple times and concatenating the results. We can achieve something similar by applying the SSD algorithm and broadcasting it across multiple dimensions.</p><h3 id=multi-contract-ssm>Multi-Contract SSM
<a class=heading-link href=#multi-contract-ssm><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>This is analogous to Multi-Query Attention, where we share K and V across all the heads of Q. For attention, this makes a lot of sense since we cache K and V pairs.</p><p>In SSMs, this is equivalent to sharing X and B across multiple heads of the SSM, and having C (parameters that control the contraction) be independent per head.</p><h3 id=multi-expand-ssm>Multi-Expand SSM
<a class=heading-link href=#multi-expand-ssm><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Here, we share C and X across multiple heads, and B (controls expansion) is independent per head.</p><h3 id=multi-input-ssm>Multi-Input SSM
<a class=heading-link href=#multi-input-ssm><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Here, we share B and C across multiple heads, and X is independent. For an SSM like Mamba, we consider X as the input. Because of this, it is a better fit to have a unique X per head.</p><p>Technically, we can view the S6 layer introduced in Mamba as having Head Dimension P = 1, which means that each channel has independent SSM dynamics A, and it is a Multi-Input SSM where we share B and C matrices across all channels.</p><h1 id=tldr>TLDR;
<a class=heading-link href=#tldr><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>This was probably a lot to take in, so to sum it up, we introduced Mamba. Mamba is a State Space model whose dynamics are dependent on the input, which improves its ability for In-Context learning. Because we want efficient computation, we need to derive a hardware-efficient algorithm, and to do that, we need to enforce structure on the matrices used by Mamba. Mamba-2 tackles the efficiency problem by enforcing even more constraints, making the model more contained but easier to scale and allowing for larger hidden dimensions.</p></div><footer><section class=see-also></section><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2020 -
2024
n1o_c0rTx
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ")}</script></body></html>