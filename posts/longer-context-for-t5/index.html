<!doctype html><html lang=en><head><title>Longer Context for T5 · Data, Code and Breaking Stuff
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="n1o_c0rTx"><meta name=description content="
  Why does T5 need a longer context?
  
    
    Link to heading
  

In my previous post
T5 the Old New Thing
we already explored why T5 is awesome. But one downside is its limited context length of 512 tokens.
However, it does have a limitation - its context length is restricted to 512 tokens. This can&rsquo;t be directly compared to the context length of a decoder-only model, as T5 is an encoder-decoder model. This means that the encoder can process an input of up to 512 tokens, and the decoder can generate an output of up to 512 tokens, making the total context length 1024 tokens. In this article, we will discuss two extensions:"><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="Longer Context for T5"><meta name=twitter:description content=" Why does T5 need a longer context? Link to heading In my previous post T5 the Old New Thing we already explored why T5 is awesome. But one downside is its limited context length of 512 tokens. However, it does have a limitation - its context length is restricted to 512 tokens. This can’t be directly compared to the context length of a decoder-only model, as T5 is an encoder-decoder model. This means that the encoder can process an input of up to 512 tokens, and the decoder can generate an output of up to 512 tokens, making the total context length 1024 tokens. In this article, we will discuss two extensions:"><meta property="og:url" content="https://n1o.github.io/posts/longer-context-for-t5/"><meta property="og:site_name" content="Data, Code and Breaking Stuff"><meta property="og:title" content="Longer Context for T5"><meta property="og:description" content=" Why does T5 need a longer context? Link to heading In my previous post T5 the Old New Thing we already explored why T5 is awesome. But one downside is its limited context length of 512 tokens. However, it does have a limitation - its context length is restricted to 512 tokens. This can’t be directly compared to the context length of a decoder-only model, as T5 is an encoder-decoder model. This means that the encoder can process an input of up to 512 tokens, and the decoder can generate an output of up to 512 tokens, making the total context length 1024 tokens. In this article, we will discuss two extensions:"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-29T14:10:36+02:00"><meta property="article:modified_time" content="2024-04-29T14:10:36+02:00"><meta property="article:tag" content="NLP"><meta property="article:tag" content="T5"><meta property="article:tag" content="Transformers"><link rel=canonical href=https://n1o.github.io/posts/longer-context-for-t5/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://n1o.github.io/>Data, Code and Breaking Stuff
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/awesome-t5/>Awesome T5</a></li><li class=navigation-item><a class=navigation-link href=/awesome-ssm/>Awesome SSM</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/longer-context-for-t5/>Longer Context for T5</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2024-04-29T14:10:36+02:00>April 29, 2024
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
7-minute read</span></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/nlp/>NLP</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/t5/>T5</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/transformers/>Transformers</a></span></div></div></header><div class=post-content><h1 id=why-does-t5-need-a-longer-context>Why does T5 need a longer context?
<a class=heading-link href=#why-does-t5-need-a-longer-context><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>In my previous post
<a href=/posts/t5-the-old-new-thing/>T5 the Old New Thing</a>
we already explored why T5 is awesome. But one downside is its limited context length of 512 tokens.
However, it does have a limitation - its context length is restricted to 512 tokens. This can&rsquo;t be directly compared to the context length of a decoder-only model, as T5 is an encoder-decoder model. This means that the encoder can process an input of up to 512 tokens, and the decoder can generate an output of up to 512 tokens, making the total context length 1024 tokens. In this article, we will discuss two extensions:</p><ol><li><a href=https://arxiv.org/abs/2112.07916 class=external-link target=_blank rel=noopener>LongT5</a></li><li><a href=https://arxiv.org/abs/2303.09752 class=external-link target=_blank rel=noopener>ColtT5</a></li></ol><p>Both LongT5 and CoLT5 explore methods to extend the context length of the encoder part of T5. This implies that we are investigating ways to process longer input lengths, not necessarily to generate longer texts. This approach is particularly beneficial for tasks such as text summarization or document question answering.</p><h1 id=longt5>LongT5
<a class=heading-link href=#longt5><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Originally published in 2022 and it uses a new pretraining strategy called Pegasus and explores using Local Attention and a unique TGlobal attention in the encoder.</p><h2 id=pegasus>Pegasus
<a class=heading-link href=#pegasus><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Pegasus is a pretraining strategy specifically tailored for abstract summarization. In this approach, we mask out key (principal) sentences from a document and teach the model to reproduce them as a single string, as if creating a summary.</p><h2 id=local-attention>Local Attention
<a class=heading-link href=#local-attention><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Local Attention is essentially a sliding window attention mechanism. This means that any given token can attend to a neighborhood of $l$ tokens.</p><h2 id=tglobal>TGlobal
<a class=heading-link href=#tglobal><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>In TGlobal, we divide the input tokens into chunks of length $k$. For each chunk, we compute a global token by summing up the individual token embeddings. When performing attention, we take a Local Window $l$ and append all the global tokens to it.</p><p><img alt=TGlobal src=/images/t_global_attention.png></p><h3 id=cons>Cons
<a class=heading-link href=#cons><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>As we do not perform full attention, we experience a slight performance degradation, and we require a few additional parameters. In terms of computation, we calculate the global tokens on the fly, but they can be computed just once per input tokens per layer and cached.</p><h3 id=pros>Pros
<a class=heading-link href=#pros><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>We can process significantly larger input lengths.</p><h2 id=notes>Notes
<a class=heading-link href=#notes><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>It&rsquo;s worth mentioning that there is a variant of LongT5 that solely uses Local Attention, without Global Tokens. This variant can be scaled up to handle even longer sequences, but it also results in a more pronounced performance drop.</p><h1 id=colt5>CoLT5
<a class=heading-link href=#colt5><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Paper from 2023, and it builds upon LongT5 by bring in ideas like <a href=https://arxiv.org/abs/2101.03961 class=external-link target=_blank rel=noopener>Mixture of Experts</a> and <a href=https://arxiv.org/abs/1911.02150 class=external-link target=_blank rel=noopener>Multi Query Attention</a>.</p><h2 id=conditional-computation>Conditional Computation
<a class=heading-link href=#conditional-computation><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p><img alt="ColT5 Attention" src=/images/colt5_transformer_layer.png></p><p>The the idea behind Conditional Computation is that not all tokens carry the same importance, and therefore, we don&rsquo;t need to allocate the same computational resources to each of them. In the context of CoLT5, we have two branches: a light branch and a heavy branch. The light branch is applied to all tokens, while the heavy branch is only applied to important tokens. This branching occurs in two places: the attention layer and the feed-forward layer (Technically three places, since we route separately for queries and key-value pairs).</p><h3 id=attention>Attention
<a class=heading-link href=#attention><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>The light attention branch has fewer heads than the heavy branch. Moreover, the light branch employs only local attention, while the heavy branch utilizes full attention.</p><h3 id=feed-forward>Feed Forward
<a class=heading-link href=#feed-forward><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>In the feed-forward layer, the light branch has a lower hidden size compared to the heavy branch.</p><h3 id=routing>Routing
<a class=heading-link href=#routing><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>The core of Conditional Computation lies in determining which tokens are important and which are not. To do this, we create a scoring function for each token. This function takes the value of the token $X_i$ and maps it to a d-dimensional embedding.</p><p>$$ s_i = X_i . u_d $$</p><ul><li>$s_i$ is the score for token $i$</li><li>$u$ is the embedding function</li></ul><p>Once we have the scores for all tokens, we need to select the top-k tokens. This isn&rsquo;t straightforward since $s_i$ has a dimensionality of $d$, and we can&rsquo;t simply pick the top-k values (we also need to normalize this score). The authors employ an iterative soft top-k algorithm from <a href=https://arxiv.org/abs/2304.04947 class=external-link target=_blank rel=noopener>lei2023conditional</a> and <a href=https://arxiv.org/abs/2211.01267 class=external-link target=_blank rel=noopener>qian2022multivector</a>. In short, this is an optimization problem where we solve the Dual Problem using Coordinate Descent.</p><h4 id=coordinate-descent>Coordinate Descent
<a class=heading-link href=#coordinate-descent><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>Coordinate Descent is a gradient free optimization algorithm (Yes it does not use Gradient Descent), the intuition is that if we have an function $f(x_1, \cdots, x_n)$ we can transform this problem into a single variable optimization problem by fixing all other variables except one. Than we use a root finding algorithm like <a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brentq.html class=external-link target=_blank rel=noopener>Brent Dekker</a> (Thechinally this is a bracketing algorithm combined with secant method and quadratic interpolation) to find the optimal value for this variable. We repeat this process for all variables.</p><h4 id=coordinate-descent-1>Coordinate Descent
<a class=heading-link href=#coordinate-descent-1><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>Coordinate Descent is a gradient-free optimization algorithm (yes, no Gradient Descent). The idea is that if we have a function $f(x_1, \cdots, x_n)$, we can transform this problem into a single-variable optimization problem by fixing all other variables except one. Then, we use a root-finding algorithm like <a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brentq.html class=external-link target=_blank rel=noopener>Brent Dekker</a> (technically, this is a bracketing algorithm combined with the secant method and quadratic interpolation) to find the optimal value for this variable. We repeat this process for all variables.</p><h5 id=least-absolute-shrinkage-and-selection-operator-lasso>Least Absolute Shrinkage and Selection Operator (LASSO)
<a class=heading-link href=#least-absolute-shrinkage-and-selection-operator-lasso><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h5><p>Many of you have probably used this algorithm without even realizing it. If you&rsquo;ve ever used LASSO regression (which is just linear regression with L1 regularization), the fastest way to solve this problem currently is using Coordinate Descent. Why is this the case? Well, firstly, the L1 norm is convex but not smooth, so we can&rsquo;t use Gradient-Based Optimizations, but we can use Coordinate Descent. There are other ways to solve this problem, like using Proximal Descent or Subgradient Descent (yes, L1 is not differentiable at 0, but the subgradient is), but Coordinate Descent is simply faster.</p><h3 id=computation>Computation
<a class=heading-link href=#computation><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Since we have an optimization problem within our routing mechanism, we want this routing to be able to send signals. As a result, we want the routing scores to be part of the computation graph.</p><h4 id=feedforward>Feedforward
<a class=heading-link href=#feedforward><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>$$X_i = X_i + FFd_{Light}(X_i) + \tilde{s_i} . FFd_{Heavy}(X_i)$$</p><ul><li>$X_i$ is the model state at token $i$</li><li>$\tilde{s}_i$ is the normalized routing score (this is 0 for non-routed tokens)</li></ul><h4 id=attention-1>Attention
<a class=heading-link href=#attention-1><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>$$X_i = X_i = A_{Light}(X_i, X) + \tilde{s}^q_i . A_{Heavy}(X_i, \tilde{s}^{kv} . X)$$</p><ul><li>$X_i$ is the model state at token $i$</li><li>$\tilde{s}^q_i$ are the normalized routing scores for the queries for token i set to 0 if not routed</li><li>$\tilde{s}^{kv}$ are the normalized routing scores for the key-values for all tokens set to 0 if not routed</li></ul><h4 id=performance>Performance
<a class=heading-link href=#performance><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>Here we compare the performance between vanilla T5, LongT5 and CoLT5:</p><ul><li>T5 $12nd^2 + 2n^2d$</li><li>LongT5 $12nd^2 + \frac{n^2}{8}d$</li><li>ColT5 $7\frac{1}{4}nd^2 + \frac{n^2}{84}d$</li></ul><h2 id=decoder>Decoder
<a class=heading-link href=#decoder><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>During output generation, long input sentences can cause a memory bandwidth bottleneck. This can be mitigated by using Multi Query Attention (MQA) to expedite the decoding process. In MQA, all the query heads share the same key-value pair.</p><p><img alt="Grouped Attention Variants" src=/images/multi_head_grouped_multi_query_attention.png></p><h3 id=performance-1>Performance
<a class=heading-link href=#performance-1><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Vanilla Multi Head Attention tends to have the highest accuracy but it requires the most memory and is the slowest to generate. By allowing query heads to share key-value pairs, we can reduce the memory requirements and improve token generation speed. However, this speedup comes at a cost, resulting in a loss of accuracy.</p><h2 id=cons-1>Cons
<a class=heading-link href=#cons-1><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>With LongT5, we have open-source implementations from <a href=https://huggingface.co/docs/transformers/model_doc/longt5 class=external-link target=_blank rel=noopener>Hugging Face LongT5</a> and <a href=https://github.com/google-research/longt5 class=external-link target=_blank rel=noopener>Google LongT5</a>. Unfortunately, with CoLT5, things get tricky. I found the following repository <a href=https://github.com/lucidrains/CoLT5-attention class=external-link target=_blank rel=noopener>ColT5</a>, but the implementation is a best-effort reproduction (and an excellent one at that). However, as the author mentions, there are some open questions about the implementation.</p><h1 id=personal-thoughts>Personal Thoughts
<a class=heading-link href=#personal-thoughts><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>CoLT5 is an excellent extension to vanilla T5 and shows a lot of promise. Its biggest downside is the lack of an official implementation and the absence of a pretrained model. LongT5, on the other hand, is a great extension to T5, but it starts to show its limits when we begin to scale up the input length to modern standards.</p><p>I would like to see a continuation of CoLT5 with exciting features such as Mixture of Experts and Sliding Window Multi Query Attention in the decoder part. With these, we would be able to efficiently process long input sequences and generate high-quality outputs efficiently.</p><h1 id=disclaimer>Disclaimer
<a class=heading-link href=#disclaimer><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Since I am not an english native speaker, I use ChatGPT to help me with the text (Formatting, Spelling, etc). However I did write every single word in this blog post, If you are interested you can check the the original text (and its history) <a href=https://github.com/n1o/n1o.github.io/blob/master/content/posts/longer-context-for-t5.md class=external-link target=_blank rel=noopener>here</a></p></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2020 -
2024
n1o_c0rTx
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ")}</script></body></html>