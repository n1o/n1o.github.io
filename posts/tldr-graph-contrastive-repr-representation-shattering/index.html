<!doctype html><html lang=en><head><title>TLDR; Graph Contrastive Learning: Representation Scattering · Artificial Intelligence and Machine Learning Research
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Marek Barak"><meta name=description content="Source Link to heading Paper link: https://openreview.net/pdf?id=R8SolCx62K Source Code: https://github.com/hedongxiao-tju/SGRL Abstract Link to heading Contrastive Learning (CL) is one of my favorite techniques, it is a self-supervised approach for learning latent representations with a special property: Similar elements have representations that are closer together and elements that are different are farther from each other. The paper: Exploitation of a Latent Mechanism in Graph Contrastive Learning Representation Scattering takes a very novel approach to CL and it gives a nice theoretical foundation of CL and Graph!"><meta name=keywords content="blog,developer,personal"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="TLDR; Graph Contrastive Learning: Representation Scattering"><meta name=twitter:description content="Source Link to heading Paper link: https://openreview.net/pdf?id=R8SolCx62K Source Code: https://github.com/hedongxiao-tju/SGRL Abstract Link to heading Contrastive Learning (CL) is one of my favorite techniques, it is a self-supervised approach for learning latent representations with a special property: Similar elements have representations that are closer together and elements that are different are farther from each other. The paper: Exploitation of a Latent Mechanism in Graph Contrastive Learning Representation Scattering takes a very novel approach to CL and it gives a nice theoretical foundation of CL and Graph!"><meta property="og:url" content="https://n1o.github.io/posts/tldr-graph-contrastive-repr-representation-shattering/"><meta property="og:site_name" content="Artificial Intelligence and Machine Learning Research"><meta property="og:title" content="TLDR; Graph Contrastive Learning: Representation Scattering"><meta property="og:description" content="Source Link to heading Paper link: https://openreview.net/pdf?id=R8SolCx62K Source Code: https://github.com/hedongxiao-tju/SGRL Abstract Link to heading Contrastive Learning (CL) is one of my favorite techniques, it is a self-supervised approach for learning latent representations with a special property: Similar elements have representations that are closer together and elements that are different are farther from each other. The paper: Exploitation of a Latent Mechanism in Graph Contrastive Learning Representation Scattering takes a very novel approach to CL and it gives a nice theoretical foundation of CL and Graph!"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-23T13:04:42+01:00"><meta property="article:modified_time" content="2025-02-23T13:04:42+01:00"><meta property="article:tag" content="TLDR"><meta property="article:tag" content="GNN"><meta property="article:tag" content="Graph-Representation"><meta property="article:tag" content="CL"><meta property="og:see_also" content="https://n1o.github.io/posts/tldr-hc-gae/"><meta property="og:see_also" content="https://n1o.github.io/posts/tldr-duplex/"><meta property="og:see_also" content="https://n1o.github.io/posts/tldr-hc-gae/"><meta property="og:see_also" content="https://n1o.github.io/posts/tldr-duplex/"><link rel=canonical href=https://n1o.github.io/posts/tldr-graph-contrastive-repr-representation-shattering/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://n1o.github.io/>Artificial Intelligence and Machine Learning Research
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Writing</a></li><li class=navigation-item><a class=navigation-link href=/awesome-t5/>Awesome T5</a></li><li class=navigation-item><a class=navigation-link href=/awesome-ssm/>Awesome SSM</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/tldr-graph-contrastive-repr-representation-shattering/>TLDR; Graph Contrastive Learning: Representation Scattering</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2025-02-23T13:04:42+01:00>February 23, 2025
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
6-minute read</span></div><div class=categories><i class="fa-solid fa-folder" aria-hidden=true></i>
<a href=/categories/tldr/>TLDR</a>
<span class=separator>•</span>
<a href=/categories/gnn/>GNN</a>
<span class=separator>•</span>
<a href=/categories/graph-representation/>Graph-Representation</a>
<span class=separator>•</span>
<a href=/categories/cl/>CL</a></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/tldr/>TLDR</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/gnn/>GNN</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/graph-representation/>Graph-Representation</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/cl/>CL</a></span></div></div></header><div class=post-content><h1 id=source>Source
<a class=heading-link href=#source><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><ul><li>Paper link: <a href="https://openreview.net/pdf?id=R8SolCx62K" class=external-link target=_blank rel=noopener>https://openreview.net/pdf?id=R8SolCx62K</a></li><li>Source Code: <a href=https://github.com/hedongxiao-tju/SGRL class=external-link target=_blank rel=noopener>https://github.com/hedongxiao-tju/SGRL</a></li></ul><h1 id=abstract>Abstract
<a class=heading-link href=#abstract><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Contrastive Learning (CL) is one of my favorite techniques, it is a self-supervised approach for learning latent representations with a special property: Similar elements have representations that are closer together and elements that are different are farther from each other. The paper: Exploitation of a Latent Mechanism in Graph Contrastive Learning Representation Scattering takes a very novel approach to CL and it gives a nice theoretical foundation of CL and Graph!</p><h1 id=representation-scattering>Representation Scattering
<a class=heading-link href=#representation-scattering><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>First what is Representation Scattering? The idea is that we learn a representation subspace, where individual entries are uniformly distributed in this subspace. Why is this useful? Without trying to cover the whole subspace, we may end up with hot-spots, where many entries live in, and waste uninhabited areas. This is suboptimal since we tend to use this subspace to compare entries and since we fail to cover the space evenly the whole distance will be extremely biased.</p><h2 id=graphs>Graphs
<a class=heading-link href=#graphs><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Let&rsquo;s elaborate on two constraints that are especially useful for Graphs in terms of Scattering:</p><ol><li><strong>Uniformity</strong>, this is just the thing from above, we try to cover the representation space uniformly.</li><li><strong>Center away</strong>, so graphs are made of nodes and edges, the idea of center away, that the individual nodes of a given graph, should be centered around a <em>Center Node</em> (later we look into how to compute this) and nodes that are connected should be closer to each other than those that are not.</li></ol><h1 id=graphs-and-other-cl-approaches>Graphs and other CL Approaches
<a class=heading-link href=#graphs-and-other-cl-approaches><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Current research, in terms of CL and Graphs is more-or-less about 3 ideas:</p><ol><li><a href=https://arxiv.org/abs/1809.10341 class=external-link target=_blank rel=noopener>DGI</a> like methods</li><li><a href=https://arxiv.org/abs/1807.03748 class=external-link target=_blank rel=noopener>InfoNCE</a> like methods</li><li><a href=https://arxiv.org/abs/2006.07733 class=external-link target=_blank rel=noopener>BGRL</a> like methods</li></ol><p>All of these approaches connect to Graph Representation Scattering, at least to some degree.</p><h2 id=dgi>DGI
<a class=heading-link href=#dgi><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>In DGI we need negative samples (this is a common theme in CL) they are constructed by corrupting (random node permutations) the original Graph. The positive samples are local patches (subgraphs) of the original graph. Because of this we have two distributions: original and noise. Where we maximize the Jensen Shannon Divergence (JSD) between the original distribution and the corrupted distribution. (JSD is just an alternation of KL divergence which is smooth and symmetric)</p><p>The connection to Representation Scattering is that: DGI tries to distinguish between the local semantics of nodes within the original graph and its mean, which correlates with representation scattering.</p><h2 id=infonce>InfoNCE
<a class=heading-link href=#infonce><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>In InfoNCE like approaches we choose an anchor point (this is a node in the Graph we want to learn representations for). We draw positive samples by using Graph augmentation methods (we do slight changes to the graph), and negative samples are in-batch negatives (or hard-negatives). The model is then trained by Contrastive Loss where we measure the cosine distance between the positive samples and anchor point, where we want this distance to be small and the same with negative samples but we want to maximize the distance.</p><p>The connection to Representation Scattering is more obvious, we have a center and we want nodes to be close to this center and the negative samples should force the individual representations to cover the whole subspace. Actually InfoNCE loss serves as an upper bound for representation scattering loss, however it is not perfect since the need of negative samples may introduce bias (false negatives) resulting in collapse of the representations subspace (super fancy words, in human language we end up with hot-spots).</p><h2 id=bgrl>BGRL
<a class=heading-link href=#bgrl><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>BGRL is inspired by the BYOL (Bootstrap Your Own Latent) method from computer vision. It avoids using explicit negative samples. Instead, it trains two neural networks – an online network and a target network – and the online network tries to predict the target network&rsquo;s representation of a different augmented view of the same input. Where the target network provides &ldquo;better&rdquo; representations, which the online network learns to predict.</p><p>The connection to Representation Scattering is the use of Batch Normalization (BN) in BGRL methods (it is also shown that leaving out BN drastically reduces the accuracy). A reminder in BN we technically Normalize the data, but instead of having zero mean and unit variance we learn a parameter for the mean and variance, from this it is obvious that BN is a Center Away like constraint, however it is not perfect since it is indirect without explicit guidance, resulting in nonuniform coverage of the hyperspace.</p><h1 id=representation-scattering-mechanism-rsm>Representation Scattering Mechanism (RSM)
<a class=heading-link href=#representation-scattering-mechanism-rsm><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Let&rsquo;s talk about how do we actually achieve Representation Scattering in a Graph:
<img src=/images/graph_representation_scattering.png></p><h2 id=methodology>Methodology
<a class=heading-link href=#methodology><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>We have two core components:</p><ol><li>Representation Scattering Mechanism (RSM)</li><li>Topology Based Constraint Mechanism (TCM)</li></ol><p>Where SRM ($f_{\phi} \rightarrow H_{target}$) and TCM $(f_{\theta} \rightarrow H_{online})$ are represented as two distinct encoders each with different parameters, each producing their own embedding, each serving a different role.</p><h2 id=rsm>RSM
<a class=heading-link href=#rsm><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The first encoder: $f_{\phi} \rightarrow H_{target}$ produces the first embedding. By taking the mean of $H_{target}$ we produce the Scattering Center (c) for all the nodes in the Graph that the nodes belong to. Parameters $\phi$ are carefully updated in a way that we want to push away the individual node representations from the center:</p><p>$$\tilde{h_i} = Trans_{R^d \rightarrow S^k}(h_i) = \frac{h_i}{Max(||h_i||_2, \epsilon)}, S^k = {\tilde{h_i} : ||\tilde{h_i}||_2 = 1}$$</p><ul><li>individual node representations $h_i$ are projected from the original space $R^d$ to a subspace $S^k$</li><li>$\epsilon$ is a small value so we do not divide by zero</li><li>$||\tilde{h_i}||<em>2 = (\sum</em>{j=1}^k \tilde{h_{ij}}^2)^{\frac{1}{2}}$</li></ul><h3 id=loss>Loss
<a class=heading-link href=#loss><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>$$ L_{scattering} = -\frac{1}{n} \sum_{i=1}^{n} ||\tilde{h_i} - c||_2^2$$</p><ul><li>$c = \frac{1}{n} \sum_{i=1}^{n} \tilde{h}_i$, is just the center again just taking the average of individual transformed node embeddings</li></ul><h3 id=cons>Cons
<a class=heading-link href=#cons><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>RSM eliminates the need of manually designing negative samples! This is a huge win since negative samples are one of the main Pain Points in CL.</p><h2 id=tcm>TCM
<a class=heading-link href=#tcm><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The second encoder: $(f_{\theta} \rightarrow H_{online})$ is responsible to preserve the topology of the graph. This means that if we have 2 nodes that are connected, then their representations should also be closer together, and it is done by injecting topologically aggregated neighborhood representations:</p><p>$$ H_{online}^{topology} = \hat{A}^k H_{online} + H_{online}$$</p><ul><li>$\hat{A}^k H_{online}$ is the aggregation of the k-order neighbourhood with $\hat{A} = A + I$ adding a self-loop</li></ul><p>And since TCM is very similar to BGRL we have an extra neural network used to predict the representations $q_{\theta}(.)$ that are used to predict the latent representations:</p><p>$$ Z_{online} = q_{\theta}(H_{online}^{topology})$$</p><h3 id=loss-1>Loss
<a class=heading-link href=#loss-1><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>$$ L_{alignment} = -\frac{1}{N} \sum_{i=1}^{N} \frac{Z_{(online, i)}^T H_{(target, i)}}{||Z_{(online, i)}|| ||H_{(target, i)}||}$$</p><p>This loss updates the encoder parameters $\theta$, the predictor parameters $\pi$ are updated using stop gradient propagation:</p><p>$$ \phi \leftarrow \tau \phi + (1 - \tau) \theta$$</p><h3 id=cons-1>Cons
<a class=heading-link href=#cons-1><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>We do not need any data augmentation since the encoder learns representations that are invariant to perturbations</p><h1 id=final-remarks>Final Remarks
<a class=heading-link href=#final-remarks><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Representation learning is one of my favorite subjects of all times where CL plays a crucial role (I also like when I can reconstruct the whole graph from the representations back, but that is for a different post). With Representation Scattering we finally have a theoretically sound approach to learn representations of individual nodes, that cover the whole subspace uniformly, where individual nodes are close together if they are connected and without the need of Negative Samples, or weird positive Samples!</p></div><footer><section class=see-also><h3 id=see-also-in-tldr>See also in TLDR
<a class=heading-link href=#see-also-in-tldr><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><nav><ul><li><a href=/posts/tldr-hc-gae/>TLDR; HC-GAE The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation Learning</a></li><li><a href=/posts/tldr-duplex/>TLDR; Duplex: Dual GAT for Complex Embeddings of Directed Graphs</a></li></ul></nav><h3 id=see-also-in-gnn>See also in GNN
<a class=heading-link href=#see-also-in-gnn><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><nav><ul><li><a href=/posts/tldr-hc-gae/>TLDR; HC-GAE The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation Learning</a></li><li><a href=/posts/tldr-duplex/>TLDR; Duplex: Dual GAT for Complex Embeddings of Directed Graphs</a></li></ul></nav></section><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2020 -
2025
Marek Barak
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ")}</script></body></html>