<!doctype html><html lang=en><head><title>Paper overview: Hungry Hungry Hippos: Towards Language Modeling with State Space Models Â· Data, Code and Breaking Stuff
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="n1o_c0rTx"><meta name=description content="High level overview Link to heading By combining State Space Models (SSMs) with Attention, we get a model that generates text more efficiently, with a speed increase of approximately 1.6 times. Additionally, this approach requires less paremters, enabling the development of larger models on existing hardware.
Language modeling requirements Link to heading The Transformer architecture, which forms the basis of ChatGPT, is riding high on the hype train due to its impressive performance."><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Paper overview: Hungry Hungry Hippos: Towards Language Modeling with State Space Models"><meta name=twitter:description content="High level overview Link to heading By combining State Space Models (SSMs) with Attention, we get a model that generates text more efficiently, with a speed increase of approximately 1.6 times. Additionally, this approach requires less paremters, enabling the development of larger models on existing hardware.
Language modeling requirements Link to heading The Transformer architecture, which forms the basis of ChatGPT, is riding high on the hype train due to its impressive performance."><meta property="og:title" content="Paper overview: Hungry Hungry Hippos: Towards Language Modeling with State Space Models"><meta property="og:description" content="High level overview Link to heading By combining State Space Models (SSMs) with Attention, we get a model that generates text more efficiently, with a speed increase of approximately 1.6 times. Additionally, this approach requires less paremters, enabling the development of larger models on existing hardware.
Language modeling requirements Link to heading The Transformer architecture, which forms the basis of ChatGPT, is riding high on the hype train due to its impressive performance."><meta property="og:type" content="article"><meta property="og:url" content="https://n1o.github.io/posts/hungry-hungry-hippos/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-06T09:39:03+01:00"><meta property="article:modified_time" content="2023-02-06T09:39:03+01:00"><link rel=canonical href=https://n1o.github.io/posts/hungry-hungry-hippos/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.0fa2dc75ed1b76894ac0e062b10a6c4730daa745096fa120114b290ed8a48788.css integrity="sha256-D6Lcde0bdolKwOBisQpsRzDap0UJb6EgEUspDtikh4g=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.593028e7f7ac55c003b79c230d1cd411bb4ca53b31556c3abb7f027170e646e9.css integrity="sha256-WTAo5/esVcADt5wjDRzUEbtMpTsxVWw6u38CcXDmRuk=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5><meta name=generator content="Hugo 0.124.1"></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>Data, Code and Breaking Stuff
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://n1o.github.io/posts/hungry-hungry-hippos/>Paper overview: Hungry Hungry Hippos: Towards Language Modeling with State Space Models</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2023-02-06T09:39:03+01:00>February 6, 2023
</time></span><span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
4-minute read</span></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/nlp/>NLP</a></span></div></div></header><div class=post-content><h1 id=high-level-overview>High level overview
<a class=heading-link href=#high-level-overview><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>By combining State Space Models (SSMs) with Attention, we get a model that generates text more efficiently, with a speed increase of approximately 1.6 times. Additionally, this approach requires less paremters, enabling the development of larger models on existing hardware.</p><h1 id=language-modeling-requirements>Language modeling requirements
<a class=heading-link href=#language-modeling-requirements><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>The Transformer architecture, which forms the basis of ChatGPT, is riding high on the hype train due to its impressive performance. Although it is just a basic <a href=ttps://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html>Transformer</a>, it has proven to be extremely effective. The reason for its success is explored in the paper &ldquo;<a href=https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html>In-Context Learning and Induction Heads</a>.&rdquo; The authors argue that the majority of the in-context learning capacity of the Transformer architecture can be evaluated by two tests:</p><p>The Induction Head Task Test evaluates a model&rsquo;s ability to recall context after encountering a special token. The test requires the model to recall the token that immediately followed the special token earlier in the sequence.</p><p>The Active Recall Task Test is similar to the Induction Head Task Test, but involves remembering multiple key-value pairs.</p><p>Attention, a key component of Transformers, has both Inductive Head and Active Recall capabilities. It compares tokens by constructing the quadratic attention matrix $QK^T$ and recalls tokens by applying the softmax function to the attention matrix and multiplying by $V$.</p><p>Therefore, a model that performs well on these two tests is likely to also perform well in language modeling.</p><h1 id=state-space-models>State Space Models
<a class=heading-link href=#state-space-models><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>SSM is a type of <a href=https://en.wikipedia.org/wiki/Hidden_Markov_model>Hidden Markov Model</a> where the hidden state is continuous. Most people know SSMs trough Kalman Filtering, a well-known algorithm in the field of Bayesian filtering, is an exact method for solving Linear-Gaussian SSMs.</p><p>In the era of neural networks, SSMs can be expressed as the following layer:</p><p>$$y = SSM_{A,B,C,D}(u)$$</p><p>where $A,B,C,D$ are parameters learned using gradient-based optimization.</p><p>A closer examination of the layer would reveal:</p><p>$$x_i = Ax_{i-1} + Bu_i $$
$$y_i = Cx_i + Du_i $$</p><p>where $x$ represents the hidden state, $u$ is an input from the user, and $y$ is the output.</p><h2 id=benefits>Benefits
<a class=heading-link href=#benefits><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>SSMs allow for efficient generation of sequences, as the next entry in the series only depends on the current state. This allows for extrapolation of larger sequences beyond those seen during training.</p><h2 id=downsides>Downsides
<a class=heading-link href=#downsides><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>The recurrent nature of SSMs can lead to high IO overhead and inefficient hardware utilization, due to cache misses. To mitigate this issue, we can employ convolutions to improve performance.</p><h2 id=convolution-and-fast-fourier-transform-fft>Convolution and Fast Fourier Transform (FFT)
<a class=heading-link href=#convolution-and-fast-fourier-transform-fft><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>For efficient traning we can write the entire sequence of input $u_1, \cdots, u_N$, the output sequence $y_1, \cdots, y_N$ as a confolution of the input with the filter f</p><p>$$f = [CA^0B, CAB, CA^2B, \cdots, CA^{N-1}B]$$</p><p>Given an initial condition $x_0$ we get</p><p>$$y_i = CA^iBx_0 + (f*u)_i + Du_i$$</p><ul><li>$(f*u)$ is the linear convolution</li></ul><p>More generally any linear time-invariant system (SSM is a special case) can be expressed as a convolution.</p><h3 id=fast-fourier-transform-fft>Fast Fourier Transform (FFT)
<a class=heading-link href=#fast-fourier-transform-fft><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Convolution is still pretty expensive $O(N^2)$ however we can speed it up using FFT.</p><p>$$(f * u) = iFFT(FFT(f) \odot FFT(u))$$</p><p>Esentially we take the FFT of booth f, and u, multiply and take the inverse the FFT. This brings down the computational costs to $O(N \log N)$.</p><h1 id=h3-layer>H3 layer
<a class=heading-link href=#h3-layer><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>The Hybrid SSM+Attention architecture aims to combine the strengths of both SSM and Attention to handle tasks that require both capturing context and efficient computation. By projecting the input $u$ into $Q,V,K$ matrices, the architecture uses a combination of SSM (with diagonal and shift operations) and attention mechanism to produce the output.</p><p>$$ Q \odot SSM_{diag}(SSM_{shift}(K) \odot V)$$</p><p><img alt=H3 src=/images/h3_layer.png></p><p>This allows for better handling of tokens after particular events, as the diagonal SSM addresses the ability to recall context, and the attention mechanism enables comparison of tokens. The resulting architecture could potentially have improved performance compared to either SSM or Attention alone, due to the combination of their strengths.</p><h2 id=shift-ssm>Shift SSM
<a class=heading-link href=#shift-ssm><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>We constrain $A \in R^{m\times m}$ to be a shift matrix</p><p>$$A_{ij} = \begin{cases} 1 & \text{if } i-1 = j \ 0 & \text{otherwise} \end{cases} $$</p><p>By shifting the hidden state down by one we create a memory of previous states.</p><h2 id=diagonal-ssm>Diagonal SSM
<a class=heading-link href=#diagonal-ssm><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Constrains A to be diagonal initialized from a diagonal version of <a href=https://arxiv.org/abs/2206.11893>Hippo</a>, this allows to remember tokens aftherwards for the rest of the sequence</p><h2 id=efficiency>Efficiency
<a class=heading-link href=#efficiency><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>H3 scales $O(N \log N)$ for a sequence of length N where Attention requires $O(N^2)$ time and $O(N^2)$ space</p><h1 id=disclaimer>Disclaimer
<a class=heading-link href=#disclaimer><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>I did read the original paper, and look at the code that was supplied with it. I did write this article on my own, however since I am not a native speaker, I did use ChatGPT for proof reading and improving my writing. If you are really interested how the original was worded. I will provide a link below.</p><h1 id=sources>Sources
<a class=heading-link href=#sources><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><ol><li><a href=https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html>https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html</a></li><li><a href=https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html>https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html</a></li><li><a href=https://en.wikipedia.org/wiki/Hidden_Markov_model>https://en.wikipedia.org/wiki/Hidden_Markov_model</a></li><li><a href=https://arxiv.org/abs/2206.11893>https://arxiv.org/abs/2206.11893</a></li><li><a href=https://arxiv.org/abs/2212.14052>https://arxiv.org/abs/2212.14052</a></li></ol></div><footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//mbarak-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}(),document.addEventListener("themeChanged",function(){document.readyState=="complete"&&DISQUS.reset({reload:!0,config:disqus_config})})</script></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>Â©
2020 -
2024
n1o_c0rTx
Â·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5WLCXX3LGJ"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5WLCXX3LGJ",{anonymize_ip:!1})}</script></body></html>